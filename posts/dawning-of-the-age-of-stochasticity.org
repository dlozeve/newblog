---
title: "The Dawning of the Age of Stochasticity"
date: 2022-03-12
tags: maths, foundations, paper, statistics, probability
toc: false
---


This article [@mumford2000_dawnin_age_stoch] is an interesting call
for a new set of foundations of mathematics on probability and
statistics. It argues that logic has had its time, and now we should
make random variables a first-class concept, as they would make for
better foundations.

* The taxonomy of mathematics

[fn::{-} This is probably the best definition of mathematics I have
seen. Before that, the most satisfying definition was "mathematics is
what mathematicians do". It also raises an interesting question: what
would the study of non-reproducible mental objects be?]

#+begin_quote
The study of mental objects with reproducible properties is called mathematics.
[@davis2012_mathem_exper_study_edition]
#+end_quote

What are the categories of reproducible mental objects? Mumford
considers the principal sub-fields of mathematics (geometry, analysis,
algebra, logic) and argues that they are indeed rooted in common
mental phenomena.

Of these, logic, and the notion of proposition, with an absolute truth
value attached to it, was made the foundation of all the
others. Mumford's argument is that instead, the random variable is (or
should be) the "paradigmatic mental object", on which all others can
be based. People are constantly weighing likelihoods, evaluating
plausibility, and sampling from posterior distributions to refine
estimates.

He then makes a quick historical overview of the principal notions of
probability, which mostly mirror the detailed historical perspective
in @hacking2006_emerg_probab. There is also a short summary of the
work into the foundations of mathematics.

Mumford also claims that although there were many advances in the
foundation of probability (e.g. Galton, Gibbs for statistical physics,
Keynes in economics, Wiener for control theory, Shannon for
information theory), most important statisticians (R. A. Fisher)
insisted on keeping the scope of statistics fairly limited to
empirical data: the so-called "frequentist" school. (This is a vision
of the whole frequentist vs Bayesian debate that I hadn't seen
before. The Bayesian school can be seen as the one who claims that
statistical inference can be applied more widely, even to real-life
complex situations and thought processes. In this point of view, the
emergence of the probabilistic method in various areas of science
would be the strongest argument in favour of bayesianism.)

* What is a "random variable"?

Here, Mumford discusses the various definitions we can apply to the
notion of random variable, from an intuitive and a formal point of
view. The conclusion is essentially that a random variable is a
complex entity that do not easily accept a satisfying definition,
except from a purely formal and axiomatic point of view. (Similar to
the notion of "set", "collection", or "category".)

* Putting random variables in the foundations

The usual way of defining random variables is : predicate logic → sets
→ natural numbers → real numbers → measures → random
variables. Instead, we could put random variables at the foundations,
and define everything else in terms of that.

There is no complete formulation of such a foundation, nor is it clear
that it is possible. However, to make his case, Mumford presents two
developments. One is from Jaynes, who has a complete formalism of
Bayesian probability from a notion of "plausibility". With a few
axioms, we can obtain an isomorphism between a vague notion of
plausibility and a true probability function.

The other example is a proof that the continuum hypothesis is false,
using a probabilistic argument, due to Christopher Freiling.

* Stochastic methods have invaded classical mathematics

I think this is by far the most convincing argument to give a greater
importance to probability and statistics methods in the foundations of
mathematics: there tend to be everywhere, and extremely productive. A
prime example is obviously graph theory, where the "probabilistic
method" has had a deep impact, thanks notably to Erdős. (See
@alon2016_probab_method and [[https://www.college-de-france.fr/site/timothy-gowers/index.htm][Timothy Gowers' lessons at the Collège de
France]] on the probabilistic method for combinatorics and number
theory.) Probabilistic methods also have a huge importance in the
analysis of Partial differential equations, chaos theory, and
mathematical physics in general.

* Thinking as Bayesian inference

I think this is not very controversial in cognitive science: we do not
think by composing propositions into syllogisms, but rather by
inferring probabilities of certain statements being true. Mumford
illustrates this very well with an example from Judea Pearl, which
uses graphical models to represent thought processes. There is also a
link with formal definitions of induction, such as PAC learning, which
is very present in machine learning.

#+begin_quote
My overall conclusion is that I believe stochastic methods will
transform pure and applied mathematics in the beginning of the third
millennium. Probability and statistics will come to be viewed as the
natural tools to use in mathematical as well as scientific modeling.
The intellectual world as a whole will come to view logic as a
beautiful elegant idealization but to view statistics as the standard
way in which we reason and think.
#+end_quote

* References
