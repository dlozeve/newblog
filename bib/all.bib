
@article{beccianiParallelTreeCode1997,
  title = {A Parallel Tree Code for Large {{N}}-Body Simulation Dynamic Load Balance and Data Distribution on a {{CRAY T3D}} System},
  volume = {106},
  issn = {00104655},
  doi = {10.1016/S0010-4655(97)00102-1},
  abstract = {N-body algorithms for long-range unscreened interactions like gravity belong to a class of highly irregular problems whose optimal solution is a challenging task for present-day massively parallel computers. In this paper we describe a strategy for optimal memory and work distribution which we have applied to our parallel implementation of the Barnes \& Hut (1986) recursive tree scheme on a Cray T3D using the CRAFT programming environment. We have performed a series of tests to find an optimal data distribution in the T3D memory, and to identify a strategy for the Dynamic Load Balance in order to obtain good performances when running large simulations (more than 10 million particles). The results of tests show that the step duration depends on two main factors: the data locality and the T3D network contention. Increasing data locality we are able to minimize the step duration if the closest bodies (direct interaction) tend to be located in the same PE local memory (contiguous block subdivision, high granularity), whereas the tree properties have a fine grain distributuion. In a very large simulation, due to network contention, an unbalanced load arises. To remedy this we have devised an automatic work redistribution mechanism which provided a good Dynamic Load Balance at the price of an insignificant overhead.},
  number = {1-2},
  journaltitle = {Computer Physics Communications},
  date = {1997},
  pages = {105--113},
  author = {Becciani, U and Ansaloni, R and Antonuccio-Delogu, V and Erbacci, G and Gambera, M and Pagliaro, A},
  file = {/home/dimitri/Nextcloud/Zotero/storage/Q4T46U2L/Becciani et al. - 1997 - A parallel tree code for large N-body simulation dynamic load balance and data distribution on a CRAY T3D syste.pdf}
}

@article{dubinskiParallelTreeCode1996,
  title = {A Parallel Tree Code},
  volume = {1},
  issn = {13841076},
  doi = {10.1016/S1384-1076(96)00009-7},
  abstract = {We describe a new implementation of a parallel N-body tree code. The code is load-balanced using the method of orthogonal recursive bisection to subdivide the N-body system into independent rectangular volumes each of which is mapped to a processor on a parallel computer. On the Cray T3D, the load balance is in the range of 70-90\% depending on the problem size and number of processors. The code can handle simulations with \textbackslash{}textgreater 10 million particles, roughly a factor of 10 greater than allowed on vectorized tree codes.},
  number = {2},
  journaltitle = {New Astronomy},
  date = {1996},
  pages = {133--147},
  keywords = {Cosmology: theory,Galaxies: formation,Galaxies: kinematics and dynamics,Methods: numerical},
  author = {Dubinski, John},
  file = {/home/dimitri/Nextcloud/Zotero/storage/XRS5U4LQ/Dubinski - 1996 - A parallel tree code.pdf}
}

@article{barnesHierarchicalLogForcecalculation1986,
  title = {A Hierarchical {{O}}({{N}} Log {{N}}) Force-Calculation Algorithm},
  volume = {324},
  issn = {0028-0836},
  url = {http://www.nature.com/doifinder/10.1038/324446a0},
  doi = {10.1038/324446a0},
  abstract = {Until recently the gravitational N-body problem has been modelled numerically either by direct integration, in which the computation needed increases as N 2, or by an iterative potential method in which the number of operations grows as N log N. Here we describe a novel method of directly calculating the force on N bodies that grows only as N log N. The technique uses a tree-structured hierarchical subdivision of space into cubic cells, each of which is recursively divided into eight subcells whenever more than one particle is found to occupy the same cell. This tree is constructed anew at every time step, avoiding ambiguity and tangling. Advantages over potential-solving codes are: accurate local interactions; freedom from geometrical assumptions and restrictions; and applicability to a wide class of systems, including (proto-)planetary, stellar, galactic and cosmological ones. Advantages over previous hierarchical tree-codes include simplicity and the possibility of rigorous analysis of error. Although we concentrate here on stellar dynamical applications, our techniques of efficiently handling a large number of long-range interactions and concentrating computational effort where most needed have potential applications in other areas of astrophysics as well.},
  number = {6096},
  journaltitle = {Nature},
  date = {1986},
  pages = {446--449},
  author = {Barnes, Josh and Hut, Piet},
  file = {/home/dimitri/Nextcloud/Zotero/storage/JNQ3X6TP/Barnes, Hut - 1986 - A hierarchical O(N log N) force-calculation algorithm.pdf}
}

@article{wadlerMonadsFunctionalProgramming1995,
  title = {Monads for Functional Programming},
  issn = {03029743},
  url = {papers2://publication/uuid/51E0DEC3-3E25-4374-A5C6-6234824D0BB0},
  doi = {10.1007/3-540-59451-5_2},
  abstract = {The use of monads to structure functional programs is de- scribed. Monads provide a convenient framework for simulating effects found in other languages, such as global state, exception handling, out- put, or non-determinism. Three case studies are looked at in detail: how monads ease the modification of a simple evaluator; how monads act as the basis of a datatype of arrays subject to in-place update; and how monads can be used to build parsers.},
  issue = {August 1992},
  journaltitle = {Advanced Functional Programming},
  date = {1995},
  pages = {1--31},
  author = {Wadler, Philip},
  file = {/home/dimitri/Nextcloud/Zotero/storage/B3RGIUKA/Wadler - 1995 - Monads for functional programming.pdf},
  eprinttype = {pmid},
  eprint = {21349828}
}

@inproceedings{skogEvaluationZerovelocityDetectors2010,
  title = {Evaluation of Zero-Velocity Detectors for Foot-Mounted Inertial Navigation Systems},
  isbn = {978-1-4244-5864-6},
  doi = {10.1109/IPIN.2010.5646936},
  abstract = {A study of the performance of four zero-velocity detectors for a foot-mounted inertial sensor based pedestrian navigation system is presented. The four detectors are the acceleration moving variance detector, the acceleration magnitude detector, the angular rate energy detector, and a novel generalized likelihood ratio test detector, refereed to as the SHOE. The performance of each detector is assessed by the accuracy of the position solution provided by the navigation system employing the detector to perform zero-velocity updates. The results show that for leveled ground forward gait at a speed of 5 km/h, the angular rate energy detector and the SHOE give the highest performance, with a position accuracy of 0.14\% of the travelled distance. The results also indicate that during leveled ground forward gait, the gyroscope signals hold the most reliable information for zero-velocity detection.},
  booktitle = {2010 {{International Conference}} on {{Indoor Positioning}} and {{Indoor Navigation}}, {{IPIN}} 2010 - {{Conference Proceedings}}},
  date = {2010},
  author = {Skog, Isaac and Nilsson, John Olof and H??ndel, Peter},
  file = {/home/dimitri/Nextcloud/Zotero/storage/N5FPFZ7N/Skog, Nilsson, Hndel - 2010 - Evaluation of zero-velocity detectors for foot-mounted inertial navigation systems.pdf}
}

@article{parkZeroVelocityDetection2010,
  title = {A Zero Velocity Detection Algorithm Using Inertial Sensors for Pedestrian Navigation Systems.},
  volume = {10},
  issn = {14248220},
  doi = {10.3390/s101009163},
  abstract = {In pedestrian navigation systems, the position of a pedestrian is computed using an inertial navigation algorithm. In the algorithm, the zero velocity updating plays an important role, where zero velocity intervals are detected and the velocity error is reset. To use the zero velocity updating, it is necessary to detect zero velocity intervals reliably. A new zero detection algorithm is proposed in the paper, where only one gyroscope value is used. A Markov model is constructed using segmentation of gyroscope outputs instead of using gyroscope outputs directly, which makes the zero velocity detection more reliable.},
  number = {10},
  journaltitle = {Sensors (Basel, Switzerland)},
  date = {2010},
  pages = {9163--9178},
  author = {Park, Sang Kyeong and Suh, Young Soo},
  file = {/home/dimitri/Nextcloud/Zotero/storage/VAWKWCFJ/Park, Suh - 2010 - A zero velocity detection algorithm using inertial sensors for pedestrian navigation systems.pdf},
  eprinttype = {pmid},
  eprint = {22163402}
}

@article{skogZerovelocityDetectionAnAlgorithm2010,
  title = {Zero-Velocity Detection-{{An}} Algorithm Evaluation},
  volume = {57},
  issn = {00189294},
  doi = {10.1109/TBME.2010.2060723},
  abstract = {In this paper, we investigate the problem of detecting-time epochs when zero-velocity updates can be applied in a foot-mounted inertial navigation (motion-tracking) system. We examine three commonly used detectors: the acceleration-moving variance detector, the acceleration-magnitude detector, and the angular rate energy detector. We demonstrate that all detectors can be derived within the same general likelihood ratio test (LRT) framework, given the different prior knowledge about the sensor signals. Further, by combining all prior knowledge, we derive a new LRT detector. Subsequently, we develop a methodology to evaluate the performance of the detectors. Employing the developed methodology, we evaluate the performance of the detectors using leveled ground, slow (approximately 3 km/h) and normal (approximately 5 km/h) gait data. The test results are presented in terms of detection versus false-alarm probability. Our preliminary results show that the new detector performs marginally better than the angular rate energy detector that outperforms both the acceleration-moving variance detector and the acceleration-magnitude detector.},
  number = {11},
  journaltitle = {IEEE Transactions on Biomedical Engineering},
  date = {2010},
  pages = {2657--2666},
  keywords = {Biomedical signal processing,detection,detection algorithm,inertial navigation,navigation},
  author = {Skog, Isaac and Händel, Peter and Nilsson, John Olof and Rantakokko, Jouni},
  file = {/home/dimitri/Nextcloud/Zotero/storage/EIL6U3YZ/Skog et al. - 2010 - Zero-velocity detection-An algorithm evaluation.pdf},
  eprinttype = {pmid},
  eprint = {20667801}
}

@article{grewalApplicationsKalmanFiltering2010,
  title = {Applications of {{Kalman Filtering}} in {{Aerospace}} 1960 to the {{Present}}},
  volume = {30},
  issn = {1066033X},
  doi = {10.1109/MCS.2010.936465},
  abstract = {In the 1960s, the Kalman filter was applied to navigation for the Apollo Project, which required estimates of the trajectories of manned spacecraft going to the Moon and back. With the lives of the astronauts at stake, it was essential that the Kalman filter be proven effective and reliable before it could be used. This article is about the lead up to Kalman's work, key discoveries in the development and maturation of the filter, a sampling of its many applications in aerospace, and recognition of some who played key roles in that history.},
  number = {3},
  journaltitle = {IEEE Control Systems},
  date = {2010},
  pages = {69--78},
  author = {Grewal, Mohinder S. and Andrews, Angus P.},
  file = {/home/dimitri/Nextcloud/Zotero/storage/K8XB23RS/Grewal, Andrews - 2010 - Applications of Kalman Filtering in Aerospace 1960 to the Present.pdf}
}

@article{zillyRecurrentHighwayNetworks2016,
  title = {Recurrent {{Highway Networks}}},
  url = {http://arxiv.org/abs/1607.03474},
  abstract = {Many sequential processing tasks require complex nonlinear transition functions from one step to the next. However, recurrent neural networks with such 'deep' transition functions remain difficult to train, even when using Long Short-Term Memory networks. We introduce a novel theoretical analysis of recurrent networks based on Gersgorin's circle theorem that illuminates several modeling and optimization issues and improves our understanding of the LSTM cell. Based on this analysis we propose Recurrent Highway Networks, which are long not only in time but also in space, generalizing LSTMs to larger step-to-step depths. Experiments indicate that the proposed architecture results in complex but efficient models, beating previous models for character prediction on the Hutter Prize Wikipedia dataset and word-level language modeling on the Penn Treebank corpus.},
  journaltitle = {arXiv preprint},
  date = {2016},
  pages = {11},
  author = {Zilly, Julian Georg and Srivastava, Rupesh Kumar and Koutník, Jan and Schmidhuber, Jürgen},
  file = {/home/dimitri/Nextcloud/Zotero/storage/LCN6L97Q/Zilly et al. - 2016 - Recurrent Highway Networks.pdf}
}

@article{chungEmpiricalEvaluationGated2014,
  title = {Empirical {{Evaluation}} of {{Gated Recurrent Neural Networks}} on {{Sequence Modeling}}},
  abstract = {In this paper we compare different types of recurrent units in recurrent neural networks (RNNs). Especially, we focus on more sophisticated units that implement a gating mechanism, such as a long short-term memory (LSTM) unit and a recently proposed gated recurrent unit (GRU). We evaluate these recurrent units on the tasks of polyphonic music modeling and speech signal modeling. Our experiments revealed that these advanced recurrent units are indeed better than more traditional recurrent units such as tanh units. Also, we found GRU to be comparable to LSTM.},
  journaltitle = {arXiv},
  date = {2014},
  pages = {1--9},
  author = {Chung, Junyoung and Gulcehre, Caglar and Cho, KyungHyun and Bengio, Yoshua},
  file = {/home/dimitri/Nextcloud/Zotero/storage/7KF8GQ4W/Chung et al. - 2014 - Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling.pdf}
}

@article{merweSquarerootUnscentedKalman2001,
  title = {The Square-Root Unscented {{Kalman}} Filter for State And\$\textbackslash{}backslash\$nparameter-Estimation},
  volume = {6},
  issn = {1520-6149},
  doi = {10.1109/ICASSP.2001.940586},
  abstract = {Over the last 20-30 years, the extended Kalman filter (EKF) has\$\textbackslash{}backslash\$nbecome the algorithm of choice in numerous nonlinear estimation and\$\textbackslash{}backslash\$nmachine learning applications. These include estimating the state of a\$\textbackslash{}backslash\$nnonlinear dynamic system as well estimating parameters for nonlinear\$\textbackslash{}backslash\$nsystem identification (eg, learning the weights of a neural network).\$\textbackslash{}backslash\$nThe EKF applies the standard linear Kalman filter methodology to a\$\textbackslash{}backslash\$nlinearization of the true nonlinear system. This approach is\$\textbackslash{}backslash\$nsub-optimal, and can easily lead to divergence. Julier et al. (1997),\$\textbackslash{}backslash\$nproposed the unscented Kalman filter (UKF) as a derivative-free\$\textbackslash{}backslash\$nalternative to the extended Kalman filter in the framework of state\$\textbackslash{}backslash\$nestimation. This was extended to parameter estimation by Wan and Van der\$\textbackslash{}backslash\$nMerwe et al., (2000). The UKF consistently outperforms the EKF in terms\$\textbackslash{}backslash\$nof prediction and estimation error, at an equal computational complexity\$\textbackslash{}backslash\$nof (OL3)l for general state-space problems. When\$\textbackslash{}backslash\$nthe EKF is applied to parameter estimation, the special form of the\$\textbackslash{}backslash\$nstate-space equations allows for an O(L2) implementation.\$\textbackslash{}backslash\$nThis paper introduces the square-root unscented Kalman filter (SR-UKF)\$\textbackslash{}backslash\$nwhich is also O(L3) for general state estimation and\$\textbackslash{}backslash\$nO(L2) for parameter estimation (note the original formulation\$\textbackslash{}backslash\$nof the UKF for parameter-estimation was O(L3)). In addition,\$\textbackslash{}backslash\$nthe square-root forms have the added benefit of numerical stability and\$\textbackslash{}backslash\$nguaranteed positive semi-definiteness of the state covariances},
  journaltitle = {2001 IEEE International Conference on Acoustics, Speech, and Signal Processing. Proceedings (Cat. No.01CH37221)},
  date = {2001},
  pages = {1--4},
  author = {Merwe, R. Van Der and Wan, E.A.},
  file = {/home/dimitri/Nextcloud/Zotero/storage/PS69UI5D/Van der Merwe, Wan - Unknown - The square-root unscented Kalman filter for state and parameter-estimation.pdf},
  eprinttype = {pmid},
  eprint = {20556847}
}

@book{kaminskiDiscreteSquareRoot1971,
  title = {Discrete {{Square Root Filtering}}: {{A Survey}} of {{Current Techniques}}},
  volume = {16},
  isbn = {0018-9286 VO - 16},
  abstract = {The conventional Kalman approach to discrete filtering involves propagation of a state estimate and an error covariance matrix from stage to stage. Alternate recursive relationships have been developed to propagate a state estimate and a square root error covariance instead. Although equivalent algebraically to the conventional approach, the square root filters exhibit improved numerical characteristics, particularly in ill-conditioned problems. In this paper, current techniques in square root filtering are surveyed and related by applying a duality association. Four efficient square root implementations are suggested, and compared with three common conventional implementations in terms of computational complexity and precision. The square root computational burden should not exceed the conventional by more than 50 percent in most practical problems. An examination of numerical conditioning predicts that the square root approach can yield twice the effective precision of the conventional filter in ill-conditioned problems. This prediction is verified in two examples. The excellent numerical characteristics and reasonable computation requirements of the square root approach make it a viable alternative to the conventional filter in many applications, particularly when computer word length is limited, or the estimation problem is badly conditioned.},
  pagetotal = {727–736},
  number = {6},
  date = {1971},
  author = {Kaminski, Paul G. and Bryson, Arthur E. and Schmidt, Stanley F.},
  file = {/home/dimitri/Nextcloud/Zotero/storage/IPTEI89Z/Kaminski, Bryson, Schmidt - 1971 - Discrete Square Root Filtering A Survey of Current Techniques.pdf},
  doi = {10.1109/TAC.1971.1099816}
}

@article{markleyAttitudeErrorRepresentations2003,
  title = {Attitude {{Error Representations}} for {{Kalman Filtering}}},
  volume = {26},
  issn = {0731-5090},
  doi = {10.2514/2.5048},
  abstract = {The quaternion has the lowest dimensionality possible for a globally nonsingular attitude representation. The quaternion must obey a unit norm constraint, though, which has led to the development of an extended Kalman filter using a quaternion for the global attitude estimate and a three-component representation for attitude errors. Various attitude error representations are considered for this multiplicative extended Kalman filter, which incorporates a nonlinear, norm preserving quaternion reset operation. Second-order bias corrections are computed in this framework.},
  number = {2},
  journaltitle = {Journal of Guidance, Control, and Dynamics},
  date = {2003},
  pages = {311--317},
  author = {Markley, F. Landis},
  file = {/home/dimitri/Nextcloud/Zotero/storage/8NP42XDP/Markley - 2003 - Attitude Error Representations for Kalman Filtering.pdf}
}

@article{heLearningImbalancedData2009,
  title = {Learning from Imbalanced Data},
  volume = {21},
  issn = {10414347},
  doi = {10.1109/TKDE.2008.239},
  abstract = {With the continuous expansion of data availability in many large-scale, complex, and networked systems, such as surveillance, security, Internet, and finance, it becomes critical to advance the fundamental understanding of knowledge discovery and analysis from raw data to support decision-making processes. Although existing knowledge discovery and data engineering techniques have shown great success in many real-world applications, the problem of learning from imbalanced data (the imbalanced learning problem) is a relatively new challenge that has attracted growing attention from both academia and industry. The imbalanced learning problem is concerned with the performance of learning algorithms in the presence of underrepresented data and severe class distribution skews. Due to the inherent complex characteristics of imbalanced data sets, learning from such data requires new understandings, principles, algorithms, and tools to transform vast amounts of raw data efficiently into information and knowledge representation. In this paper, we provide a comprehensive review of the development of research in learning from imbalanced data. Our focus is to provide a critical review of the nature of the problem, the state-of-the-art technologies, and the current assessment metrics used to evaluate learning performance under the imbalanced learning scenario. Furthermore, in order to stimulate future research in this field, we also highlight the major opportunities and challenges, as well as potential important research directions for learning from imbalanced data.},
  number = {9},
  journaltitle = {IEEE Transactions on Knowledge and Data Engineering},
  date = {2009},
  pages = {1263--1284},
  keywords = {Active learning,Assessment metrics,Classification,Cost-sensitive learning,Imbalanced learning,Kernel-based learning,Sampling methods},
  author = {He, Haibo and Garcia, Edwardo A.},
  file = {/home/dimitri/Nextcloud/Zotero/storage/P7FZG872/He, Garcia - 2009 - Learning from imbalanced data.pdf},
  eprinttype = {pmid},
  eprint = {24807526}
}

@article{gouldSpandrelsSanMarco1979,
  title = {The Spandrels of {{San Marco}} and the {{Panglossian}} Paradigm: A Critique of the Adaptationist Programme.},
  volume = {205},
  issn = {0962-8452},
  doi = {10.1098/rspb.1979.0086},
  abstract = {An adaptationist programme has dominated evolutionary thought in England and the United States during the past 40 years. It is based on faith in the power of natural selection as an optimizing agent. It proceeds by breaking an oragnism into unitary 'traits' and proposing an adaptive story for each considered separately. Trade-offs among competing selective demands exert the only brake upon perfection; non-optimality is thereby rendered as a result of adaptation as well. We criticize this approach and attempt to reassert a competing notion (long popular in continental Europe) that organisms must be analysed as integrated wholes, with Baupläne so constrained by phyletic heritage, pathways of development and general architecture that the constraints themselves become more interesting and more important in delimiting pathways of change than the selective force that may mediate change when it occurs. We fault the adaptationist programme for its failure to distinguish current utility from reasons for origin (male tyrannosaurs may have used their diminutive front legs to titillate female partners, but this will not explain why they got so small); for its unwillingness to consider alternatives to adaptive stories; for its reliance upon plausibility alone as a criterion for accepting speculative tales; and for its failure to consider adequately such competing themes as random fixation of alleles, production of non-adaptive structures by developmental correlation with selected features (allometry, pleiotropy, material compensation, mechanically forced correlation), the separability of adaptation and selection, multiple adaptive peaks, and current utility as an epiphenomenon of non-adaptive structures. We support Darwin's own pluralistic approach to identifying the agents of evolutionary change.},
  number = {1161},
  journaltitle = {Proceedings of the Royal Society of London Series B Containing papers of a Biological character Royal Society Great Britain},
  date = {1979},
  pages = {581--598},
  author = {Gould, Stephen Jay and Lewontin, Richard C},
  file = {/home/dimitri/Nextcloud/Zotero/storage/T55ZTRVS/Gould, Lewontin - 1979 - The Spandrels of San Marco and the Panglossian Paradigm A Critique of the Adaptationist Programme.pdf},
  eprinttype = {pmid},
  eprint = {42062}
}

@article{manniniOnlineDecodingHidden2014,
  title = {Online Decoding of Hidden Markov Models for Gait Event Detection Using Foot-Mounted Gyroscopes},
  volume = {18},
  issn = {21682194},
  doi = {10.1109/JBHI.2013.2293887},
  abstract = {In this paper, we present an approach to the online implementation of a gait event detector based on machine learning algorithms. Gait events were detected using a uniaxial gyro that measured the foot instep angular velocity in the sagittal plane to feed a four-state left-right hidden Markov model (HMM). The short-time Viterbi algorithm was used to overcome the limitation of the standard Viterbi algorithm, which does not allow the online decoding of hidden state sequences. Supervised learning of the HMM structure and validation with the leave-one-subject-out validation method were performed using treadmill gait reference data from an optical motion capture system. The four gait events were foot strike, flat foot (FF), heel off (HO), and toe off. The accuracy ranged, on average, from 45 ms (early detection, FF) to 35 ms (late detection, HO); the latency of detection was less than 100 ms for all gait events but the HO, where the probability that it was greater than 100 ms was 25\%. Overground walking tests of the HMM-based gait event detector were also successfully performed.},
  number = {4},
  journaltitle = {IEEE Journal of Biomedical and Health Informatics},
  date = {2014},
  pages = {1122--1130},
  keywords = {Gait event detection,gyroscope,hidden Markov model (HMM),human movement analysis,short-time Viterbi (STV)},
  author = {Mannini, Andrea and Genovese, Vincenzo and Sabatini, Angelo Maria},
  file = {/home/dimitri/Nextcloud/Zotero/storage/6SYM8IWC/Mannini, Genovese, Sabatini - 2014 - Online decoding of hidden markov models for gait event detection using foot-mounted gyroscopes.pdf},
  eprinttype = {pmid},
  eprint = {25014927}
}

@article{liAccurateFastFall2008,
  title = {Accurate, {{Fast Fall Detection Using Posture}} and {{Context Information}}},
  doi = {10.1109/p3644.45},
  abstract = {Traditional fall detection is only based on acceleration analysis. In this work we present a novel fall detection method that also utilizes posture and context information. This information can help reduce both false positives and negatives. Our solution also strives for low computational cost and fast response.},
  journaltitle = {Sensys'08: Proceedings of the 6th Acm Conference on Embedded Networked Sensor Systems},
  date = {2008},
  pages = {443--444},
  author = {Li, Qiang and Zhou, Gang and Stankovic, John A and {Acm}},
  file = {/home/dimitri/Nextcloud/Zotero/storage/A39YLXJC/Li et al. - Unknown - Accurate, Fast Fall Detection Using Gyroscopes and Accelerometer-Derived Posture Information.pdf}
}

@article{perelmanEventDetectionWater2012,
  title = {Event Detection in Water Distribution Systems from Multivariate Water Quality Time Series.},
  volume = {46},
  issn = {1520-5851},
  doi = {10.1021/es3014024},
  abstract = {In this study, a general framework integrating a data-driven estimation model with sequential probability updating is suggested for detecting quality faults in water distribution systems from multivariate water quality time series. The method utilizes artificial neural networks (ANNs) for studying the interplay between multivariate water quality parameters and detecting possible outliers. The analysis is followed by updating the probability of an event, initially assumed rare, by recursively applying Bayes' rule. The model is assessed through correlation coefficient (R(2)), mean squared error (MSE), confusion matrices, receiver operating characteristic (ROC) curves, and true and false positive rates (TPR and FPR). The product of the suggested methodology consists of alarms indicating a possible contamination event based on single and multiple water quality parameters. The methodology was developed and tested on real data attained from a water utility.},
  number = {15},
  journaltitle = {Environmental science \& technology},
  date = {2012},
  pages = {8212--9},
  keywords = {Bayes Theorem,Models,Multivariate Analysis,Neural Networks (Computer),Probability,ROC Curve,Theoretical,Water Quality},
  author = {Perelman, Lina and Arad, Jonathan and Housh, Mashor and Ostfeld, Avi},
  file = {/home/dimitri/Nextcloud/Zotero/storage/M8ZR9BHV/Perelman et al. - Unknown - Event Detection in Water Distribution Systems from Multivariate Water Quality Time Series.pdf},
  eprinttype = {pmid},
  eprint = {22708647}
}

@article{batalMiningRecentTemporal2012,
  title = {Mining {{Recent Temporal Patterns}} for {{Event Detection}} in {{Multivariate Time Series Data}}},
  issn = {2154-817X},
  doi = {10.1145/2339530.2339578},
  abstract = {Improving the performance of classifiers using patternmining tech- niques has been an active topic of data mining research. In this work we introduce the recent temporal pattern mining framework for finding predictive patterns for monitoring and event detection problems in complex multivariate time series data. This framework first converts time series into time-interval sequences of temporal abstractions. It then constructs more complex temporal patterns backwards in time using temporal operators. We apply our frame- work to health care data of 13,558 diabetic patients and show its benefits by efficiently finding useful patterns for detecting and di- agnosing adverse medical conditions that are associated with dia- betes.},
  journaltitle = {Kdd},
  date = {2012},
  pages = {280--288},
  keywords = {event detection,patient classification,patterns,temporal abstractions,temporal pattern mining,time-interval},
  author = {Batal, Iyad and Harrison, James and Moerchen, Fabian},
  file = {/home/dimitri/Nextcloud/Zotero/storage/V53VW5KD/Batal et al. - Unknown - Mining Recent Temporal Patterns for Event Detection in Multivariate Time Series Data.pdf},
  eprinttype = {pmid},
  eprint = {25937993}
}

@article{hartmannApplyingStatisticalTechniques2016,
  title = {Applying Statistical Techniques to Operations Data},
  volume = {59},
  doi = {10.1145/2890780},
  abstract = {MODERN IT SYSTEMS collect an increasing wealth of data from network gear, operating systems, applications, and other components. This data needs to be analyzed to derive vital information about the user experience and business performance. For instance, faults need to be detected, service quality needs to be measured and resource usage of the next days and month needs to be forecasted. Rule \#1: Spend more time working on code that analyzes the meaning of metrics, than code that collects, moves, stores and displays metrics. —Adrian Cockcroft 1 Statistics is the art of extracting information from data, and hence becomes an essential tool for operating modern IT systems. Despite a rising awareness of this fact within the community (see the quote above), resources for learning the relevant statistical methods for this domain are hard to find. The statistics courses offered in universities usually depend on their students having prior knowledge of probability, measure, and set theory, which is a high barrier of entry. Even worse, these cours-es often focus on parametric methods, such as t-tests, that are inadequate for this kind of analysis since they rely on strong assumptions on the distribution of data (for example, normality) that are not met by operations data. This lack of relevance of classical, parametric statistics can be explained by history. The origins of statistics reach back to the 17},
  number = {7},
  journaltitle = {COMMUNICATIONS OF THE ACM},
  date = {2016},
  author = {Hartmann, Heinrich},
  file = {/home/dimitri/Nextcloud/Zotero/storage/M6YLG84G/Hartmann - 2016 - Applying statistical techniques to operations data.pdf}
}

@article{vanderplasFrequentismBayesianismPythondriven2014,
  title = {Frequentism and {{Bayesianism}}: {{A Python}}-Driven {{Primer}}},
  volume = {astro-ph.I},
  url = {https://arxiv.org/abs/1411.5018},
  abstract = {This paper presents a brief, semi-technical comparison of the essential features of the frequentist and Bayesian approaches to statistical inference, with several illustrative examples implemented in Python. The differences between frequentism and Bayesianism fundamentally stem from differing definitions of probability, a philosophical divide which leads to distinct approaches to the solution of statistical problems as well as contrasting ways of asking and answering questions about unknown parameters. After an example-driven discussion of these differences, we briefly compare several leading Python statistical packages which implement frequentist inference using classical methods and Bayesian inference using Markov Chain Monte Carlo.},
  issue = {Scipy},
  journaltitle = {arXiv},
  date = {2014},
  pages = {1--9},
  keywords = {astro-ph.IM},
  author = {VanderPlas, Jake},
  file = {/home/dimitri/Nextcloud/Zotero/storage/JEE6NNV2/VanderPlas - 2014 - Frequentism and Bayesianism A Python-driven Primer.pdf}
}

@article{breimanStatisticalModelingTwo2001,
  title = {Statistical {{Modeling}}: {{The Two Cultures}}},
  volume = {16},
  issn = {08834237},
  url = {https://projecteuclid.org/download/pdf_1/euclid.ss/1009213726},
  doi = {10.2307/2676681},
  abstract = {There are two cultures in the use of statistical modeling to reach conclusions from data. One assumes that the data are generated by a given stochastic data model. The other uses algorithmic models and treats the data mechanism as unknown. The statistical community has been committed to the almost exclusive use of data models. This commit-ment has led to irrelevant theory, questionable conclusions, and has kept statisticians from working on a large range of interesting current prob-lems. Algorithmic modeling, both in theory and practice, has developed rapidly in fields outside statistics. It can be used both on large complex data sets and as a more accurate and informative alternative to data modeling on smaller data sets. If our goal as a field is to use data to solve problems, then we need to move away from exclusive dependence on data models and adopt a more diverse set of tools.},
  number = {3},
  journaltitle = {Statistical Science},
  date = {2001},
  pages = {199--231},
  author = {Breiman, Leo},
  file = {/home/dimitri/Nextcloud/Zotero/storage/LDWJ35U3/Breiman - 2001 - Statistical Modeling The Two Cultures.pdf},
  eprinttype = {pmid},
  eprint = {10511666}
}

@article{bernsteinSalsa20FamilyStream2008,
  title = {The Salsa20 Family of Stream Ciphers},
  volume = {4986 LNCS},
  issn = {03029743},
  doi = {10.1007/978-3-540-68351-3_8},
  abstract = {Salsa20 is a family of 256-bit stream ciphers designed in 2005 and submitted to eSTREAM, the ECRYPT Stream Cipher Project. Salsa20 has progressed to the third round of eSTREAM without any changes. The 20-round stream cipher Salsa20/20 is consistently faster than AES and is recommended by the designer for typical cryptographic applications. The reduced-round ciphers Salsa20/12 and Salsa20/8 are among the fastest 256-bit stream ciphers available and are recommended for applications where speed is more important than confidence. The fastest known attacks use ≈ 2153 simple operations against Salsa20/7, ≈ 2249 simple operations against Salsa20/8, and ≈ 2255 simple operations against Salsa20/9, Salsa20/10, etc. In this paper, the Salsa20 designer presents Salsa20 and discusses the decisions made in the Salsa20 design.},
  journaltitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  date = {2008},
  pages = {84--97},
  author = {Bernstein, Daniel J.},
  file = {/home/dimitri/Nextcloud/Zotero/storage/QIM2Q9VV/Bernstein - 2008 - The salsa20 family of stream ciphers.pdf}
}

@article{bernsteinChaChaVariantSalsa202008,
  title = {{{ChaCha}}, a Variant of {{Salsa20}}},
  url = {http://cr.yp.to/chacha/chacha-20080120.pdf},
  abstract = {ChaCha8 is a 256-bit stream cipher based on the 8-round cipher Salsa20/8. The changes from Salsa20/8 to ChaCha8 are designed to improve diffusion per round, conjecturally increasing resistance to cryptanalysis, while preserving—and often improving—time per round. ChaCha12 and ChaCha20 are analogous modifications of the 12-round and 20-round ciphers Salsa20/12 and Salsa20/20. This paper presents the ChaCha family and explains the differences between Salsa20 and ChaCha.},
  journaltitle = {Workshop Record of SASC},
  date = {2008},
  pages = {1--6},
  author = {Bernstein, Daniel J},
  file = {/home/dimitri/Nextcloud/Zotero/storage/FRDZ39PP/Bernstein - Unknown - ChaCha, a variant of Salsa20.pdf}
}

@article{jensenDigitalProvideInformation2007,
  title = {The Digital Provide: {{Information}} (Technology), Market Performance, and Welfare in the {{South Indian}} Fisheries Sector},
  volume = {122},
  abstract = {When information is limited or costly, agents are unable to engage in optimal arbitrage. Excess price dispersion across markets can arise, and goods may not be allocated efficiently. In this setting, information technologies may improve market performance and increase welfare. Between 1997 and 2001, mobile phone service was introduced throughout Kerala, a state in India with a large fishing industry. Using microlevel survey data, we show that the adoption of mobile phones by fishermen and wholesalers was associated with a dramatic reduction in price dispersion, the complete elimination of waste, and near-perfect adherence to the Law of One Price. Both consumer and producer welfare increased.},
  number = {3},
  journaltitle = {The quarterly journal of economics},
  date = {2007},
  pages = {879--924},
  author = {Jensen, Robert},
  file = {/home/dimitri/Nextcloud/Zotero/storage/TGRZV5JM/Jensen - 2007 - The digital provide Information (technology), market performance, and welfare in the South Indian fisheries sector.pdf}
}

@article{yaoDeepSenseUnifiedDeep2017,
  title = {{{DeepSense}}: A {{Unified Deep Learning Framework}} for {{Time}}-{{Series Mobile Sensing Data Processing}}},
  url = {http://dx.doi.org/10.1145/3038912.3052577},
  doi = {10.1145/3038912.3052577},
  abstract = {Mobile sensing and computing applications usually require time-series inputs from sensors, such as accelerometers, gyroscopes, and magnetometers. Some applications, such as tracking, can use sensed acceleration and rate of rotation to calculate displacement based on physical system models. Other applications, such as ac-tivity recognition, extract manually designed features from sensor inputs for classification. Such applications face two challenges. On one hand, on-device sensor measurements are noisy. For many mobile applications, it is hard to find a distribution that exactly describes the noise in practice. Unfortunately, calculating target quantities based on physical system and noise models is only as accurate as the noise assumptions. Similarly, in classification ap-plications, although manually designed features have proven to be effective, it is not always straightforward to find the most robust features to accommodate diverse sensor noise patterns and het-erogeneous user behaviors. To this end, we propose DeepSense, a deep learning framework that directly addresses the aforemen-tioned noise and feature customization challenges in a unified man-ner. DeepSense integrates convolutional and recurrent neural net-works to exploit local interactions among similar mobile sensors, merge local interactions of different sensory modalities into global interactions, and extract temporal relationships to model signal dy-namics. DeepSense thus provides a general signal estimation and classification framework that accommodates a wide range of ap-plications. We demonstrate the effectiveness of DeepSense using three representative and challenging tasks: car tracking with mo-tion sensors, heterogeneous human activity recognition, and user identification with biometric motion analysis. DeepSense signifi-cantly outperforms the state-of-the-art methods for all three tasks. In addition, we show that DeepSense is feasible to implement on smartphones and embedded devices thanks to its moderate energy consumption and low latency.},
  date = {2017},
  pages = {17--4},
  keywords = {Activity Recognition,Deep Learning,Internet of Things,Mobile Computing,Mobile Sensing,Tracking,User Identification c 2017},
  author = {Yao, Shuochao and Hu, Shaohan and Zhao, Yiran and Zhang, Aston and Abdelzaher, Tarek},
  file = {/home/dimitri/Nextcloud/Zotero/storage/YXTTIG45/Yao et al. - 2016 - DeepSense A Unified Deep Learning Framework for Time-Series Mobile Sensing Data Processing.pdf}
}

@article{briersSmoothingAlgorithmsState2010,
  title = {Smoothing Algorithms for State – Space Models},
  volume = {XX},
  doi = {10.1007/s10463-009-0236-2},
  abstract = {Rigorous Foundations for calculation, or approximation of smoothed distribution},
  issue = {June 2009},
  journaltitle = {Stat},
  date = {2010},
  pages = {61--89},
  keywords = {non-linear diffusion,parameter estimation,rao-blackwellisation,sequential monte carlo,space models,state,two-filter smoothing},
  author = {Briers, Mark and Doucet, Arnaud and Maskell, Simon},
  file = {/home/dimitri/Nextcloud/Zotero/storage/VCF9J4Y2/Briers, Doucet, Maskell - Unknown - Smoothing Algorithms for State-Space Models.pdf}
}

@article{wallFixedintervalSmoothingProblem1981,
  title = {On the Fixed-Interval Smoothing Problem},
  volume = {5},
  issn = {0090-9491},
  url = {http://www.tandfonline.com/doi/abs/10.1080/17442508108833172},
  doi = {10.1080/17442508108833172},
  abstract = {After a review of the development of the Mayne-Fraser two-filter smoother, a first principle argument is used to rederive this smoother. Reversed-time Markov models play a key role in forming a state estimate from future observations. The built-in asymmetry of the Mayne-Fraser smoother is pointed out, and it is shown how this asymmetry may be removed. Additionally, a covariance analysis of the two-filter smoother is provided, and reduced-order smoothers are analyzed.},
  number = {1-2},
  journaltitle = {Stochastics},
  date = {1981},
  pages = {1--41},
  author = {Wall, Joseph E and Willsky, Alan S. and Sandell, Nils R},
  file = {/home/dimitri/Nextcloud/Zotero/storage/8B5JW76D/Wall, Wi Llsky, Sandell - 1981 - On the Fixed-Interval Smoothing Problem.pdf}
}

@article{rauchMaximumLikelihoodEstimates1965,
  title = {Maximum Likelihood Estimates of Linear Dynamic Systems},
  volume = {3},
  issn = {0001-1452},
  doi = {10.2514/3.3166},
  abstract = {This paper considers the problem of estimating the states of linear dynamic systems in the presence of additive Gaussian noise. Difference equations relating the estimates for the prob- lems of filtering and smoothing are derived as well as a similar set of equations relating the covariance of the errors. The derivation is based on the method of maximum likelihood and depends primarily on the simple manipulation of the probability density functions. The solutions are in a form easily mechanized on a digital computer. A numerical example is in- cluded to show the advantage of smoothing in reducing the errors in estimation. In the Appendix the results for discrete systems are formally extended to continuous systems.},
  number = {8},
  journaltitle = {AIAA Journal},
  date = {1965},
  pages = {1445--1450},
  author = {Rauch, H E and Tung, F and Striebel, C T},
  file = {/home/dimitri/Nextcloud/Zotero/storage/PSFG5VU2/Rauch, Tung, Striebel - 1965 - Maximum Likelihood Estimates of Linear Dynamic Systems.pdf}
}

@book{hastieElementsStatisticalLearning2009,
  title = {The {{Elements}} of {{Statistical Learning}}},
  volume = {1},
  isbn = {978-0-387-84857-0},
  url = {http://www.springerlink.com/index/10.1007/b94608},
  abstract = {During the past decade there has been an explosion in computation and information technology. With it has come vast amounts of data in a variety of fields such as medicine, biology, finance, and marketing. The challenge of understanding these data has led to the development of new tools in the field of statistics, and spawned new areas such as data mining, machine learning, and bioinformatics. Many of these tools have common underpinnings but are often expressed with different terminology. This book describes the important ideas in these areas in a common conceptual framework. While the approach is statistical, the emphasis is on concepts rather than mathematics. Many examples are given, with a liberal use of color graphics. It should be a valuable resource for statisticians and anyone interested in data mining in science or industry. The book's coverage is broad, from supervised learning (prediction) to unsupervised learning. The many topics include neural networks, support vector machines, classification trees and boosting-the first comprehensive treatment of this topic in any book. Trevor Hastie, Robert Tibshirani, and Jerome Friedman are professors of statistics at Stanford University. They are prominent researchers in this area: Hastie and Tibshirani developed generalized additive models and wrote a popular book of that title. Hastie wrote much of the statistical modeling software in S-PLUS and invented principal curves and surfaces. Tibshirani proposed the Lasso and is co-author of the very successful An Introduction to the Bootstrap. Friedman is the co-inventor of many data-mining tools including CART, MARS, and projection pursuit. FROM THE REVIEWS: TECHNOMETRICS "This is a vast and complex book. Generally, it concentrates on explaining why and how the methods work, rather than how to use them. Examples and especially the visualizations are principle features...As a source for the methods of statistical learning...it will probably be a long time before there is a competitor to this book."},
  pagetotal = {1–694},
  date = {2009},
  author = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome},
  file = {/home/dimitri/Nextcloud/Zotero/storage/AVAIRYAL/Hastie, Tibshirani, Friedman - Unknown - Springer Series in Statistics The Elements of Statistical Learning The Elements of Statistical.pdf},
  doi = {10.1007/b94608},
  eprinttype = {pmid},
  eprint = {12377617}
}

@book{bishopPatternRecognitionMachine2013,
  title = {Pattern {{Recognition}} and {{Machine Learning}}},
  volume = {53},
  isbn = {978-0-387-31073-2},
  abstract = {The dramatic growth in practical applications for machine learning over the last ten years has been accompanied by many important developments in the underlying algorithms and techniques. For example, Bayesian methods have grown from a specialist niche to become mainstream, while graphical models have emerged as a general framework for describing and applying probabilistic techniques. The practical applicability of Bayesian methods has been greatly enhanced by the development of a range of approximate inference algorithms such as variational Bayes and expectation propagation, while new models based on kernels have had a significant impact on both algorithms and applications. This completely new textbook reflects these recent developments while providing a comprehensive introduction to the fields of pattern recognition and machine learning. It is aimed at advanced undergraduates or first-year PhD students, as well as researchers and practitioners. No previous knowledge of pattern recognition or machine learning concepts is assumed. Familiarity with multivariate calculus and basic linear algebra is required, and some experience in the use of probabilities would be helpful though not essential as the book includes a self-contained introduction to basic probability theory. The book is suitable for courses on machine learning, statistics, computer science, signal processing, computer vision, data mining, and bioinformatics. Extensive support is provided for course instructors, including more than 400 exercises, graded according to difficulty. Example solutions for a subset of the exercises are available from the book web site, while solutions for the remainder can be obtained by instructors from the publisher. The book is supported by a great deal of additional material, and the reader is encouraged to visit the book web site for the latest information. Christopher M. Bishop is Deputy Director of Microsoft Research Cambridge, and holds a Chair in Computer Science at the University of Edinburgh. He is a Fellow of Darwin College Cambridge, a Fellow of the Royal Academy of Engineering, and a Fellow of the Royal Society of Edinburgh. His previous textbook "Neural Networks for Pattern Recognition" has been widely adopted.},
  pagetotal = {1689–1699},
  number = {9},
  date = {2013},
  author = {Bishop, Christopher M},
  file = {/home/dimitri/Nextcloud/Zotero/storage/MR3SWAG6/Unknown - Unknown - Bishop - Pattern Recognition And Machine Learning - Springer 2006.pdf},
  doi = {10.1117/1.2819119},
  eprinttype = {pmid},
  eprint = {25246403}
}

@book{airoldiGettingStartedProbabilistic2007,
  title = {Getting Started in Probabilistic Graphical Models},
  volume = {3},
  isbn = {1553-7358},
  abstract = {Probabilistic graphical models (PGMs) have become a popular tool for computational analysis of biological data in a variety of domains. But, what exactly are they and how do they work? How can we use PGMs to discover patterns that are biologically relevant? And to what extent can PGMs help us formulate new hypotheses that are testable at the bench? This note sketches out some answers and illustrates the main ideas behind the statistical approach to biological pattern discovery.},
  pagetotal = {2421–2425},
  number = {12},
  date = {2007},
  author = {Airoldi, Edoardo M.},
  file = {/home/dimitri/Nextcloud/Zotero/storage/IXDM9MPQ/Airoldi - 2007 - Getting started in probabilistic graphical models.pdf},
  doi = {10.1371/journal.pcbi.0030252},
  eprinttype = {pmid},
  eprint = {18069887}
}

@article{waiteCassiniFindsMolecular2017,
  title = {Cassini Finds Molecular Hydrogen in the {{Enceladus}} Plume: {{Evidence}} for Hydrothermal Processes},
  volume = {356},
  issn = {0036-8075},
  url = {http://www.sciencemag.org/lookup/doi/10.1126/science.aai8703},
  doi = {10.1126/science.aai8703},
  abstract = {Saturn's moon Enceladus has an ice-covered ocean; a plume of material erupts from cracks in the ice. The plume contains chemical signatures of water-rock interaction between the ocean and a rocky core. We used the Ion Neutral Mass Spectrometer onboard the Cassini spacecraft to detect molecular hydrogen in the plume. By using the instrument's open-source mode, background processes of hydrogen production in the instrument were minimized and quantified, enabling the identification of a statistically significant signal of hydrogen native to Enceladus. We find that the most plausible source of this hydrogen is ongoing hydrothermal reactions of rock containing reduced minerals and organic materials. The relatively high hydrogen abundance in the plume signals thermodynamic disequilibrium that favors the formation of methane from CO2 in Enceladus' ocean.},
  number = {6334},
  journaltitle = {Science},
  date = {2017},
  pages = {155--159},
  author = {Waite, J Hunter and Glein, Christopher R and Perryman, Rebecca S and Teolis, Ben D and Magee, Brian A and Miller, Greg and Grimes, Jacob and Perry, Mark E and Miller, Kelly E and Bouquet, Alexis and Lunine, Jonathan I and Brockwell, Tim and Bolton, Scott J},
  file = {/home/dimitri/Nextcloud/Zotero/storage/6KEZYGRY/Waite et al. - Unknown - Cassini finds molecular hydrogen in the Enceladus plume Evidence for hydrothermal processes.pdf}
}

@book{nagelIFMBEProceedings4th2007,
  title = {{{IFMBE Proceedings}} 4th {{International Workshop}} on {{Wearable}} and {{Implantable Body Sensor Networks}} 2007},
  volume = {13},
  isbn = {978-3-540-70993-0},
  date = {2007},
  author = {Nagel, J H},
  file = {/home/dimitri/Nextcloud/Zotero/storage/LURCG44U/Magjarevic, Nagel - Unknown - IFMBE Proceedings Volume 13.pdf}
}

@article{ruppeltNovelFiniteState2015,
  title = {A {{Novel Finite State Machine Based Step Detection Technique}} for {{Pedestrian Navigation Systems}}},
  abstract = {— In this paper we present a novel finite state machine based step detection technique for precise personal navigation so-lutions with a foot-mounted inertial measurement unit (IMU). Generally, step detection methods are used to improve the naviga-tion solution by applying Zero Velocity Updates (ZUPTs) in the navigation filter. All step detection techniques distort the naviga-tion solution if ZUPTs are utilized at wrong times. Our approach based on a finite state machine is able to detect different stances of the foot with high accuracy. Therefore, Zero Velocity Updates can be applied in time and positively affect the precision of the naviga-tion solution. The functionality of the step detection module in combination with a constraint, stochastic cloning (SC) Kalman fil-ter are analyzed with real sensor data recorded with our pedes-trian navigation system. Even with ultra-low cost inertial sensors, this new approach can clearly increase the accuracy of pedestrian navigation systems compared to state-of-the-art approaches.},
  issue = {October},
  date = {2015},
  pages = {13--16},
  keywords = {navigation,can be corrupted due,finite state machine,for that reason,indoor,indoor navigation,navigation systems need other,navigation techniques to reduce,pedestrian,step detection,to multipath effects},
  author = {Ruppelt, J and Kronenwett, N},
  file = {/home/dimitri/Nextcloud/Zotero/storage/YBXN3UHI/Ruppelt, Kronenwett, Trommer - 2015 - A Novel Finite State Machine Based Step Detection Technique for Pedestrian Navigation Systems.pdf}
}

@article{dworkGuiltFreeDataReuse2017,
  title = {Guilt-{{Free Data Reuse}}},
  volume = {60},
  issn = {15577317},
  url = {http://delivery.acm.org/10.1145/3060000/3051088/p86-dwork.pdf?ip=152.3.43.176&id=3051088&acc=ACTIVE SERVICE&key=7777116298C9657D.18C4EEC63BFE39A6.4D4702B0C3E38B35.4D4702B0C3E38B35&CFID=948749011&CFTOKEN=81454117&__acm__=1497539113_1d96ab072d041eef26249474},
  doi = {10.1145/3051088},
  abstract = {Existing approaches to ensuring the validity of inferences drawn from data assume a fixed procedure to be performed, selected before the data are examined. Yet the practice of data analysis is an intrinsically interactive and adaptive process: new analyses and hypotheses are proposed after seeing the results of previous ones, parameters are tuned on the basis of obtained results, and datasets are shared and reused. In this work, we initiate a principled study of how to guar-antee the validity of statistical inference in adaptive data analysis. We demonstrate new approaches for addressing the challenges of adaptivity that are based on techniques developed in privacy-preserving data analysis. As an application of our techniques we give a simple and practical method for reusing a holdout (or testing) set to validate the accuracy of hypotheses produced adaptively by a learning algorithm operating on a training set.},
  number = {4},
  journaltitle = {COMMUNICATIONS OF THE ACM},
  date = {2017},
  author = {Dwork, Cynthia and Feldman, Vitaly and Hardt, Moritz and Pitassi, Toniann and Reingold, Omer and Roth, Aaron},
  file = {/home/dimitri/Nextcloud/Zotero/storage/V7HCNYYZ/Dwork et al. - 2017 - Guilt-Free Data Reuse.pdf}
}

@article{horowitzArtElectronics1990,
  title = {The {{Art}} of {{Electronics}}},
  issn = {00029505},
  doi = {10.1119/1.16385},
  abstract = {This is the thoroughly revised and updated second edition of the hugely successful The Art of Electronics. Widely accepted as the authoritative text and reference on electronic circuit design, both analog and digital, this book revolutionized the teaching of electronics by emphasizing the methods actually used by circuit designers – a combination of some basic laws, rules of thumb, and a large bag of tricks. The result is a largely nonmathematical treatment that encourages circuit intuition, brainstorming, and simplified calculations of circuit values and performance. The new Art of Electronics retains the feeling of informality and easy access that helped make the first edition so successful and popular. It is an ideal first textbook on electronics for scientists and engineers and an indispensable reference for anyone, professional or amateur, who works with electronic circuits.},
  journaltitle = {American Journal of Physics},
  date = {1990},
  pages = {1131},
  author = {Horowitz, Paul and Winfield, Hill},
  file = {/home/dimitri/Nextcloud/Zotero/storage/KUN2QAIC/Horowitz, Winfield - 1990 - The Art of Electronics.pdf},
  eprinttype = {pmid},
  eprint = {3700264}
}

@article{kumarTimeseriesBitmapsPractical2005,
  title = {Time-Series {{Bitmaps}}: A {{Practical Visualization Tool}} for {{Working}} with {{Large Time Series Databases}}},
  url = {http://epubs.siam.org/doi/abs/10.1137/1.9781611972757.55},
  doi = {http://dx.doi.org/10.1137/1.9781611972757.55 Book Code: PR119 Series: Proceedings Pages: 5 Read More: http://epubs.siam.org/doi/abs/10.1137/1.9781611972757.55},
  abstract = {The increasing interest in time series data mining in the last decade has resulted in the introduction of a variety of similarity measures, representations and algorithms. Surprisingly, this massive research effort has had little impact on real world applications. Real world practitioners who work with time series on a daily basis rarely take advantage of the wealth of tools that the data mining community has made available. In this work we attempt to address this problem by introducing a simple parameter-light tool that allows users to efficiently navigate through large collections of time series. Our system has the unique advantage that it can be embedded directly into the any standard graphical user interface, such as Microsoft Windows, thus making deployment easier. Our approach extracts features from a time series of arbitrary length, and uses information about the relative frequency of its features to color a bitmap in a principled way. By visualizing the similarities and differences within a collection of bitmaps, a user can quickly discover clusters, anomalies, and other regularities within their data collection. We demonstrate the utility of our approach with a set of comprehensive experiments on real datasets from a variety of domains},
  journaltitle = {SIAM International Conference on Data Mining},
  date = {2005},
  pages = {531--535},
  keywords = {Chaos Game,Time Series,Visualization},
  author = {Kumar, Nitin and Lolla, Vn and Keogh, Eamonn},
  file = {/home/dimitri/Nextcloud/Zotero/storage/SW9YUMIR/Kumar Venkata et al. - Unknown - Time-series Bitmaps a Practical Visualization Tool for Working with Large Time Series Databases.pdf}
}

@article{kyrkouEmbeddedHardwareEfficientRealTime2016,
  title = {Embedded {{Hardware}}-{{Efficient Real}}-{{Time Classification With Cascade Support Vector Machines}}},
  volume = {27},
  issn = {21622388},
  doi = {10.1109/TNNLS.2015.2428738},
  abstract = {Cascade support vector machines (SVMs) are optimized to efficiently handle problems, where the majority of the data belong to one of the two classes, such as image object classification, and hence can provide speedups over monolithic (single) SVM classifiers. However, SVM classification is a computationally demanding task and existing hardware architectures for SVMs only consider monolithic classifiers. This paper proposes the acceleration of cascade SVMs through a hybrid processing hardware architecture optimized for the cascade SVM classification flow, accompanied by a method to reduce the required hardware resources for its implementation, and a method to improve the classification speed utilizing cascade information to further discard data samples. The proposed SVM cascade architecture is implemented on a Spartan-6 field-programmable gate array (FPGA) platform and evaluated for object detection on 800×600 (Super Video Graphics Array) resolution images. The proposed architecture, boosted by a neural network that processes cascade information, achieves a real-time processing rate of 40 frames/s for the benchmark face detection application. Furthermore, the hardware-reduction method results in the utilization of 25\% less FPGA custom-logic resources and 20\% peak power reduction compared with a baseline implementation.},
  number = {1},
  journaltitle = {IEEE Transactions on Neural Networks and Learning Systems},
  date = {2016},
  pages = {99--112},
  keywords = {Cascade classifier,field-programmable gate array (FPGA),local binary pattern (LBP),neural networks (NNs),parallel architectures,real-time and embedded systems,support vector machines (SVMs)},
  author = {Kyrkou, Christos and Bouganis, Christos Savvas and Theocharides, Theocharis and Polycarpou, Marios M.},
  file = {/home/dimitri/Nextcloud/Zotero/storage/5LBY3ARV/Unknown - Unknown - Embedded Hardware-Efficient Real-Time Classification with Cascade Support Vector Machines.pdf},
  eprinttype = {pmid},
  eprint = {26011869}
}

@article{bakoRealtimeClassificationDatasets2010,
  title = {Real-Time Classification of Datasets with Hardware Embedded Neuromorphic Neural Networks},
  volume = {11},
  issn = {14675463},
  doi = {10.1093/bib/bbp066},
  abstract = {Neuromorphic artificial neural networks attempt to understand the essential computations that take place in the dense networks of interconnected neurons making up the central nervous systems in living creatures. This article demonstrates that artificial spiking neural networks–built to resemble the biological model–encoding information in the timing of single spikes, are capable of computing and learning clusters from realistic data. It shows how a spiking neural network based on spike-time coding can successfully perform unsupervised and supervised clustering on real-world data. A temporal encoding procedure of continuously valued data is developed, together with a hardware implementation oriented new learning rule set. Solutions that make use of embedded soft-core microcontrollers are investigated, to implement some of the most resource-consuming components of the artificial neural network. Details of the implementations are given, with benchmark application evaluation and test bench description. Measurement results are presented, showing real-time and adaptive data processing capabilities, comparing these to related findings in the specific literature.},
  number = {3},
  journaltitle = {Briefings in Bioinformatics},
  date = {2010},
  pages = {348--363},
  keywords = {Clustering,Embedded design,FPGA,Hardware implementation,Spiking neuron models},
  author = {Bako, Laszlo},
  file = {/home/dimitri/Nextcloud/Zotero/storage/L7YDJWL4/Bako - 2010 - Real-time classification of datasets with hardware embedded neuromorphic neural networks.pdf},
  eprinttype = {pmid},
  eprint = {20053732}
}

@article{karantonisImplementationRealtimeHuman2006,
  title = {Implementation of a Real-Time Human Movement Classifier Using a Triaxial Accelerometer for Ambulatory Monitoring},
  volume = {10},
  issn = {10897771},
  doi = {10.1109/TITB.2005.856864},
  abstract = {The real-time monitoring of human movement can provide valuable information regarding an individual's degree of functional ability and general level of activity. This paper presents the implementation of a real-time classification system for the types of human movement associated with the data acquired from a single, waist-mounted triaxial accelerometer unit. The major advance proposed by the system is to perform the vast majority of signal processing onboard the wearable unit using embedded intelligence. In this way, the system distinguishes between periods of activity and rest, recognizes the postural orientation of the wearer, detects events such as walking and falls, and provides an estimation of metabolic energy expenditure. A laboratory-based trial involving six subjects was undertaken, with results indicating an overall accuracy of 90.8\% across a series of 12 tasks (283 tests) involving a variety of movements related to normal daily activities. Distinction between activity and rest was performed without error; recognition of postural orientation was carried out with 94.1\% accuracy, classification of walking was achieved with less certainty (83.3\% accuracy), and detection of possible falls was made with 95.6\% accuracy. Results demonstrate the feasibility of implementing an accelerometry-based, real-time movement classifier using embedded intelligence.},
  number = {1},
  journaltitle = {IEEE Transactions on Information Technology in Biomedicine},
  date = {2006},
  pages = {156--167},
  keywords = {Accelerometer,Ambulatory monitoring,Home telecare,Movement classification},
  author = {Karantonis, Dean M. and Narayanan, Michael R. and Mathie, Merryn and Lovell, Nigel H. and Celler, Branko G.},
  file = {/home/dimitri/Nextcloud/Zotero/storage/RK2USFN2/Karantonis et al. - 2006 - Implementation of a real-time human movement classifier using a triaxial accelerometer for ambulatory monitor.pdf},
  eprinttype = {pmid},
  eprint = {16445260}
}

@inproceedings{kasettyRealtimeClassificationStreaming2008,
  title = {Real-Time Classification of Streaming Sensor Data},
  volume = {1},
  isbn = {978-0-7695-3440-4},
  doi = {10.1109/ICTAI.2008.143},
  abstract = {The last decade has seen a huge interest in classification of time series. Most of this work assumes that the data resides in main memory and is processed offline. However, recent advances in sensor technologies require resource-efficient algorithms that can be implemented directly on the sensors as real-time algorithms. We show how a recently introduced framework for time series classification, time series bitmaps, can be implemented as efficient classifiers which can be updated in constant time and space in the face of very high data arrival rates. We describe results from a case study of an important entomological problem, and further demonstrate the generality of our ideas with an example from robotics.},
  booktitle = {Proceedings - {{International Conference}} on {{Tools}} with {{Artificial Intelligence}}, {{ICTAI}}},
  date = {2008},
  pages = {149--156},
  author = {Kasetty, Shashwati and Stafford, Candice and Walker, Gregory P. and Wang, Xiaoyue and Keogh, Eamonn},
  file = {/home/dimitri/Nextcloud/Zotero/storage/V6Y2MDRI/Kasetty et al. - 2008 - Real-time classification of streaming sensor data.pdf}
}

@article{heEarlyClassificationMultivariate2015,
  title = {Early Classification on Multivariate Time Series},
  volume = {149},
  issn = {18728286},
  doi = {10.1016/j.neucom.2014.07.056},
  abstract = {Multivariate time series (MTS) classification is an important topic in time series data mining, and has attracted great interest in recent years. However, early classification on MTS data largely remains a challenging problem. To address this problem without sacrificing the classification performance, we focus on discovering hidden knowledge from the data for early classification in an explainable way. At first, we introduce a method MCFEC (Mining Core Feature for Early Classification) to obtain distinctive and early shapelets as core features of each variable independently. Then, two methods are introduced for early classification on MTS based on core features. Experimental results on both synthetic and real-world datasets clearly show that our proposed methods can achieve effective early classification on MTS.},
  issue = {PB},
  journaltitle = {Neurocomputing},
  date = {2015},
  pages = {777--787},
  keywords = {Early classification,Feature selection,Multivariate time series},
  author = {He, Guoliang and Duan, Yong and Peng, Rong and Jing, Xiaoyuan and Qian, Tieyun and Wang, Lingling},
  file = {/home/dimitri/Nextcloud/Zotero/storage/49R52XVC/Xing, Pei, Yu - Unknown - Early Classification on Time Series.pdf}
}

@article{haighMachineLearningEmbedded2012,
  title = {Machine {{Learning}} for {{Embedded Systems}} : {{A Case Study}}},
  url = {http://www.cs.cmu.edu/ khaigh/papers/2015-HaighTechReport-Embedded.pdf},
  abstract = {—We describe our application's need for Machine Learning on a General Purpose Processor of an embedded device. Existing ML toolkits tend to be slow and consume memory, making them incompatible with real-time systems, limited hardware resources, or the rapid timing requirements of most embedded systems. We present our ML application, and the suite of optimizations we performed to create a system that can operate effectively on an embeddded platform. We perform an ablation study to analyze the impact of each optimization, and demonstrate over 20x improvement in runtimes over the original implementation, over a suite of 19 benchmark datasets. We present our results on two embedded systems. I. INTRODUCTION Mobile ad hoc networks (MANETs) operate in highly dy-namic, potentially hostile environments. Current approaches to network configuration tend to be static, and therefore perform poorly. It is instead desirable to adaptively configure the radio and network stack to maintain consistent communications. A human is unable to perform this dynamic configuration partly because of the rapid timescales involved, and partly because there are an exponential number of configurations [7]. Machine Learning is a suite of techniques that learn from their experience, by analyzing their observations, updating models of how previous actions performed, and using those insights to make better decisions in the future. The system can then learn how current conditions affect communications quality, and automatically select a configuration to improve performance, even in highly-dynamic missions. The domain requires the ability for the decision maker to select a config-uration in real-time, within the decision-making loop of the radio and IP stack. This paper presents our effort to place Support Vector Ma-chines (SVMs) [21], [22] onto the general purpose processors of two communications networks. Existing SVM libraries are slow and memory intensive. This paper describes how we optimized an existing SVM library to obtain a 20x runtime improvement and controlled the memory footprint of the system. This paper describes the optimizations that either had the most effect on results, or were the most surprising to us as developers.},
  date = {2012},
  author = {Haigh, Karen Zita and Mackay, Allan M and Cook, Michael R and Lin, Li G},
  file = {/home/dimitri/Nextcloud/Zotero/storage/QSJ8NIDC/Haigh et al. - Unknown - Machine Learning for Embedded Systems A Case Study.pdf}
}

@inproceedings{mcmahanCommunicationEfficientLearningDeep2016,
  title = {Communication-{{Efficient Learning}} of {{Deep Networks}} from {{Decentralized Data}}},
  volume = {54},
  url = {http://arxiv.org/abs/1602.05629},
  abstract = {Modern mobile devices have access to a wealth of data suitable for learning models, which in turn can greatly improve the user experience on the device. For example, language models can improve speech recognition and text entry, and image models can automatically select good photos. However, this rich data is often privacy sensitive, large in quantity, or both, which may preclude logging to the data center and training there using conventional approaches. We advocate an alternative that leaves the training data distributed on the mobile devices, and learns a shared model by aggregating locally-computed updates. We term this decentralized approach Federated Learning. We present a practical method for the federated learning of deep networks based on iterative model averaging, and conduct an extensive empirical evaluation, considering five different model architectures and four datasets. These experiments demonstrate the approach is robust to the unbalanced and non-IID data distributions that are a defining characteristic of this setting. Communication costs are the principal constraint, and we show a reduction in required communication rounds by 10-100x as compared to synchronized stochastic gradient descent.},
  booktitle = {International {{Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  date = {2016},
  author = {McMahan, H. Brendan and Moore, Eider and Ramage, Daniel and Hampson, Seth and family=Arcas, given=Blaise Agüera, prefix=y, useprefix=false},
  file = {/home/dimitri/Nextcloud/Zotero/storage/H359CB6E/McMahan et al. - 2016 - Communication-Efficient Learning of Deep Networks from Decentralized Data.pdf}
}

@article{konecnyFederatedOptimizationDistributed2016,
  title = {Federated {{Optimization}}: {{Distributed Machine Learning}} for {{On}}-{{Device Intelligence}}},
  url = {http://arxiv.org/abs/1610.02527},
  abstract = {We introduce a new and increasingly relevant setting for distributed optimization in machine learning, where the data defining the optimization are unevenly distributed over an extremely large number of nodes. The goal is to train a high-quality centralized model. We refer to this setting as Federated Optimization. In this setting, communication efficiency is of the utmost importance and minimizing the number of rounds of communication is the principal goal. A motivating example arises when we keep the training data locally on users' mobile devices instead of logging it to a data center for training. In federated optimziation, the devices are used as compute nodes performing computation on their local data in order to update a global model. We suppose that we have extremely large number of devices in the network — as many as the number of users of a given service, each of which has only a tiny fraction of the total data available. In particular, we expect the number of data points available locally to be much smaller than the number of devices. Additionally, since different users generate data with different patterns, it is reasonable to assume that no device has a representative sample of the overall distribution. We show that existing algorithms are not suitable for this setting, and propose a new algorithm which shows encouraging experimental results for sparse convex problems. This work also sets a path for future research needed in the context of \$\textbackslash{}backslash\$federated optimization.},
  date = {2016},
  pages = {1--38},
  author = {Konečný, Jakub and McMahan, H. Brendan and Ramage, Daniel and Richtárik, Peter}
}

@article{vandermaatenVisualizingHighdimensionalData2008,
  title = {Visualizing High-Dimensional Data Using t-Sne},
  volume = {9},
  issn = {1532-4435},
  url = {https://lvdmaaten.github.io/publications/papers/JMLR_2008.pdf%0Ahttp://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=pubmed&cmd=Retrieve&dopt=AbstractPlus&list_uids=7911431479148734548related:VOiAgwMNy20J},
  doi = {10.1007/s10479-011-0841-3},
  abstract = {We present a new technique called “t-SNE” that visualizes high-dimensional data by giving each datapoint a location in a two or three-dimensional map. The technique is a variation of Stochastic Neighbor Embedding (Hinton and Roweis, 2002) that is much easier to optimize, and produces significantly better visualizations by reducing the tendency to crowd points together in the center of the map. t-SNE is better than existing techniques at creating a single map that reveals structure at many different scales. This is particularly important for high-dimensional data that lie on several different, but related, low-dimensional manifolds, such as images of objects from multiple classes seen from multiple viewpoints. For visualizing the structure of very large data sets, we show how t-SNE can use random walks on neighborhood graphs to allow the implicit structure of all of the data to influence theway in which a subset of the data is displayed. We illustrate the performance of t-SNE on a wide variety of data sets and compare it with many other non-parametric visualization techniques, including Sammon mapping, Isomap, and Locally Linear Embedding. The visualiza- tions produced by t-SNE are significantly better than those produced by the other techniques on almost all of the data sets.},
  journaltitle = {Journal of Machine Learning Research},
  date = {2008},
  pages = {2579--2605},
  keywords = {dimensionality reduction,embedding algorithms,manifold learning,multidimensional scaling,visualization},
  author = {Van Der Maaten, L J P and Hinton, G E},
  file = {/home/dimitri/Nextcloud/Zotero/storage/FBMQJSS8/Van Der Maaten, Hinton - 2008 - Visualizing Data using t-SNE.pdf},
  eprinttype = {pmid},
  eprint = {20652508}
}

@article{samerFixedParameterTractability2008,
  title = {Fixed-{{Parameter Tractability}}},
  date = {2008},
  author = {Samer, Marko and Szeider, Stefan},
  file = {/home/dimitri/Nextcloud/Zotero/storage/KXMMTXM3/SATChapter13.pdf}
}

@article{biereBoundedModelChecking2009,
  title = {Bounded Model Checking},
  volume = {185},
  issn = {09226389},
  doi = {10.3233/978-1-58603-929-5-457},
  abstract = {One of the most important industrial applications of SAT is currently Bounded Model Checking (BMC). This technique is typically used for formal hardware verification in the context of Electronic Design Automation. But BMC has successfully been applied to many other domains as well. In practice, BMC is mainly used for falsification, which is concerned with violations of temporal properties. In addition, a considerable part of this chapter discusses complete extensions, including k-induction and interpolation. These extensions also allow to prove properties. © 2009 John Franco and John Martin and IOS Press.},
  number = {1},
  journaltitle = {Frontiers in Artificial Intelligence and Applications},
  date = {2009},
  pages = {457--481},
  author = {Biere, Armin},
  file = {/home/dimitri/Nextcloud/Zotero/storage/SB4RYQKV/SATChapter14.pdf}
}

@article{biereIncompleteAlgorithmsDraft2008,
  title = {Incomplete {{Algorithms}} [Draft V2]},
  date = {2008},
  author = {Biere, Editors A and Heule, M and Maaren, H Van and Walsh, T},
  file = {/home/dimitri/Nextcloud/Zotero/storage/A35Q6CJH/SATChapter6.pdf}
}

@article{achlioptasRandomSatisfiability2009,
  title = {Random Satisfiability},
  volume = {185},
  issn = {09226389},
  doi = {10.3233/978-1-58603-929-5-245},
  abstract = {Satisfiability has received a great deal of study as the canonical NP-complete prob- lem. In the last twenty years a significant amount of this effort has been devoted to the study of randomly generated satisfiability instances and the performance of different algorithms on them. Historically, the motivation for studying random instances has been the desire to understand the hardness of “typical” instances. In fact, some early results suggested that deciding satisfiability is “easy on average”. Unfortunately, while “easy” is easy to interpret, “on average” is not. One of the earliest and most often quoted results for satisfiability being easy on average is due to Goldberg [Gol79]. In [FP83], though, Franco and Paull pointed out that the distribution of instances used in the analysis of [Gol79] is so greatly dominated by “very satisfiable” formulas that if one tries truth assignments completely at random, the expected number of trials until finding a satisfying one is O(1). Alternatively, Franco and Paull pioneered the analysis of random instances of k-SAT, i.e., asking the satisfiability question for random k- CNF formulas (defined precisely below). Among other things, they showed [FP83] that for all k ≥ 3 the DPLL algorithm needs an exponential number of steps to report all cylinders of solutions of such a formula, or that no solutions exist.},
  number = {1},
  journaltitle = {Frontiers in Artificial Intelligence and Applications},
  date = {2009},
  pages = {245--270},
  author = {Achlioptas, Dimitris},
  file = {/home/dimitri/Nextcloud/Zotero/storage/EJRV9WMI/SATChapter8.pdf}
}

@article{barrettSatisfiabilityModuloTheories2009,
  title = {Satisfiability modulo Theories},
  volume = {185},
  issn = {09226389},
  doi = {10.3233/978-1-58603-929-5-825},
  abstract = {We present a unifying framework for understanding and developing SAT-based decision procedures for Satisfiability Modulo Theories (SMT). The framework is based on a reduction of the decision problem to propositional logic by means of a deductive system. The two commonly used techniques, eager encodings (a direct reduction to propositional logic) and lazy encodings (a family of techniques based on an interplay between a SAT solver and a decision procedure) are identified as special cases. This framework offers the first generic approach for eager encodings, and a simple generalization of various lazy techniques that are found in the literature.},
  number = {1},
  journaltitle = {Frontiers in Artificial Intelligence and Applications},
  date = {2009},
  pages = {825--885},
  author = {Barrett, Clark and Sebastiani, Roberto and Seshia, Sanjit A. and Tinelli, Cesare},
  file = {/home/dimitri/Nextcloud/Zotero/storage/J6SFG3SZ/SATChapter12.pdf}
}

@article{kullmannFundamentsBranchingHeuristics2009,
  title = {Fundaments of Branching Heuristics},
  volume = {185},
  issn = {09226389},
  doi = {10.3233/978-1-58603-929-5-205},
  abstract = {"Search trees", "branching trees", "backtracking trees" or "enumeration trees" are at the heart of many (complete) approaches towards hard combinatorial problems, constraint problems, and, of course, SAT problems. Given many choices for branching, the fundamental question is how to guide the choices so that the resulting trees are (relatively) small. Despite (or perhaps because) of its apparently more narrow scope, especially in the SAT area several approaches from theory and applications have found together, and the rudiments of a theory of branching heuristics emerged. In this chapter the first systematic treatment is given. So a general theory of heuristics guiding the construction of "branching trees" is developed, ranging from a general theoretical analysis to the analysis of the historical development of branching heuristics for SAT solvers, and also to heuristics beyond SAT solving. © 2009 John Franco and John Martin and IOS Press.},
  number = {1},
  journaltitle = {Frontiers in Artificial Intelligence and Applications},
  date = {2009},
  pages = {205--244},
  author = {Kullmann, Oliver},
  file = {/home/dimitri/Nextcloud/Zotero/storage/2AM5IVWL/SATChapter7.pdf}
}

@article{darwicheCompleteAlgorithms2009,
  title = {Complete Algorithms},
  volume = {185},
  issn = {09226389},
  doi = {10.3233/978-1-58603-929-5-99},
  abstract = {Complete SAT algorithms form an important part of the SAT literature. From a theoretical perspective, complete algorithms can be used as tools for studying the complexities of different proof systems. From a practical point of view, these algorithms form the basis for tackling SAT problems arising from real-world applications. The practicality of modern, complete SAT solvers undoubtedly contributes to the growing interest in the class of complete SAT algorithms. We review these algorithms in this chapter, including Davis-Putnum resolution, Stalmarck's algorithm, symbolic SAT solving, the DPLL algorithm, and modern clause-learning SAT solvers. We also discuss the issue of certifying the answers of modern complete SAT solvers.},
  number = {1},
  journaltitle = {Frontiers in Artificial Intelligence and Applications},
  date = {2009},
  pages = {99--130},
  author = {Darwiche, Adnan and Pipatsrisawat, Knot},
  file = {/home/dimitri/Nextcloud/Zotero/storage/ESKBMUVR/SATChapter3.pdf}
}

@article{marques-silvaConflictdrivenClauseLearning2009,
  title = {Conflict-Driven Clause Learning {{SAT}} Solvers},
  volume = {185},
  issn = {09226389},
  doi = {10.3233/978-1-58603-929-5-131},
  abstract = {One of the most important paradigm shifts in the use of SAT solvers for solving industrial problems has been the introduction of clause learning. Clause learning entails adding a new clause for each conflict during backtrack search. This new clause prevents the same conflict from occurring again during the search process. Moreover, sophisticated techniques such as the identification of unique implication points in a graph of implications, allow creating clauses that more precisely identify the assignments responsible for conflicts. Learned clauses often have a large number of literals. As a result, another paradigm shift has been the development of new data structures, namely lazy data structures, which are particularly effective at handling large clauses. These data structures are called lazy due to being in general unable to provide the actual status of a clause. Efficiency concerns and the use of lazy data structures motivated the introduction of dynamic heuristics that do not require knowing the precise status of clauses. This chapter describes the ingredients of conflict-driven clause learning SAT solvers, namely conflict analysis, lazy data structures, search restarts, conflict-driven heuristics and clause deletion strategies.},
  number = {1},
  journaltitle = {Frontiers in Artificial Intelligence and Applications},
  date = {2009},
  pages = {131--153},
  author = {Marques-Silva, Joao and Lynce, Ines and Malik, Sharad},
  file = {/home/dimitri/Nextcloud/Zotero/storage/AY8GBUK7/SATChapter4.pdf}
}

@book{francoHistorySatisfiability2009,
  title = {A History of Satisfiability},
  volume = {185},
  isbn = {978-1-58603-929-5},
  pagetotal = {3–74},
  number = {1},
  date = {2009},
  author = {Franco, John and Martin, John},
  file = {/home/dimitri/Nextcloud/Zotero/storage/8PDLCNJZ/SATChapter1.pdf},
  doi = {10.3233/978-1-58603-929-5-3}
}

@article{marques-silvaImpactBranchingHeuristics1999,
  title = {The Impact of Branching Heuristics in Propositional Satisfiability Algorithms},
  volume = {1695},
  issn = {16113349},
  doi = {10.1007/3-540-48159-1_5},
  abstract = {This paper studies the practical impact of the branching heuristics used in Propositional Satisfiability (SAT) algorithms, when applied to solving real-world instances of SAT. In addition, different SAT algorithms are experimentally evaluated. The main conclusion of this study is that even though branching heuristics are crucial for solving SAT, other aspects of the organization of SAT algorithms are also essential. Moreover, we provide empirical evidence that for practical instances of SAT, the search pruning techniques included in the most competitive SAT algorithms may be of more fundamental significance than branching heuristics.},
  journaltitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  date = {1999},
  pages = {62--74},
  keywords = {Backtrack search,Branching heuristics,Propositional satisfiability},
  author = {Marques-Silva, João}
}

@inproceedings{lintaozhangEfficientConflictDriven2001,
  title = {Efficient Conflict Driven Learning in a {{Boolean}} Satisfiability Solver},
  isbn = {0-7803-7247-6},
  url = {http://ieeexplore.ieee.org/document/968634/},
  doi = {10.1109/ICCAD.2001.968634},
  abstract = {One of the most important features of current state-of-the-art SAT solvers is the use of conflict based backtracking and learning techniques. In this paper, we generalize various conflict driven learning strategies in terms of different partitioning schemes of the implication graph. We re-examine the learning techniques used in various SAT solvers and propose an array of new learning schemes. Extensive experiments with real world examples show that the best performing new learning scheme has at least a 2X speedup compared with learning schemes employed in state-of-the-art SAT solvers.},
  booktitle = {{{IEEE}}/{{ACM International Conference}} on {{Computer Aided Design}}. {{ICCAD}} 2001. {{IEEE}}/{{ACM Digest}} of {{Technical Papers}} ({{Cat}}. {{No}}.{{01CH37281}})},
  date = {2001},
  pages = {279--285},
  keywords = {CDCL,Clause Learning,Learning,SAT,UIP First UIP},
  author = {{Lintao Zhang} and Madigan, C.F. and Moskewicz, M.H. and Malik, S.},
  file = {/home/dimitri/Nextcloud/Zotero/storage/L78I4MD2/cdcl.pdf}
}

@article{mattiPreprocessingInprocessing2014,
  title = {Preprocessing (and {{Inprocessing}})},
  date = {2014},
  author = {Matti, J},
  file = {/home/dimitri/Nextcloud/Zotero/storage/LYZKRIW5/birs14-jarvisalo.pdf}
}

@incollection{duHandbookCombinatorialOptimization2013,
  title = {Handbook of {{Combinatorial Optimization}}},
  isbn = {978-1-4419-7996-4},
  url = {http://link.springer.com/10.1007/978-1-4419-7997-1},
  abstract = {Combinatorial (or discrete) optimization is one of the most active fields in the interface of operations research, computer science, and applied math ematics. Combinatorial optimization problems arise in various applications, including communications network design, VLSI design, machine vision, air line crew scheduling, corporate planning, computer-aided design and man ufacturing, database query design, cellular telephone frequency assignment, constraint directed reasoning, and computational biology. Furthermore, combinatorial optimization problems occur in many diverse areas such as linear and integer programming, graph theory, artificial intelligence, and number theory. All these problems, when formulated mathematically as the minimization or maximization of a certain function defined on some domain, have a commonality of discreteness. Historically, combinatorial optimization starts with linear programming. Linear programming has an entire range of important applications including production planning and distribution, personnel assignment, finance, alloca tion of economic resources, circuit simulation, and control systems. Leonid Kantorovich and Tjalling Koopmans received the Nobel Prize (1975) for their work on the optimal allocation of resources. Two important discover ies, the ellipsoid method (1979) and interior point approaches (1984) both provide polynomial time algorithms for linear programming. These algo rithms have had a profound effect in combinatorial optimization. Many polynomial-time solvable combinatorial optimization problems are special cases of linear programming (e.g. matching and maximum flow). In addi tion, linear programming relaxations are often the basis for many approxi mation algorithms for solving NP-hard problems (e.g. dual heuristics).},
  number = {January 1998},
  booktitle = {Satisfiability {{Problem}}: {{Theory}} and {{Applications}}},
  date = {2013},
  pages = {2359--2389},
  keywords = {0,1 2,3 40,56,6,640,7,8,9,a4 5 8,b,bdcae f0g9g,d,h,hpi0qsrut,i,irh,r,sg,t,v0w,wri g,x,xhgpirqsw v autvq wxv0i0v0way,xt,y,ybadcerutfw},
  author = {Du, Hongwei and Wu, Weili and Wu, Lidong and Xing, Kai and Zhang, Xuefei and Li, Deying},
  file = {/home/dimitri/Nextcloud/Zotero/storage/RJF7EFCG/algorithms-for-satisfiability.pdf},
  doi = {10.1007/978-1-4419-7997-1},
  eprinttype = {pmid},
  eprint = {17777135}
}

@article{kullmannSWANSEAUNIVERSITYFundaments2008,
  title = {{{SWANSEA UNIVERSITY Fundaments}} of {{Branching Heuristics}} : By {{Fundaments}} of {{Branching Heuristics}} : {{Theory}} and {{Examples}}},
  date = {2008},
  author = {Kullmann, Oliver and Kullmann, Oliver},
  file = {/home/dimitri/Nextcloud/Zotero/storage/AQADNDP7/branchingheuristics.pdf}
}

@article{malikBooleanSatisfiabilityTheoretical2009,
  title = {Boolean Satisfiability from Theoretical Hardness to Practical Success},
  volume = {52},
  issn = {00010782},
  doi = {10.1145/1536616.1536637},
  abstract = {THERE ARE MANY practical situations where we need to satisfy several potentially confl icting constraints. Simple examples of this abound in daily life, for example, determining a schedule for a series of games that resolves the availability of players and venues, or fi nding a seating assignment at dinner consistent with various rules the host would like to impose. This also applies to applications in computing, for example, ensuring that a hardware/software system functions correctly with its overall behavior constrained by the behavior of its components and their composition, or fi nding a plan for a robot to reach a goal that is consistent with the moves it can make at any step. While the applications may seem varied, at the core they all have variables whose values we need to determine (for example, the person sitting at a given seat at dinner) and constraints that these variables must satisfy (for example, the host's seating rules).},
  number = {8},
  journaltitle = {Communications of the ACM},
  date = {2009},
  pages = {76},
  author = {Malik, Sharad and Zhang, Lintao},
  file = {/home/dimitri/Nextcloud/Zotero/storage/KXA3N65N/p76-malik.pdf}
}

@article{larrosaSatisfiabilityAlgorithmsApplications2010,
  title = {Satisfiability : {{Algorithms}} , {{Applications}} and {{Extensions}}},
  date = {2010},
  pages = {95},
  author = {Larrosa, Javier and Lynce, Ines and Marques-Silva, Joao},
  file = {/home/dimitri/Nextcloud/Zotero/storage/IENVR6HN/sac10.pdf}
}

@article{vizelBooleanSatisfiabilitySolvers2015,
  title = {Boolean {{Satisfiability Solvers}} and {{Their Applications}} in {{Model Checking}}},
  volume = {103},
  issn = {00189219},
  doi = {10.1109/JPROC.2015.2455034},
  abstract = {Boolean satisfiability (SAT)-the problem of determining whether there exists an assignment satisfying a given Boolean formula-is a fundamental intractable problem in computer science. SAT has many applications in electronic design automation (EDA), notably in synthesis and verification. Consequently, SAT has received much attention from the EDA community, who developed algorithms that have had a significant impact on the performance of SAT solvers. EDA researchers introduced techniques such as conflict-driven clause learning, novel branching heuristics, and efficient unit propagation. These techniques form the basis of all modern SAT solvers. Using these ideas, contemporary SAT solvers can often handle practical instances with millions of variables and constraints. The continuing advances of SAT solvers are the driving force of modern model checking tools, which are used to check the correctness of hardware designs. Contemporary automated verification techniques such as bounded model checking, proof-based abstraction, interpolation-based model checking, and IC3 have in common that they are all based on SAT solvers and their extensions. In this paper, we trace the most important contributions made to modern SAT solvers by the EDA community, and discuss applications of SAT in hardware model checking.},
  number = {11},
  journaltitle = {Proceedings of the IEEE},
  date = {2015},
  pages = {2021--2035},
  keywords = {IC3,interpolation,model checking,proofs,satisfiability solving},
  author = {Vizel, Yakir and Weissenbacher, Georg and Malik, Sharad},
  file = {/home/dimitri/Nextcloud/Zotero/storage/EHER8QVJ/vizel.pdf}
}

@article{eenEffectivePreprocessingSAT2005,
  title = {Effective {{Preprocessing}} in {{SAT Through Variable}} and {{Clause Elimination}}},
  issn = {03029743},
  url = {http://link.springer.com/chapter/10.1007/11499107_5},
  doi = {10.1007/11499107_5},
  abstract = {Preprocessing SAT instances can reduce their size considerably. We combine variable elimination with subsumption and self-subsuming resolution, and show that these techniques not only shrink the formula further than previous preprocessing efforts based on variable elimination, but also decrease runtime of SAT solvers substantially for typical industrial SAT problems. We discuss critical implementation details that make the reduction procedure fast enough to be practical.},
  journaltitle = {Theory and Applications of Satisfiability Testing},
  date = {2005},
  pages = {61--75},
  keywords = {Algorithm Analysis and Problem Complexity,Artificial Intelligence (incl. Robotics),Mathematical Logic and Formal Languages,Mathematical Logic and Foundations,Numeric Computing,Operating Systems},
  author = {Eén, Niklas and Biere, Armin},
  file = {/home/dimitri/Nextcloud/Zotero/storage/S86K9VSH/preprocessing.pdf}
}

@article{chawlaSMOTESyntheticMinority2002,
  title = {{{SMOTE}}: {{Synthetic}} Minority over-Sampling Technique},
  volume = {16},
  issn = {10769757},
  doi = {10.1613/jair.953},
  abstract = {An approach to the construction of classifiers from imbalanced datasets is described. A dataset is imbalanced if the classification categories are not approximately equally rep-resented. Often real-world data sets are predominately composed of " normal " examples with only a small percentage of " abnormal " or " interesting " examples. It is also the case that the cost of misclassifying an abnormal (interesting) example as a normal example is often much higher than the cost of the reverse error. Under-sampling of the majority (nor-mal) class has been proposed as a good means of increasing the sensitivity of a classifier to the minority class. This paper shows that a combination of our method of over-sampling the minority (abnormal) class and under-sampling the majority (normal) class can achieve better classifier performance (in ROC space) than only under-sampling the majority class. This paper also shows that a combination of our method of over-sampling the minority class and under-sampling the majority class can achieve better classifier performance (in ROC space) than varying the loss ratios in Ripper or class priors in Naive Bayes. Our method of over-sampling the minority class involves creating synthetic minority class examples. Experiments are performed using C4.5, Ripper and a Naive Bayes classifier. The method is evaluated using the area under the Receiver Operating Characteristic curve (AUC) and the ROC convex hull strategy.},
  journaltitle = {Journal of Artificial Intelligence Research},
  date = {2002},
  pages = {321--357},
  author = {Chawla, Nitesh V. and Bowyer, Kevin W. and Hall, Lawrence O. and Kegelmeyer, W. Philip},
  file = {/home/dimitri/Nextcloud/Zotero/storage/2U6ALGSG/Chawla et al. - 2002 - SMOTE Synthetic minority over-sampling technique.pdf},
  eprinttype = {pmid},
  eprint = {18190633}
}

@article{welchIntroductionKalmanFilter2006,
  title = {An {{Introduction}} to the {{Kalman Filter}}},
  volume = {7},
  issn = {10069313},
  url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.79.6578&rep=rep1&type=pdf},
  doi = {10.1.1.117.6808},
  abstract = {In 1960, R.E. Kalman published his famous paper describing a recursive solution to the discrete-data linear filtering problem. Since that time, due in large part to advances in digital computing, the Kalman filter has been the subject of extensive research and application, particularly in the area of autonomous or assisted navigation. The Kalman filter is a set of mathematical equations that provides an efficient computational (recursive) means to estimate the state of a process, in a way that minimizes the mean of the squared error. The filter is very powerful in several aspects: it supports estimations of past, present, and even future states, and it can do so even when the precise nature of the modeled system is unknown. The purpose of this paper is to provide a practical introduction to the discrete Kalman filter. This introduction includes a description and some discussion of the basic discrete Kalman filter, a derivation, description and some discussion of the extended Kalman filter, and a relatively simple (tangible) example with real numbers \& results.},
  number = {1},
  journaltitle = {In Practice},
  date = {2006},
  pages = {1--16},
  author = {Welch, Greg and Bishop, Gary},
  file = {/home/dimitri/Nextcloud/Zotero/storage/LJ7QQCXF/Bishop, Welch - Unknown - An Introduction to the Kalman Filter.pdf},
  eprinttype = {pmid},
  eprint = {20578276}
}

@article{einickeContinuousTimeMinimumVarianceFiltering2012,
  title = {Continuous-{{Time Minimum}}-{{Variance Filtering}}},
  journaltitle = {Filtering and Prediction - Estimating The Past, Present and Future},
  date = {2012},
  pages = {49--73},
  author = {Einicke, Garry},
  file = {/home/dimitri/Nextcloud/Zotero/storage/LAPXUUZA/Unknown - Unknown - Continuous-Time Minimum-Variance Filtering 3 Continuous-Time Minimum-Variance Filtering.pdf}
}

@book{faragherUnderstandingBasisKalman2012,
  title = {Understanding the Basis of the Kalman Filter via a Simple and Intuitive Derivation [Lecture Notes]},
  volume = {29},
  isbn = {1053-5888},
  abstract = {This article provides a simple and intuitive derivation of the Kalman filter, with the aim of teaching this useful tool to students from disciplines that do not require a strong mathematical background. The most complicated level of mathematics required to understand this derivation is the ability to multiply two Gaussian functions together and reduce the result to a compact form. View full abstract},
  pagetotal = {128–132},
  number = {5},
  date = {2012},
  author = {Faragher, Ramsey},
  file = {/home/dimitri/Nextcloud/Zotero/storage/NWG5BBQF/Faragher - 2012 - Understanding the basis of the kalman filter via a simple and intuitive derivation lecture notes(2).pdf;/home/dimitri/Nextcloud/Zotero/storage/ZX4XKIDQ/Faragher - 2012 - Understanding the basis of the kalman filter via a simple and intuitive derivation lecture notes(2).pdf},
  doi = {10.1109/MSP.2012.2203621}
}

@article{alazardIntroductionAuFiltre2005,
  title = {Introduction Au Filtre de {{Kalman}}- {{Notes}} de Cours- {{Exercices}} Corrig�s-{{Sessions Matlab}}},
  date = {2005},
  author = {Alazard, D}
}

@article{pressSocietySupernaturalMedieval2013,
  title = {Society and the {{Supernatural}} : {{A Medieval Change Author}} ( s ): {{Peter Brown Source}} : {{Daedalus}} , {{Vol}} . 104 , {{No}} . 2 , {{Wisdom}} , {{Revelation}} , and {{Doubt}} : {{Perspectives}} on the {{First Millennium B}} . {{C}} . ( {{Spring}} , 1975 ), Pp . 133-151 {{Published}} by : {{The MIT Press}}},
  volume = {104},
  number = {2},
  date = {2013},
  pages = {133--151},
  author = {Press, The M I T and Academy, American},
  file = {/home/dimitri/Nextcloud/Zotero/storage/75ADTNHP/1975-brown.pdf}
}

@article{peixotoEfficientMonteCarlo2014,
  title = {Efficient {{Monte Carlo}} and Greedy Heuristic for the Inference of Stochastic Block Models},
  volume = {89},
  issn = {15393755},
  doi = {10.1103/PhysRevE.89.012804},
  abstract = {We present an efficient algorithm for the inference of stochastic block models in large networks. The algorithm can be used as an optimized Markov chain Monte Carlo (MCMC) method, with a fast mixing time and a much reduced susceptibility to getting trapped in metastable states, or as a greedy agglomerative heuristic, with an almost linear O(Nln2N)\textbackslash{}textlessmath\textbackslash{}textgreater\textbackslash{}textlessmrow\textbackslash{}textgreater\textbackslash{}textlessmi\textbackslash{}textgreaterO\textbackslash{}textlessmo\textbackslash{}textgreater(\textbackslash{}textlessmi\textbackslash{}textgreaterN\textbackslash{}textlessmsup\textbackslash{}textgreater\textbackslash{}textlessmo form="prefix"\textbackslash{}textgreaterln\textbackslash{}textlessmn\textbackslash{}textgreater2\textbackslash{}textlessmi\textbackslash{}textgreaterN\textbackslash{}textlessmo\textbackslash{}textgreater) complexity, where N\textbackslash{}textlessmath\textbackslash{}textgreater\textbackslash{}textlessmi\textbackslash{}textgreaterN is the number of nodes in the network, independent of the number of blocks being inferred. We show that the heuristic is capable of delivering results which are indistinguishable from the more exact and numerically expensive MCMC method in many artificial and empirical networks, despite being much faster. The method is entirely unbiased towards any specific mixing pattern, and in particular it does not favor assortative community structures.},
  number = {1},
  journaltitle = {Physical Review E - Statistical, Nonlinear, and Soft Matter Physics},
  date = {2014},
  author = {Peixoto, Tiago P.},
  file = {/home/dimitri/Nextcloud/Zotero/storage/7IH5MQEC/Peixoto - 2014 - Efficient Monte Carlo and greedy heuristic for the inference of stochastic block models.pdf},
  eprinttype = {pmid},
  eprint = {24580278}
}

@article{zhangFeatureExtractionEEG2008,
  title = {Feature {{Extraction}} of {{EEG Signals Using Power Spectral Entropy}}},
  volume = {2},
  doi = {10.1109/BMEI.2008.254},
  abstract = {Brain-Computer Interfaces (BCI) use electroencephalography (EEG) signals recorded from the scalp to create a new communication channel between the brain and an output device by bypassing conventional motor output pathways of nerves and muscles. One of the most important components of BCI is feature extraction of EEG signals. How to rapidly and reliably extract EEG features for expressing the brain states of different mental tasks is the crucial element for exact classification. This paper presents an approach that performs EEG feature extraction during imagined right and left hand movements by using power spectral entropy (PSE). It acquires good classification results with the time-variable linear classifier. The maximal accuracy achieves 90\%. The results show that the PSE is a sensitive parameter for EEG of imaginary hand movements. The method is simple and quick and it provides a promising method for on-line BCI system.},
  journaltitle = {2008 International Conference on BioMedical Engineering and Informatics},
  date = {2008},
  pages = {1359--1363},
  keywords = {Brain-computer interface (BCI),feature extraction,power spectral entropy (PSE),time-variable linear classifier},
  author = {Zhang, Aihua Zhang Aihua and Yang, Bin Yang Bin and Huang, Ling Huang Ling},
  file = {/home/dimitri/Nextcloud/Zotero/storage/RDCTLZLB/Zhang, Yang, Huang - Unknown - Feature Extraction of EEG Signals Using Power Spectral Entropy.pdf}
}

@article{sabatiniAssessmentWalkingFeatures2005,
  title = {Assessment of Walking Features from Foot Inertial Sensing},
  volume = {52},
  issn = {00189294},
  doi = {10.1109/TBME.2004.840727},
  abstract = {An ambulatory monitoring system is developed for the estimation of spatio-temporal gait parameters. The inertial measurement unit embedded in the system is composed of one biaxial accelerometer and one rate gyroscope, and it reconstructs the sagittal trajectory of a sensed point on the instep of the foot. A gait phase segmentation procedure is devised to determine temporal gait parameters, including stride time and relative stance; the procedure allows to define the time intervals needed for carrying an efficient implementation of the strapdown integration, which allows to estimate stride length, walking speed, and incline. The measurement accuracy of walking speed and inclines assessments is evaluated by experiments carried on adult healthy subjects walking on a motorized treadmill. Root-mean-square errors less than 0.18 km/h (speed) and 1.52\% (incline) are obtained for tested speeds and inclines varying in the intervals [3, 6] km/h and [-5, + 15]\%, respectively. Based on the results of these experiments, it is concluded that foot inertial sensing is a promising tool for the reliable identification of subsequent gait cycles and the accurate assessment of walking speed and incline.},
  number = {3},
  journaltitle = {IEEE Transactions on Biomedical Engineering},
  date = {2005},
  pages = {486--494},
  keywords = {Ambulatory measurements,Gait analysis,Inertial sensing,Uphill and downhill walking},
  author = {Sabatini, Angelo M. and Martelloni, Chiara and Scapellato, Sergio and Cavallo, Filippo},
  file = {/home/dimitri/Nextcloud/Zotero/storage/U3XDZ5NZ/Sabatini et al. - 2005 - Assessment of walking features from foot inertial sensing.pdf},
  eprinttype = {pmid},
  eprint = {15759579}
}

@article{ayrulu-erdemLegMotionClassification2011,
  title = {Leg Motion Classification with Artificial Neural Networks Using Wavelet-Based Features of Gyroscope Signals},
  volume = {11},
  issn = {14248220},
  doi = {10.3390/s110201721},
  abstract = {We extract the informative features of gyroscope signals using the discrete wavelet transform (DWT) decomposition and provide them as input to multi-layer feed-forward artificial neural networks (ANNs) for leg motion classification. Since the DWT is based on correlating the analyzed signal with a prototype wavelet function, selection of the wavelet type can influence the performance of wavelet-based applications significantly. We also investigate the effect of selecting different wavelet families on classification accuracy and ANN complexity and provide a comparison between them. The maximum classification accuracy of 97.7\% is achieved with the Daubechies wavelet of order 16 and the reverse bi-orthogonal (RBO) wavelet of order 3.1, both with similar ANN complexity. However, the RBO 3.1 wavelet is preferable because of its lower computational complexity in the DWT decomposition and reconstruction.},
  number = {2},
  journaltitle = {Sensors},
  date = {2011},
  pages = {1721--1743},
  keywords = {Accelerometers,Artificial neural networks,Discrete wavelet transform,Feature extraction,Gyroscopes,Inertial sensors,Leg motion classification,Pattern recognition,Wavelet decomposition},
  author = {Ayrulu-Erdem, Birsel and Barshan, Billur},
  file = {/home/dimitri/Nextcloud/Zotero/storage/XDQKBFJN/Ayrulu-Erdem, Barshan - 2011 - Leg motion classification with artificial neural networks using wavelet-based features of gyroscope signa.pdf},
  eprinttype = {pmid},
  eprint = {22319378}
}

@article{brezmesActivityRecognitionAccelerometer2009,
  title = {Activity Recognition from Accelerometer Data on a Mobile Phone},
  volume = {5518 LNCS},
  issn = {03029743},
  url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.92.1333&rep=rep1&type=pdf%5Cnhttp://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.92.1333},
  doi = {10.1007/978-3-642-02481-8_120},
  abstract = {Real-time monitoring of human movements can be easily envisaged as a useful tool for many purposes and future applications. This paper presents the implementation of a real-time classification system for some basic human movements using a conventional mobile phone equipped with an accelerometer. The aim of this study was to check the present capacity of conventional mobile phones to execute in real-time all the necessary pattern recognition algorithms to classify the corresponding human movements. No server processing data is involved in this approach, so the human monitoring is completely decentralized and only an additional software will be required to remotely report the human monitoring. The feasibility of this approach opens a new range of opportunities to develop new applications at a reasonable low-cost},
  issue = {PART 2},
  journaltitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  date = {2009},
  pages = {796--799},
  keywords = {Accelerometer,Pattern recognition,Human movement's detection},
  author = {Brezmes, Tomas and Gorricho, Juan Luis and Cotrina, Josep},
  file = {/home/dimitri/Nextcloud/Zotero/storage/MWYEJD5U/Ravi et al. - 2005 - Activity recognition from accelerometer data.pdf}
}

@article{krishnanAnalysisLowResolution2008,
  title = {Analysis of Low Resolution Accelerometer Data for Continuous Human Activty Recognition},
  issn = {15206149},
  url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4518365},
  doi = {10.1109/ICASSP.2008.4518365},
  abstract = {The advent of wearable sensors like accelerometers has opened a plethora\$\textbackslash{}backslash\$nof opportunities to recognize human activities from other low resolution\$\textbackslash{}backslash\$nsensory streams. In this paper we formulate recognizing activities from\$\textbackslash{}backslash\$naccelerometer data as a classification problem. In addition to the\$\textbackslash{}backslash\$nstatistical and spectral features extracted from the acceleration data,\$\textbackslash{}backslash\$nwe propose to extract features that characterize the variations in the\$\textbackslash{}backslash\$nfirst order derivative of the acceleration signal. We evaluate the\$\textbackslash{}backslash\$nperformance of different state of the art discriminative classifiers\$\textbackslash{}backslash\$nlike, boosted decision stumps (AdaBoost), support vector machines (SVM)\$\textbackslash{}backslash\$nand Regularized Logistic Regression(RLogReg) under three different\$\textbackslash{}backslash\$nevaluation scenarios(namely Subject Independent, Subject Adaptive and\$\textbackslash{}backslash\$nSubject Dependent). We propose a novel computationally inexpensive\$\textbackslash{}backslash\$nmethodology for incorporating smoothing classification temporally, that\$\textbackslash{}backslash\$ncan be coupled with any classifier with minimal training for classifying\$\textbackslash{}backslash\$ncontinuous sequences. While a 3\% increase in the classification\$\textbackslash{}backslash\$naccuracy was observed on adding the new features, the proposed technique\$\textbackslash{}backslash\$nfor continuous recognition showed a 2.5-3\% improvement in the\$\textbackslash{}backslash\$nperformance.},
  journaltitle = {ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
  date = {2008},
  pages = {3337--3340},
  keywords = {Accelerometers,AdaBoost,Human activity recognition,SVM},
  author = {Krishnan, Narayanan C. and Panchanathan, Sethuraman},
  file = {/home/dimitri/Nextcloud/Zotero/storage/6LJCKWNP/Krishnan, Panchanathan - 2008 - Analysis of low resolution accelerometer data for continuous human activity recognition.pdf}
}

@article{wuGestureRecognition3D2009,
  title = {Gesture {{Recognition}} with a 3-{{D Accelerometer}}},
  volume = {5585},
  issn = {0302-9743},
  url = {https://www.thomsoninnovation.com/tip-innovation/%5Cnhttps://www.thomsoninnovation.com/tip-innovation/recordView.do?datasource=WOK&category=LIT&selRecord=1&totalRecords=1&databaseIds=WOS&idType=uid/recordid&recordKeys=000270444600003/WOS:000270444600003},
  doi = {10.1007/978-3-642-02830-4_4},
  abstract = {Gesture-based interaction, as a natural way for human-computer interaction, has a wide range of applications in ubiquitous computing environment. This paper presents an acceleration-based gesture recognition approach, called FDSVM (Frame-based Descriptor and multi-class SVM), which needs only a wearable 3-dimensional accelerometer. With FDSVM, firstly, the acceleration data of a gesture is collected and represented by a frame-based descriptor, to extract the discriminative information. Then a SVM-based multi-class gesture classifier is built for recognition in the nonlinear gesture feature space. Extensive experimental results on a data set with 3360 gesture samples of 12 gestures over weeks demonstrate that the proposed FDSVM approach significantly outperforms other four methods: DTW, Naive Bayes, C4.5 and HMM. In the user-dependent case, FDSVM achieves the recognition rate of 99.38\% for the 4 direction gestures and 95.21\% for all the 12 gestures. In the user-independent case, it obtains the recognition rate of 98.93\% for 4 gestures and 89.29\% for 12 gestures. Compared to other accelerometer-based gesture recognition approaches reported in literature FDSVM gives the best resulrs for both user-dependent and user-independent cases.},
  journaltitle = {Ubiquitous Intelligence and Computing, Proceedings;5585: 25-38 2009},
  date = {2009},
  pages = {25--38},
  author = {Wu, J H and Pan, G and Zhang, D Q and Qi, G D and Li, S J},
  file = {/home/dimitri/Nextcloud/Zotero/storage/GQAZSX9Z/Wu et al. - 2009 - Gesture recognition with a 3-d accelerometer.pdf}
}

@article{dernbachSimpleComplexActivity2012,
  title = {Simple and {{Complex Activity Recognition}} through {{Smart Phones}}},
  doi = {10.1109/IE.2012.39},
  abstract = {Due to an increased popularity of assistive healthcare technologies activity recognition has become one of the most widely studied problems in technology-driven assistive healthcare domain. Current approaches for smart-phone based activity recognition focus only on simple activities such as locomotion. In this paper, in addition to recognizing simple activities, we investigate the ability to recognize complex activities, such as cooking, cleaning, etc. through a smart phone. Features extracted from the raw inertial sensor data of the smart phone corresponding to the user's activities, are used to train and test supervised machine learning algorithms. The results from the experiments conducted on ten participants indicate that, in isolation, while simple activities can be easily recognized, the performance of the prediction models on complex activities is poor. However, the prediction model is robust enough to recognize simple activities even in the presence of complex activities.},
  journaltitle = {2012 8th International Conference on Intelligent Environments (IE)},
  date = {2012},
  pages = {214--221},
  keywords = {-activity},
  author = {Dernbach, Stefan and Das, B. and Krishnan, Narayanan C. and Thomas, B.L. and Cook, D.J.},
  file = {/home/dimitri/Nextcloud/Zotero/storage/XG2ZD7GD/Dernbach et al. - 2012 - Simple and Complex Activity Recognition through Smart Phones.pdf}
}

@article{heActivityRecognitionAcceleration2009,
  title = {Activity Recognition from Acceleration Data Based on Discrete Consine Transform and {{SVM}}},
  issn = {1062922X},
  doi = {10.1109/ICSMC.2009.5346042},
  abstract = {This paper developed a high-accuracy human activity recognition system based on single tri-axis accelerometer for use in a naturalistic environment. This system exploits the discrete cosine transform (DCT), the Principal Component Analysis (PCA) and Support Vector Machine (SVM) for classification human different activity. First, the effective features are extracted from accelerometer data using DCT. Next, feature dimension is reduced by PCA in DCT domain. After implementing the PCA, the most invariant and discriminating information for recognition is maintained. As a consequence, Multi-class Support Vector Machines is adopted to distinguish different human activities. Experiment results show that the proposed system achieves the best accuracy is 97.51\%, which is better than other approaches.},
  issue = {October},
  journaltitle = {Conference Proceedings - IEEE International Conference on Systems, Man and Cybernetics},
  date = {2009},
  pages = {5041--5044},
  keywords = {SVM,Activity recognition,Discrete cosine transform,Principal component analysis,Tri-axial accelerometer},
  author = {He, Zhenyu and Jin, Lianwen},
  file = {/home/dimitri/Nextcloud/Zotero/storage/IIUPVJQK/He, Jin - 2009 - Activity recognition from acceleration data based on discrete consine transform and SVM.pdf}
}

@article{preeceComparisonFeatureExtraction2009,
  title = {A Comparison of Feature Extraction Methods for the Classification of Dynamic Activities from Accelerometer Data},
  volume = {56},
  issn = {00189294},
  url = {http://ieeexplore.ieee.org.ezproxy.ugm.ac.id/ielx5/10/4838904/04663615.pdf?tp=&arnumber=4663615&isnumber=4838904%0Ahttp://ieeexplore.ieee.org.ezproxy.ugm.ac.id/xpls/abs_all.jsp?arnumber=4663615%0Ahttp://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumb},
  doi = {10.1109/TBME.2008.2006190},
  abstract = {Driven by the demands on healthcare resulting from the shift toward more sedentary lifestyles, considerable effort has been devoted to the monitoring and classification of human activity. In previous studies, various classification schemes and feature extraction methods have been used to identify different activities from a range of different datasets. In this paper, we present a comparison of 14 methods to extract classification features from accelerometer signals. These are based on the wavelet transform and other well-known time- and frequency-domain signal characteristics. To allow an objective comparison between the different features, we used two datasets of activities collected from 20 subjects. The first set comprised three commonly used activities, namely, level walking, stair ascent, and stair descent, and the second a total of eight activities. Furthermore, we compared the classification accuracy for each feature set across different combinations of three different accelerometer placements. The classification analysis has been performed with robust subject-based cross-validation methods using a nearest-neighbor classifier. The findings show that, although the wavelet transform approach can be used to characterize nonstationary signals, it does not perform as accurately as frequency-based features when classifying dynamic activities performed by healthy subjects. Overall, the best feature sets achieved over 95\% intersubject classification accuracy.},
  number = {3},
  journaltitle = {IEEE Transactions on Biomedical Engineering},
  date = {2009},
  pages = {871--879},
  keywords = {Ambulatory monitoring,Activity classification,Machine learning,Wavelet transform},
  author = {Preece, Stephen J. and Goulermas, John Yannis and Kenney, Laurence P J and Howard, David},
  file = {/home/dimitri/Nextcloud/Zotero/storage/LMVNUXZ5/Preece et al. - 2009 - A Comparison of Feature Extraction Methods for the Classification of Dynamic Activities From Accelerometer Data.pdf},
  eprinttype = {pmid},
  eprint = {19272902}
}

@book{uzanPhysiqueInformationStatistique2007,
  title = {Physique , Information Statistique et Complexité Algorithmique},
  isbn = {978-2-84174-439-8},
  date = {2007},
  author = {Uzan, Pierre}
}

@article{nelsonEconomicGeographyUnited2016,
  title = {An {{Economic Geography}} of the {{United States}}: {{From Commutes}} to {{Megaregions}}},
  volume = {11},
  issn = {1932-6203},
  url = {http://dx.plos.org/10.1371/journal.pone.0166083},
  doi = {10.1371/journal.pone.0166083},
  abstract = {The emergence in the United States of large-scale " megaregions " centered on major metro-politan areas is a phenomenon often taken for granted in both scholarly studies and popular accounts of contemporary economic geography. This paper uses a data set of more than 4,000,000 commuter flows as the basis for an empirical approach to the identification of such megaregions. We compare a method which uses a visual heuristic for understanding areal aggregation to a method which uses a computational partitioning algorithm, and we reflect upon the strengths and limitations of both. We discuss how choices about input parameters and scale of analysis can lead to different results, and stress the importance of comparing computational results with " common sense " interpretations of geographic coher-ence. The results provide a new perspective on the functional economic geography of the United States from a megaregion perspective, and shed light on the old geographic problem of the division of space into areal units.},
  number = {11},
  journaltitle = {PLoS ONE},
  date = {2016},
  pages = {e0166083},
  author = {Nelson, Garrett Dash and Rae, Alasdair},
  file = {/home/dimitri/Nextcloud/Zotero/storage/TAN6RUZL/Dash Nelson, Rae - 2016 - An Economic Geography of the United States From Commutes to Megaregions.pdf}
}

@article{traudComparingCommunityStructure2008,
  title = {Comparing {{Community Structure}} to {{Characteristics}} in {{Online Collegiate Social Networks}}},
  volume = {53},
  issn = {0036-1445},
  url = {http://arxiv.org/abs/0809.0690},
  doi = {10.1137/080734315},
  abstract = {We study the structure of social networks of students by examining the graphs of Facebook "friendships" at five American universities at a single point in time. We investigate each single-institution network's community structure and employ graphical and quantitative tools, including standardized pair-counting methods, to measure the correlations between the network communities and a set of self-identified user characteristics (residence, class year, major, and high school). We review the basic properties and statistics of the pair-counting indices employed and recall, in simplified notation, a useful analytical formula for the z-score of the Rand coefficient. Our study illustrates how to examine different instances of social networks constructed in similar environments, emphasizes the array of social forces that combine to form "communities," and leads to comparative observations about online social lives that can be used to infer comparisons about offline social structures. In our illustration of this methodology, we calculate the relative contributions of different characteristics to the community structure of individual universities and subsequently compare these relative contributions at different universities, measuring for example the importance of common high school affiliation to large state universities and the varying degrees of influence common major can have on the social structure at different universities. The heterogeneity of communities that we observe indicates that these networks typically have multiple organizing factors rather than a single dominant one.},
  number = {3},
  journaltitle = {SIAM Review},
  date = {2008},
  pages = {17},
  keywords = {080734315,10,1137,2008,62h17,82-08,91-08,91d30,accepted for publication,ams subject classifications,august,community structure,contingency tables,doi,in revised form,networks,pair counting,received by the editors,september 4},
  author = {Traud, Amanda L. and Kelsic, Eric D. and Mucha, Peter J. and family=Porter, given=Mason, prefix=a., useprefix=false},
  file = {/home/dimitri/Nextcloud/Zotero/storage/7NA4GN3X/Traud et al. - 2008 - Comparing Community Structure to Characteristics in Online Collegiate Social Networks.pdf}
}

@article{traudSocialStructureFacebook2012,
  title = {Social Structure of {{Facebook}} Networks},
  volume = {391},
  issn = {03784371},
  url = {http://dx.doi.org/10.1016/j.physa.2011.12.021},
  doi = {10.1016/j.physa.2011.12.021},
  abstract = {We study the social structure of Facebook "friendship" networks at one hundred American colleges and universities at a single point in time, and we examine the roles of user attributes-gender, class year, major, high school, and residence-at these institutions. We investigate the influence of common attributes at the dyad level in terms of assortativity coefficients and regression models. We then examine larger-scale groupings by detecting communities algorithmically and comparing them to network partitions based on user characteristics. We thereby examine the relative importance of different characteristics at different institutions, finding for example that common high school is more important to the social organization of large institutions and that the importance of common major varies significantly between institutions. Our calculations illustrate how microscopic and macroscopic perspectives give complementary insights on the social organization at universities and suggest future studies to investigate such phenomena further. ?? 2012 Elsevier B.V. All rights reserved.},
  number = {16},
  journaltitle = {Physica A: Statistical Mechanics and its Applications},
  date = {2012},
  pages = {4165--4180},
  keywords = {Clustering,Community detection,Online social networks},
  author = {Traud, Amanda L. and Mucha, Peter J. and Porter, Mason A.},
  file = {/home/dimitri/Nextcloud/Zotero/storage/VFXFSE5E/Traud, Mucha, Porter - 2012 - Social structure of Facebook networks.pdf}
}

@article{jeubThinkLocallyAct2015,
  title = {Think Locally, Act Locally: {{Detection}} of Small, Medium-Sized, and Large Communities in Large Networks},
  volume = {91},
  issn = {15502376},
  doi = {10.1103/PhysRevE.91.012821},
  abstract = {It is common in the study of networks to investigate meso-scale features to try to gain an understanding of network structure and function. For example, numerous algorithms have been developed to try to identify "communities," which are typically construed as sets of nodes with denser connections internally than with the remainder of a network. In this paper, we adopt a complementary perspective that "communities" are associated with bottlenecks of locally-biased dynamical processes that begin at seed sets of nodes, and we employ several different community-identification procedures (using diffusion-based and geodesic-based dynamics) to investigate community quality as a function of community size. Using several empirical and synthetic networks, we identify several distinct scenarios for “size-resolved community structure” that can arise in real (and realistic) networks. Depending on which scenario holds, one may or may not be able to successfully identify “good” communities in a given network, the manner in which different small communities fit together to form meso-scale network structures can be very different, and processes such as viral propagation and information diffusion can exhibit very different dynamics.In addition, our results suggest that, for many large realistic networks, the output of locally-biased methods that focus on communities that are centered around a given seed node might have better conceptual grounding and greater practical utility than the output of global community-detection methods. They also illustrate subtler structural properties that are important to consider in the development of better benchmark networks to test methods for community detection. [Note: Because of space limitations in the arXiv's abstract field, this is an abridged version of the paper's abstract.]},
  number = {1},
  journaltitle = {Physical Review E - Statistical, Nonlinear, and Soft Matter Physics},
  date = {2015},
  pages = {1--29},
  author = {Jeub, Lucas G S and Balachandran, Prakash and Porter, Mason A. and Mucha, Peter J. and Mahoney, Michael W.},
  file = {/home/dimitri/Nextcloud/Zotero/storage/T2EJNZJS/Jeub et al. - 2015 - Think locally, act locally Detection of small, medium-sized, and large communities in large networks.pdf}
}

@article{shiComparisonObjectiveFunctions2010,
  title = {A {{Comparison}} of {{Objective Functions}} in {{Network Community Detection}}},
  issn = {15504786},
  doi = {10.1109/ICDMW.2010.107},
  abstract = {Community detection, as an important unsupervised learning problem in social network analysis, has attracted great interests in various research areas. Many objective functions for community detection that can capture the intuition of communities have been introduced from different research fields. Based on the classical single objective optimization framework, this paper compares a variety of these objective functions and explores the characteristics of communities they can identify. Experiments show most objective functions have the resolution limit and their communities structure have many different characteristics.},
  journaltitle = {2010 IEEE International Conference on Data Mining Workshops},
  date = {2010},
  pages = {1234--1241},
  keywords = {Community detection,multi-objective optimization,objective functions,single-objective optimization},
  author = {Shi, Chuan and Cai, Yanan and Yu, Philip S. and Yan, Zhenyu and Wu, Bin},
  file = {/home/dimitri/Nextcloud/Zotero/storage/8IBEBIY4/Shi et al. - 2010 - A Comparison of Objective Functions in Network Community Detection.pdf}
}

@article{fortunatoQualityFunctionsCommunity2007,
  title = {Quality Functions in Community Detection},
  volume = {6601},
  issn = {0277786X},
  doi = {10.1117/12.726703},
  abstract = {Community structure represents the local organization of complex networks and\$\textbackslash{}backslash\$nthe single most important feature to extract functional relationships between\$\textbackslash{}backslash\$nnodes. In the last years, the problem of community detection has been\$\textbackslash{}backslash\$nreformulated in terms of the optimization of a function, the Newman-Girvan\$\textbackslash{}backslash\$nmodularity, that is supposed to express the quality of the partitions of a\$\textbackslash{}backslash\$nnetwork into communities. Starting from a recent critical survey on modularity\$\textbackslash{}backslash\$noptimization, pointing out the existence of a resolution limit that poses\$\textbackslash{}backslash\$nsevere limits to its applicability, we discuss the general issue of the use of\$\textbackslash{}backslash\$nquality functions in community detection. Our main conclusion is that quality\$\textbackslash{}backslash\$nfunctions are useful to compare partitions with the same number of modules,\$\textbackslash{}backslash\$nwhereas the comparison of partitions with different numbers of modules is not\$\textbackslash{}backslash\$nstraightforward and may lead to ambiguities.},
  journaltitle = {Optimization},
  date = {2007},
  pages = {16--18},
  keywords = {community structure,complex networks,modularity},
  author = {Fortunato, Santo},
  file = {/home/dimitri/Nextcloud/Zotero/storage/YVJSGXTY/Fortunato - 2007 - Quality functions in community detection.pdf}
}

@article{wierzbickiAdvancesNetworkScience2016,
  title = {Advances in Network Science: 12th International Conference and School, {{NetSci}}-{{X}} 2016 {{Wroclaw}}, {{Poland}}, {{January}} 11–13, 2016 Proceedings},
  volume = {9564},
  issn = {16113349},
  doi = {10.1007/978-3-319-28361-6},
  abstract = {Ciencia de las redes es una disciplina emergente se ocupa del estudio de los modelos de red en dominios que van desde la biología y la física con la informática, de los mercados financieros para la integración cultural y de medios de comunicación social a las enfermedades infecciosas. También es una herramienta esencial en la comprensión de muchos tipos de datos grandes, dando lugar a numerosas aplicaciones prácticas. Los modelos de red ayudan a los investigadores y los profesionales tienen sentido de un mundo cada vez más complejo, especialmente en relación con los fenómenos sociales mediadas a través de tecnología de la información. Este volumen contiene varias contribuciones a la investigación en el área de la ciencia de las redes, seleccionados de las mejores presentaciones de la conferencia NetSci X-2016. La tasa de aceptación de conferencias para las comunicaciones completas fue del 20\%. La Conferencia Internacional y la Facultad de Ciencias de la red (NetSci) es un evento interdisciplinario, reuniendo todos los investigadores interesados \mbox\mbox{}en la ciencia de las redes. Después de 11 ediciones, la conferencia es el evento más grande y más conocida de la zona. Publicado por primera vez en los Lecture Notes in Computer Science serie, el volumen conserva el carácter interdisciplinario de la ciencia a la red, al tiempo que subraya su conexión con la informática. Las obras de los investigadores de diversos orígenes, tales como las ciencias sociales, biología, economía y ciencias de la computación, se unen en el objetivo de una mejor comprensión de las redes complejas. El desarrollo de mejores modelos de fenómenos complejos, como las redes complejas, es en sí mismo una importante contribución a la informática. El uso de tales modelos computacionales puede mejorar la tecnología de la información existente, así como ampliar el alcance de las aplicaciones de la tecnología de la información en nuevas áreas. Por esta razón, el estudio de la ciencia de las redes puede ser beneficioso para los informáticos, y los avances en la ciencia de las redes puede ser considerado como los avances en la informática.},
  journaltitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  date = {2016},
  pages = {111--125},
  keywords = {community detection,quality functions,social networks},
  author = {Wierzbicki, Adam and Brandes, Ulrik and Schweitzer, Frank and Pedreschi, Dino},
  file = {/home/dimitri/Nextcloud/Zotero/storage/XGC4LBZI/Wierzbicki et al. - 2016 - Advances in network science 12th international conference and school, NetSci-X 2016 Wroclaw, Poland, Januar.pdf}
}

@article{blondelFastUnfoldingCommunities2008,
  title = {Fast Unfolding of Communities in Large Networks},
  volume = {10008},
  issn = {1742-5468},
  url = {http://arxiv.org/abs/0803.0476},
  doi = {10.1088/1742-5468/2008/10/P10008},
  abstract = {We propose a simple method to extract the community structure of large networks. Our method is a heuristic method that is based on modularity optimization. It is shown to outperform all other known community detection method in terms of computation time. Moreover, the quality of the communities detected is very good, as measured by the so-called modularity. This is shown first by identifying language communities in a Belgian mobile phone network of 2.6 million customers and by analyzing a web graph of 118 million nodes and more than one billion links. The accuracy of our algorithm is also verified on ad-hoc modular networks. .},
  number = {10},
  journaltitle = {Journal of Statistical Mechanics: Theory and Experiment},
  date = {2008},
  pages = {6},
  keywords = {networks,critical phenomena of socio-economic,fast unfolding of communities,in large networks,random graphs,socio-economic networks,systems},
  author = {Blondel, Vincent D. and Guillaume, Jean-Loup and Lambiotte, Renaud and Lefebvre, Etienne},
  file = {/home/dimitri/Nextcloud/Zotero/storage/UHWIV34V/Blondel et al. - 2008 - Fast unfolding of communities in large networks.pdf},
  eprinttype = {pmid},
  eprint = {260529900010}
}

@article{fortunatoCommunityDetectionNetworks2016,
  title = {Community Detection in Networks: {{A}} User Guide},
  volume = {659},
  issn = {03701573},
  url = {http://arxiv.org/abs/1608.00163},
  doi = {10.1016/j.physrep.2016.09.002},
  abstract = {Community detection in networks is one of the most popular topics of modern network science. Communities, or clusters, are usually groups of vertices having higher probability of being connected to each other than to members of other groups, though other patterns are possible. Identifying communities is an ill-defined problem. There are no universal protocols on the fundamental ingredients, like the definition of community itself, nor on other crucial issues, like the validation of algorithms and the comparison of their performances. This has generated a number of confusions and misconceptions, which undermine the progress in the field. We offer a guided tour through the main aspects of the problem. We also point out strengths and weaknesses of popular methods, and give directions to their use.},
  journaltitle = {Physics Reports},
  date = {2016},
  pages = {1--44},
  keywords = {Clustering,Communities,Networks},
  author = {Fortunato, Santo and Hric, Darko},
  file = {/home/dimitri/Nextcloud/Zotero/storage/X7YDNYNQ/Fortunato, Hric - 2016 - Community detection in networks A user guide.pdf}
}

@article{fortunatoCommunityStructureGraphs2012,
  title = {Community Structure in Graphs},
  volume = {9781461418},
  issn = {978-0-387-75888-6},
  doi = {10.1007/978-1-4614-1800-9_33},
  abstract = {Graph vertices are often organized into groups that seem to live fairly independently of the rest of the graph, with which they share but a few edges, whereas the relationships between group members are stronger, as shown by the large number of mutual connections. Such groups of vertices, or communities, can be considered as independent compartments of a graph. Detecting communities is of great importance in sociology, biology and computer science, disciplines where systems are often represented as graphs. The task is very hard, though, both conceptually, due to the ambiguity in the definition of community and in the discrimination of different partitions and practically, because algorithms must find “good” partitions among an exponentially large number of them. Other complications are represented by the possible occurrence of hierarchies, i.e. communities which are nested inside larger communities, and by the existence of overlaps between communities, due to the presence of nodes belonging to more groups. All these aspects are dealt with in some detail and many methods are described, from traditional approaches used in computer science and sociology to recent techniques developed mostly within statistical physics.},
  journaltitle = {Computational Complexity: Theory, Techniques, and Applications},
  date = {2012},
  pages = {490--512},
  author = {Fortunato, Santo and Castellano, Claudio},
  file = {/home/dimitri/Nextcloud/Zotero/storage/XULWRSER/Fortunato, Castellano - 2012 - Community structure in graphs.pdf},
  eprinttype = {pmid},
  eprint = {25246403}
}

@article{morupBayesianCommunityDetection2012,
  title = {Bayesian {{Community Detection}}},
  volume = {24},
  issn = {0899-7667},
  doi = {10.1162/NECO_a_00314},
  abstract = {Many networks of scientific interest naturally decompose into clusters or communities with comparatively fewer external than internal links; however, current Bayesian models of network communities do not exert this intuitive notion of communities. We formulate a nonparametric Bayesian model for community detection consistent with an intuitive definition of communities and present a Markov chain Monte Carlo procedure for inferring the community structure. A Matlab toolbox with the proposed inference procedure is available for download. On synthetic and real networks, our model detects communities consistent with ground truth, and on real networks, it outperforms existing approaches in predicting missing links. This suggests that community structure is an important structural property of networks that should be explicitly modeled.},
  number = {9},
  journaltitle = {Neural Computation},
  date = {2012},
  pages = {2434--2456},
  keywords = {complex networks,modularity,community detection,infinite relational model,normalized cut,stochastic block-model},
  author = {Mørup, Morten and Schmidt, Mikkel N.},
  eprinttype = {pmid},
  eprint = {22509971}
}

@article{yangDefiningEvaluatingNetwork2015,
  title = {Defining and Evaluating Network Communities Based on Ground-Truth},
  volume = {42},
  issn = {02193116},
  doi = {10.1007/s10115-013-0693-z},
  abstract = {Nodes in real-world networks organize into densely linked communities where edges appear with high concentration among the members of the community. Identifying such communities of nodes has proven to be a challenging task due to a plethora of defin-itions of network communities, intractability of methods for detecting them, and the issues with evaluation which stem from the lack of a reliable gold-standard ground-truth. In this paper, we distinguish between structural and functional definitions of network communities. Structural definitions of communities are based on connectivity patterns, like the density of connections between the community members, while functional definitions are based on (often unobserved) common function or role of the community members in the network. We argue that the goal of network community detection is to extract functional commu-nities based on the connectivity structure of the nodes in the network. We then identify networks with explicitly labeled functional communities to which we refer as ground-truth communities. In particular, we study a set of 230 large real-world social, collaboration, and information networks where nodes explicitly state their community memberships. For exam-ple, in social networks, nodes explicitly join various interest-based social groups. We use such social groups to define a reliable and robust notion of ground-truth communities. We then propose a methodology, which allows us to compare and quantitatively evaluate how different structural definitions of communities correspond to ground-truth functional com-munities. We study 13 commonly used structural definitions of communities and examine their sensitivity, robustness and performance in identifying the ground-truth. We show that the 13 structural definitions are heavily correlated and naturally group into four classes. We find that two of these definitions, Conductance and Triad participation ratio, consistently give the best performance in identifying ground-truth communities. We also investigate a task of detecting communities given a single seed node. We extend the local spectral cluster-ing algorithm into a heuristic parameter-free community detection method that easily scales to networks with more than 100 million nodes. The proposed method achieves 30 \% relative improvement over current local clustering methods.},
  number = {1},
  journaltitle = {Knowledge and Information Systems},
  date = {2015},
  pages = {181--213},
  keywords = {Community detection,Community scoring function,Ground-truth communities,Modularity,Network communities},
  author = {Yang, Jaewon and Leskovec, Jure},
  file = {/home/dimitri/Nextcloud/Zotero/storage/GREEW4BK/Yang, Leskovec - 2015 - Defining and evaluating network communities based on ground-truth.pdf}
}

@article{fortunatoCommunityDetectionGraphs2010,
  title = {Community Detection in Graphs},
  volume = {486},
  issn = {03701573},
  doi = {10.1016/j.physrep.2009.11.002},
  abstract = {The modern science of networks has brought significant advances to our understanding of complex systems. One of the most relevant features of graphs representing real systems is community structure, or clustering, i.e. the organization of vertices in clusters, with many edges joining vertices of the same cluster and comparatively few edges joining vertices of different clusters. Such clusters, or communities, can be considered as fairly independent compartments of a graph, playing a similar role like, e.g., the tissues or the organs in the human body. Detecting communities is of great importance in sociology, biology and computer science, disciplines where systems are often represented as graphs. This problem is very hard and not yet satisfactorily solved, despite the huge effort of a large interdisciplinary community of scientists working on it over the past few years. We will attempt a thorough exposition of the topic, from the definition of the main elements of the problem, to the presentation of most methods developed, with a special focus on techniques designed by statistical physicists, from the discussion of crucial issues like the significance of clustering and how methods should be tested and compared against each other, to the description of applications to real networks. ?? 2009 Elsevier B.V.},
  number = {3-5},
  journaltitle = {Physics Reports},
  date = {2010},
  pages = {75--174},
  keywords = {Clusters,Graphs,Statistical physics},
  author = {Fortunato, Santo},
  file = {/home/dimitri/Nextcloud/Zotero/storage/BAAZPFRK/Fortunato - 2010 - Community detection in graphs.pdf},
  eprinttype = {pmid},
  eprint = {22166104}
}

@article{newmanMixingPatternsNetworks2003,
  title = {Mixing Patterns in Networks},
  volume = {67},
  issn = {1063-651X},
  doi = {10.1103/PhysRevE.67.026126},
  abstract = {We study assortative mixing in networks, the tendency for vertices in networks to be connected to other vertices that are like (or unlike) them in some way. We consider mixing according to discrete characteristics such as language or race in social networks and scalar characteristics such as age. As a special example of the latter we consider mixing according to vertex degree, i.e., according to the number of connections vertices have to other vertices: do gregarious people tend to associate with other gregarious people? We propose a number of measures of assortative mixing appropriate to the various mixing types, and apply them to a variety of real-world networks, showing that assortative mixing is a pervasive phenomenon found in many networks. We also propose several models of assortatively mixed networks, both analytic ones based on generating function methods, and numerical ones based on Monte Carlo graph generation techniques. We use these models to probe the properties of networks as their level of assortativity is varied. In the particular case of mixing by degree, we find strong variation with assortativity in the connectivity of the network and in the resilience of the network to the removal of vertices.},
  number = {2},
  journaltitle = {Physical Review E},
  date = {2003},
  pages = {026126},
  author = {Newman, M. E. J.},
  file = {/home/dimitri/Nextcloud/Zotero/storage/DSKPB8IT/Newman - 2003 - Mixing patterns in networks.pdf},
  eprinttype = {pmid},
  eprint = {12636767}
}

@article{newmanEstimatingNumberCommunities2016,
  title = {Estimating the {{Number}} of {{Communities}} in a {{Network}}},
  volume = {117},
  issn = {10797114},
  doi = {10.1103/PhysRevLett.117.078301},
  abstract = {Community detection, the division of a network into dense subnetworks with only sparse connections between them, has been a topic of vigorous study in recent years. However, while there exist a range of powerful and flexible methods for dividing a network into a specified number of communities, it is an open question how to determine exactly how many communities one should use. Here we describe a mathematically principled approach for finding the number of communities in a network using a maximum-likelihood method. We demonstrate this approach on a range of real-world examples with known community structure, finding that it is able to determine the number of communities correctly in every case.},
  number = {7},
  journaltitle = {Physical Review Letters},
  date = {2016},
  author = {Newman, M. E J and Reinert, Gesine},
  file = {/home/dimitri/Nextcloud/Zotero/storage/H3AZ34BL/Newman, Reinert - 2016 - Estimating the Number of Communities in a Network.pdf},
  eprinttype = {pmid},
  eprint = {27564002}
}

@article{newmanRandomGraphsArbitrary2001,
  title = {Random Graphs with Arbitrary Degree Distributions and Their Applications.},
  volume = {64},
  issn = {1063-651X},
  doi = {10.1103/PhysRevE.64.026118},
  abstract = {Recent work on the structure of social networks and the internet has focused attention on graphs with distributions of vertex degree that are significantly different from the Poisson degree distributions that have been widely studied in the past. In this paper we develop in detail the theory of random graphs with arbitrary degree distributions. In addition to simple undirected, unipartite graphs, we examine the properties of directed and bipartite graphs. Among other results, we derive exact expressions for the position of the phase transition at which a giant component first forms, the mean component size, the size of the giant component if there is one, the mean number of vertices a certain distance away from a randomly chosen vertex, and the average vertex-vertex distance within a graph. We apply our theory to some real-world graphs, including the world-wide web and collaboration graphs of scientists and Fortune 1000 company directors. We demonstrate that in some cases random graphs with appropriate distributions of vertex degree predict with surprising accuracy the behavior of the real world, while in others there is a measurable discrepancy between theory and reality, perhaps indicating the presence of additional social structure in the network that is not captured by the random graph.},
  issue = {2 Pt 2},
  journaltitle = {Physical review. E, Statistical, nonlinear, and soft matter physics},
  date = {2001},
  pages = {026118},
  author = {Newman, M E and Strogatz, S H and Watts, D J},
  file = {/home/dimitri/Nextcloud/Zotero/storage/M9TZK4VP/Newman, Strogatz, Watts - 2001 - Random graphs with arbitrary degree distributions and their applications.pdf},
  eprinttype = {pmid},
  eprint = {11497662}
}

@article{girvanCommunityStructureSocial2002,
  title = {Community Structure in Social and Biological Networks},
  volume = {99},
  issn = {0027-8424},
  url = {http://arxiv.org/abs/cond-mat/0112110},
  doi = {10.1073/pnas.},
  abstract = {A number of recent studies have focused on the statistical properties of networked systems such as social networks and the Worldwide Web. Researchers have concentrated particularly on a few properties that seem to be common to many networks: the small-world property, power-law degree distributions, and network transitivity. In this article, we highlight another property that is found in many networks, the property of community structure, in which network nodes are joined together in tightly knit groups, between which there are only looser connections. We propose a method for detecting such communities, built around the idea of using centrality indices to find community boundaries. We test our method on computer-generated and real-world graphs whose community structure is already known and find that the method detects this known structure with high sensitivity and reliability. We also apply the method to two networks whose community structure is not well known-a collaboration network and a food web-and find that it detects significant and informative community divisions in both cases.},
  number = {12},
  journaltitle = {Proceedings of the National Academy of Sciences of United States of America},
  date = {2002},
  pages = {7821--7826},
  keywords = {Models,Neural Networks (Computer),Theoretical,Algorithms,Animals,Community Networks,Computer Simulation,Humans,Nerve Net,Nerve Net: physiology,Social Behavior},
  author = {Girvan, M. and Newman, M. E. J},
  file = {/home/dimitri/Nextcloud/Zotero/storage/X3C73Q5I/Girvan, Newman - 2002 - Community structure in social and biological networks.pdf},
  eprinttype = {pmid},
  eprint = {12060727}
}

@article{newmanGeneralizedCommunitiesNetworks2015,
  title = {Generalized {{Communities}} in {{Networks}}},
  volume = {115},
  issn = {10797114},
  doi = {10.1103/PhysRevLett.115.088701},
  abstract = {A substantial volume of research is devoted to studies of community structure in networks, but communities are not the only possible form of large-scale network structure. Here, we describe a broad extension of community structure that encompasses traditional communities but includes a wide range of generalized structural patterns as well. We describe a principled method for detecting this generalized structure in empirical network data and demonstrate with real-world examples how it can be used to learn new things about the shape and meaning of networks.},
  number = {8},
  journaltitle = {Physical Review Letters},
  date = {2015},
  author = {Newman, M. E J and Peixoto, Tiago P.},
  file = {/home/dimitri/Nextcloud/Zotero/storage/EVZH8XTZ/Newman, Peixoto - 2015 - Generalized Communities in Networks.pdf},
  eprinttype = {pmid},
  eprint = {26340218}
}

@article{lovaszRandomWalksGraphs1993,
  title = {Random Walks on Graphs: {{A}} Survey},
  volume = {2},
  issn = {03044149},
  url = {http://www.cs.yale.edu/publications/techreports/tr1029.pdf},
  doi = {10.1.1.39.2847},
  abstract = {This paper we'll formulate the results in terms of random walks, and mostly restrict our attention to the undirected case. 2 L. Lov'asz},
  issue = {Volume 2},
  journaltitle = {Combinatorics Paul Erdos is Eighty},
  date = {1993},
  pages = {1--46},
  author = {Lovász, L}
}

@article{newmanAnalysisWeightedNetworks2004,
  title = {Analysis of Weighted Networks},
  volume = {70},
  issn = {15393755},
  doi = {10.1103/PhysRevE.70.056131},
  abstract = {The connections in many networks are not merely binary entities, either present or not, but have associated weights that record their strengths relative to one another. Recent studies of networks have, by and large, steered clear of such weighted networks, which are often perceived as being harder to analyze than their unweighted counterparts. Here we point out that weighted networks can in many cases be analyzed using a simple mapping from a weighted network to an unweighted multigraph, allowing us to apply standard techniques for unweighted graphs to weighted ones as well. We give a number of examples of the method, including an algorithm for detecting community structure in weighted networks and a simple proof of the maximum-flow-minimum-cut theorem.},
  issue = {5 2},
  journaltitle = {Physical Review E - Statistical, Nonlinear, and Soft Matter Physics},
  date = {2004},
  author = {Newman, M. E J},
  file = {/home/dimitri/Nextcloud/Zotero/storage/Q8YDE5VC/Newman - 2004 - Analysis of weighted networks.pdf},
  eprinttype = {pmid},
  eprint = {15600716}
}

@article{newmanFastAlgorithmDetecting2004,
  title = {Fast Algorithm for Detecting Community Structure in Networks},
  volume = {69},
  issn = {15393755},
  doi = {10.1103/PhysRevE.69.066133},
  abstract = {Many networks display community structure–groups of vertices within which connections are dense but between which they are sparser–and sensitive computer algorithms have in recent years been developed for detecting this structure. These algorithms, however, are computationally demanding, which limits their application to small networks. Here we describe an algorithm which gives excellent results when tested on both computer-generated and real-world networks and is much faster, typically thousands of times faster, than previous algorithms. We give several example applications, including one to a collaboration network of more than 50,000 physicists.},
  issue = {6 2},
  journaltitle = {Physical Review E - Statistical, Nonlinear, and Soft Matter Physics},
  date = {2004},
  author = {Newman, M. E J},
  file = {/home/dimitri/Nextcloud/Zotero/storage/9CKYMKCY/Newman - 2004 - Fast algorithm for detecting community structure in networks.pdf},
  eprinttype = {pmid},
  eprint = {15244693}
}

@article{vonluxburgTutorialSpectralClustering2007,
  title = {A Tutorial on Spectral Clustering},
  volume = {17},
  issn = {09603174},
  url = {http://www.springerlink.com/index/10.1007/s11222-007-9033-z},
  doi = {10.1007/s11222-007-9033-z},
  abstract = {In recent years, spectral clustering has become one of the most popular modern clustering algorithms. It is simple to implement, can be solved efficiently by standard linear algebra software, and very often outperforms traditional clustering algorithms such as the k-means algorithm. Nevertheless, on the first glance spectral clustering looks a bit mysterious, and it is not obvious to see why it works at all and what it really does. This article is a tutorial introduction to spectral clustering. We describe different graph Laplacians and their basic properties, present the most common spectral clustering algorithms, and derive those algorithms from scratch by several different approaches. Advantages and disadvantages of the different spectral clustering algorithms are discussed.},
  number = {4},
  journaltitle = {Statistics and Computing},
  date = {2007},
  pages = {395--416},
  keywords = {Graph Laplacian,Spectral clustering},
  author = {Von Luxburg, Ulrike},
  file = {/home/dimitri/Nextcloud/Zotero/storage/274AGM44/Luxburg - 2006 - A Tutorial on Spectral Clustering A Tutorial on Spectral Clustering.pdf},
  eprinttype = {pmid},
  eprint = {19784854}
}

@article{newmanModularityCommunityStructure2006,
  title = {Modularity and Community Structure in Networks},
  volume = {103},
  issn = {0027-8424},
  url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=1482622&tool=pmcentrez&rendertype=abstract%5Cnhttp://www.pnas.org/content/103/23/8577.short},
  doi = {10.1073/pnas.0601602103},
  abstract = {Many networks of interest in the sciences, including social networks, computer networks, and metabolic and regulatory networks, are found to divide naturally into communities or modules. The problem of detecting and characterizing this community structure is one of the outstanding issues in the study of networked systems. One highly effective approach is the optimization of the quality function known as "modularity" over the possible divisions of a network. Here I show that the modularity can be expressed in terms of the eigenvectors of a characteristic matrix for the network, which I call the modularity matrix, and that this expression leads to a spectral algorithm for community detection that returns results of demonstrably higher quality than competing methods in shorter running times. I illustrate the method with applications to several published network data sets.},
  number = {23},
  journaltitle = {Proceedings of the National Academy of Sciences},
  date = {2006},
  pages = {8577},
  author = {Newman, M.E.J.},
  file = {/home/dimitri/Nextcloud/Zotero/storage/KB8M7HT8/Newman - 2006 - Modularity and community structure in networks.pdf},
  eprinttype = {pmid},
  eprint = {16723398}
}

@article{ricci-tersenghiPerformanceCommunityDetection2016,
  title = {Performance of a Community Detection Algorithm Based on Semidefinite Programming},
  volume = {699},
  issn = {1742-6588},
  url = {http://arxiv.org/abs/1603.09045%5Cnhttp://stacks.iop.org/1742-6596/699/i=1/a=012015?key=crossref.252ca19dc33209455e53a35dcdd31edc},
  doi = {10.1088/1742-6596/699/1/012015},
  abstract = {The problem of detecting communities in a graph is maybe one the most studied inference problems, given its simplicity and widespread diffusion among several disciplines. A very common benchmark for this problem is the stochastic block model or planted partition problem, where a phase transition takes place in the detection of the planted partition by changing the signal-to-noise ratio. Optimal algorithms for the detection exist which are based on spectral methods, but we show these are extremely sensible to slight modification in the generative model. Recently Javanmard, Montanari and Ricci-Tersenghi (arXiv:1511.08769) have used statistical physics arguments, and numerical simulations to show that finding communities in the stochastic block model via semidefinite programming is quasi optimal. Further, the resulting semidefinite relaxation can be solved efficiently, and is very robust with respect to changes in the generative model. In this paper we study in detail several practical aspects of this new algorithm based on semidefinite programming for the detection of the planted partition. The algorithm turns out to be very fast, allowing the solution of problems with \$O(10\^5)\$ variables in few second on a laptop computer.},
  journaltitle = {Journal of Physics: Conference Series},
  date = {2016},
  pages = {012015},
  author = {Ricci-Tersenghi, Federico and Javanmard, Adel and Montanari, Andrea},
  file = {/home/dimitri/Nextcloud/Zotero/storage/5TQSHQGW/Ricci-Tersenghi, Javanmard, Montanari - 2016 - Performance of a community detection algorithm based on semidefinite programming.pdf}
}

@article{decelleAsymptoticAnalysisStochastic2011,
  title = {Asymptotic Analysis of the Stochastic Block Model for Modular Networks and Its Algorithmic Applications},
  volume = {84},
  issn = {15393755},
  doi = {10.1103/PhysRevE.84.066106},
  abstract = {In this paper we extend our previous work on the stochastic block model, a commonly used generative model for social and biological networks, and the problem of inferring functional groups or communities from the topology of the network. We use the cavity method of statistical physics to obtain an asymptotically exact analysis of the phase diagram. We describe in detail properties of the detectability/undetectability phase transition and the easy/hard phase transition for the community detection problem. Our analysis translates naturally into a belief propagation algorithm for inferring the group memberships of the nodes in an optimal way, i.e., that maximizes the overlap with the underlying group memberships, and learning the underlying parameters of the block model. Finally, we apply the algorithm to two examples of real-world networks and discuss its performance.},
  number = {6},
  journaltitle = {Physical Review E - Statistical, Nonlinear, and Soft Matter Physics},
  date = {2011},
  author = {Decelle, Aurelien and Krzakala, Florent and Moore, Cristopher and Zdeborová, Lenka},
  file = {/home/dimitri/Nextcloud/Zotero/storage/IMMHHTS5/Decelle et al. - 2011 - Asymptotic analysis of the stochastic block model for modular networks and its algorithmic applications.pdf},
  eprinttype = {pmid},
  eprint = {22304154}
}

@article{gopalanEfficientDiscoveryOverlapping2013,
  title = {Efficient Discovery of Overlapping Communities in Massive Networks.},
  volume = {110},
  issn = {1091-6490},
  url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3767539&tool=pmcentrez&rendertype=abstract},
  doi = {10.1073/pnas.1221839110},
  abstract = {Detecting overlapping communities is essential to analyzing and exploring natural networks such as social networks, biological networks, and citation networks. However, most existing approaches do not scale to the size of networks that we regularly observe in the real world. In this paper, we develop a scalable approach to community detection that discovers overlapping communities in massive real-world networks. Our approach is based on a Bayesian model of networks that allows nodes to participate in multiple communities, and a corresponding algorithm that naturally interleaves subsampling from the network and updating an estimate of its communities. We demonstrate how we can discover the hidden community structure of several real-world networks, including 3.7 million US patents, 575,000 physics articles from the arXiv preprint server, and 875,000 connected Web pages from the Internet. Furthermore, we demonstrate on large simulated networks that our algorithm accurately discovers the true community structure. This paper opens the door to using sophisticated statistical models to analyze massive networks.},
  number = {36},
  journaltitle = {Proceedings of the National Academy of Sciences of the United States of America},
  date = {2013},
  pages = {14534--9},
  keywords = {Bayes Theorem,Models,Algorithms,Community Networks,Computer Simulation,Humans,Social Behavior,Statistical,Stochastic Processes},
  author = {Gopalan, Prem K and Blei, David M},
  file = {/home/dimitri/Nextcloud/Zotero/storage/ZBDI3YHZ/Gopalan, Blei - 2013 - Efficient discovery of overlapping communities in massive networks.pdf},
  eprinttype = {pmid},
  eprint = {23950224}
}

@article{newmanFindingCommunityStructure2006,
  title = {Finding Community Structure in Networks Using the Eigenvectors of Matrices},
  volume = {74},
  issn = {15393755},
  doi = {10.1103/PhysRevE.74.036104},
  abstract = {We consider the problem of detecting communities or modules in networks, groups of vertices with a higher-than-average density of edges connecting them. Previous work indicates that a robust approach to this problem is the maximization of the benefit function known as "modularity" over possible divisions of a network. Here we show that this maximization process can be written in terms of the eigenspectrum of a matrix we call the modularity matrix, which plays a role in community detection similar to that played by the graph Laplacian in graph partitioning calculations. This result leads us to a number of possible algorithms for detecting community structure, as well as several other results, including a spectral measure of bipartite structure in networks and a centrality measure that identifies vertices that occupy central positions within the communities to which they belong. The algorithms and measures proposed are illustrated with applications to a variety of real-world complex networks.},
  number = {3},
  journaltitle = {Physical Review E - Statistical, Nonlinear, and Soft Matter Physics},
  date = {2006},
  author = {Newman, M. E J},
  file = {/home/dimitri/Nextcloud/Zotero/storage/TJLPZTK4/Newman - 2006 - Finding community structure in networks using the eigenvectors of matrices.pdf},
  eprinttype = {pmid},
  eprint = {17025705}
}

@article{newmanStructureInferenceAnnotated2016,
  title = {Structure and Inference in Annotated Networks},
  volume = {7},
  issn = {2041-1723},
  url = {http://arxiv.org/abs/1507.04001},
  doi = {10.1038/ncomms11863},
  abstract = {For many networks of scientific interest we know both the connections of the network and information about the network nodes, such as the age or gender of individuals in a social network, geographic location of nodes in the Internet, or cellular function of nodes in a gene regulatory network. Here we demonstrate how this "metadata" can be used to improve our analysis and understanding of network structure. We focus in particular on the problem of community detection in networks and develop a mathematically principled approach that combines a network and its metadata to detect communities more accurately than can be done with either alone. Crucially, the method does not assume that the metadata are correlated with the communities we are trying to find. Instead the method learns whether a correlation exists and correctly uses or ignores the metadata depending on whether they contain useful information. The learned correlations are also of interest in their own right, allowing us to make predictions about the community membership of nodes whose network connections are unknown. We demonstrate our method on synthetic networks with known structure and on real-world networks, large and small, drawn from social, biological, and technological domains.},
  issue = {May},
  journaltitle = {Nature Communications},
  date = {2016},
  pages = {1--16},
  author = {Newman, M. E. J. and Clauset, Aaron},
  file = {/home/dimitri/Nextcloud/Zotero/storage/6MYFQBWB/Newman, Clauset - 2016 - Structure and inference in annotated networks.pdf},
  eprinttype = {pmid},
  eprint = {27306566}
}

@article{martinStructuralInferenceUncertain2016,
  title = {Structural Inference for Uncertain Networks},
  volume = {93},
  issn = {15502376},
  doi = {10.1103/PhysRevE.93.012306},
  abstract = {In the study of networked systems such as biological, technological, and social networks the available data are often uncertain. Rather than knowing the structure of a network exactly, we know the connections between nodes only with a certain probability. In this paper we develop methods for the analysis of such uncertain data, focusing particularly on the problem of community detection. We give a principled maximum-likelihood method for inferring community structure and demonstrate how the results can be used to make improved estimates of the true structure of the network. Using computer-generated benchmark networks we demonstrate that our methods are able to reconstruct known communities more accurately than previous approaches based on data thresholding. We also give an example application to the detection of communities in a protein-protein interaction network.},
  number = {1},
  journaltitle = {Physical Review E - Statistical, Nonlinear, and Soft Matter Physics},
  date = {2016},
  author = {Martin, Travis and Ball, Brian and Newman, M. E J},
  file = {/home/dimitri/Nextcloud/Zotero/storage/T832RGMI/Martin, Ball, Newman - 2016 - Structural inference for uncertain networks.pdf},
  eprinttype = {pmid},
  eprint = {26871091}
}

@book{malliarosClusteringCommunityDetection2013,
  title = {Clustering and Community Detection in Directed Networks: {{A}} Survey},
  volume = {533},
  isbn = {0370-1573},
  abstract = {Networks (or graphs) appear as dominant structures in diverse domains, including sociology, biology, neuroscience and computer science. In most of the aforementioned cases graphs are directed - in the sense that there is directionality on the edges, making the semantics of the edges nonsymmetric as the source node transmits some property to the target one but not vice versa. An interesting feature that real networks present is the clustering or community structure property, under which the graph topology is organized into modules commonly called communities or clusters. The essence here is that nodes of the same community are highly similar while on the contrary, nodes across communities present low similarity. Revealing the underlying community structure of directed complex networks has become a crucial and interdisciplinary topic with a plethora of relevant application domains. Therefore, naturally there is a recent wealth of research production in the area of mining directed graphs - with clustering being the primary method sought and the primary tool for community detection and evaluation. The goal of this paper is to offer an in-depth comparative review of the methods presented so far for clustering directed networks along with the relevant necessary methodological background and also related applications. The survey commences by offering a concise review of the fundamental concepts and methodological base on which graph clustering algorithms capitalize on. Then we present the relevant work along two orthogonal classifications. The first one is mostly concerned with the methodological principles of the clustering algorithms, while the second one approaches the methods from the viewpoint regarding the properties of a good cluster in a directed network. Further, we present methods and metrics for evaluating graph clustering results, demonstrate interesting application domains and provide promising future research directions. ?? 2013 Elsevier B.V.},
  pagetotal = {95–142},
  number = {4},
  date = {2013},
  keywords = {Community detection,Complex networks,Directed networks,Graph clustering,Graph mining},
  author = {Malliaros, Fragkiskos D. and Vazirgiannis, Michalis},
  file = {/home/dimitri/Nextcloud/Zotero/storage/7RH5ELFL/Malliaros, Vazirgiannis - 2013 - Clustering and community detection in directed networks A survey.pdf},
  doi = {10.1016/j.physrep.2013.08.002}
}

@article{krzakalaSpectralRedemptionClustering2013,
  title = {Spectral Redemption in Clustering Sparse Networks},
  volume = {110},
  issn = {0027-8424, 1091-6490},
  url = {http://www.pnas.org/content/110/52/20935%5Cnhttp://www.ncbi.nlm.nih.gov/pubmed/24277835%5Cnhttp://www.pnas.org/content/110/52/20935.full.pdf},
  doi = {10.1073/pnas.1312486110},
  abstract = {Spectral algorithms are classic approaches to clustering and community detection in networks. However, for sparse networks the standard versions of these algorithms are suboptimal, in some cases completely failing to detect communities even when other algorithms such as belief propagation can do so. Here, we present a class of spectral algorithms based on a nonbacktracking walk on the directed edges of the graph. The spectrum of this operator is much better-behaved than that of the adjacency matrix or other commonly used matrices, maintaining a strong separation between the bulk eigenvalues and the eigenvalues relevant to community structure even in the sparse case. We show that our algorithm is optimal for graphs generated by the stochastic block model, detecting communities all of the way down to the theoretical limit. We also show the spectrum of the nonbacktracking operator for some real-world networks, illustrating its advantages over traditional spectral clustering.},
  number = {52},
  journaltitle = {Proceedings of the National Academy of Sciences of the United States of America},
  date = {2013},
  pages = {20935--20940},
  author = {Krzakala, Florent and Moore, Cristopher and Mossel, Elchanan and Neeman, Joe and Sly, Allan and Zdeborová, Lenka and Zhang, Pan},
  file = {/home/dimitri/Nextcloud/Zotero/storage/WFVBRMZ2/Krzakala et al. - 2013 - Spectral redemption in clustering sparse networks.pdf},
  eprinttype = {pmid},
  eprint = {24277835}
}

@article{newmanFindingEvaluatingCommunity2004,
  title = {Finding and Evaluating Community Structure in Networks},
  volume = {69},
  issn = {1063651X},
  doi = {10.1103/PhysRevE.69.026113},
  abstract = {We propose and study a set of algorithms for discovering community structure in networks-natural divisions of network nodes into densely connected subgroups. Our algorithms all share two definitive features: first, they involve iterative removal of edges from the network to split it into communities, the edges removed being identified using any one of a number of possible "betweenness" measures, and second, these measures are, crucially, recalculated after each removal. We also propose a measure for the strength of the community structure found by our algorithms, which gives us an objective metric for choosing the number of communities into which a network should be divided. We demonstrate that our algorithms are highly effective at discovering community structure in both computer-generated and real-world network data, and show how they can be used to shed light on the sometimes dauntingly complex structure of networked systems.},
  issue = {2 2},
  journaltitle = {Physical Review E - Statistical, Nonlinear, and Soft Matter Physics},
  date = {2004},
  author = {Newman, M. E J and Girvan, M.},
  file = {/home/dimitri/Nextcloud/Zotero/storage/78QE3H3I/Newman, Girvan - 2004 - Finding and evaluating community structure in networks.pdf},
  eprinttype = {pmid},
  eprint = {14995526}
}

@article{aldousReversibleMarkovChains1999,
  title = {Reversible {{Markov Chains}} and {{Random Walks}} on {{Graphs}}},
  volume = {2002},
  url = {http://stat-www.berkeley.edu/users/aldous/RWG/book.html},
  abstract = {Classical mathematical probability focuses on time-asymptotics, describing what happens if some random process runs for ever. In contrast, the word problems each ask “how long until a chain does something?”, and the focus of this book is on finite-time behavior. More precisely, the word problems ask about hitting times, the time until a state or a set of states is first visited, or until each state in a set is visited; or ask about mixing times, the number of steps until the distribution is approximately the stationary distribution. The card-shuffling problems (section 1.1.4) provide a very intuitive setting for such questions; how many shuffles are needed, as a function of the size of the deck, until the deck is well shuffled? Such size-asymptotic results, of which (1.1) is perhaps the best-known, are one of the themes of this book. Thus in one sense our work is in the spirit of the birthday and coupon-collector's problems in undergraduate probability; in another sense our goals are reminiscent of those of computational complexity (P =? NP and all that), which seeks to relate the time required to solve an algorithmic problem to the size of the problem.},
  date = {1999},
  pages = {1--516},
  author = {Aldous, David and Fill, James Allen},
  file = {/home/dimitri/Nextcloud/Zotero/storage/N4KHETEE/Aldous, Fill - 1999 - Reversible Markov Chains and Random Walks on Graphs.pdf}
}

@article{saadeSpectralClusteringGraphs2014,
  title = {Spectral {{Clustering}} of {{Graphs}} with the {{Bethe Hessian}}},
  issn = {10495258},
  url = {http://arxiv.org/abs/1406.1880},
  abstract = {Spectral clustering is a standard approach to label nodes on a graph by studying the (largest or lowest) eigenvalues of a symmetric real matrix such as e.g. the adjacency or the Laplacian. Recently, it has been argued that using instead a more complicated, non-symmetric and higher dimensional operator, related to the non-backtracking walk on the graph, leads to improved performance in detecting clusters, and even to optimal performance for the stochastic block model. Here, we propose to use instead a simpler object, a symmetric real matrix known as the Bethe Hessian operator, or deformed Laplacian. We show that this approach combines the performances of the non-backtracking operator, thus detecting clusters all the way down to the theoretical limit in the stochastic block model, with the computational, theoretical and memory advantages of real symmetric matrices.},
  number = {1},
  journaltitle = {arXiv preprint arXiv:1406.1880},
  date = {2014},
  pages = {1--8},
  author = {Saade, a and Krzakala, F and Zdeborová, L}
}

@article{rothschildEquilibriumCompetitiveInsurance1976,
  eprinttype = {jstor},
  eprint = {1885326%5Cnhttp://www.jstor.org/%5Cnhttp://www.jstor.org/action/showPublisher?publisherCode=mitpress.%5Cnhttp://www.jstor.org},
  title = {Equilibrium in {{Competitive Insurance Markets}}: {{An Essay}} on the {{Economics}} of {{Imperfect Information Author}}(s): {{EQUILIBRIUM IN COMPETITIVE INSURANCE MARKETS}}: {{AN ESSAY ON THE ECONOMICS OF IMPERFECT INFORMATION}}*},
  volume = {90},
  issn = {00335533},
  doi = {10.2307/1885326},
  abstract = {Each copy of any part of a JSTOR transmission must contain the same copyright notice that appears on the screen or printed page of such transmission. JSTOR is a not-for-profit service that helps scholars, researchers, and students discover, use, and build upon a wide range of content in a trusted digital archive. We use information technology and tools to increase productivity and facilitate new forms of scholarship. For more information about JSTOR, please contact support@jstor.org. Introduction, 629.-I. The basic model, 630.-Il. Robustness, 638-III. Conclusion, 648.},
  number = {4},
  journaltitle = {Source: The Quarterly Journal of Economics},
  date = {1976},
  pages = {629--649},
  author = {Rothschild, Michael and Stiglitz, Joseph},
  file = {/home/dimitri/Nextcloud/Zotero/storage/RSHG3D3N/Rothschild, Stiglitz - 1976 - Equilibrium in Competitive Insurance Markets An Essay on the Economics of Imperfect Information Author(s).pdf}
}

@article{akerlofMarketLemonsQuality1970,
  title = {The {{Market}} for "{{Lemons}}": {{Quality Uncertainty}} and the {{Market Mechanism}}},
  volume = {84},
  issn = {00335533},
  url = {http://link.springer.com/chapter/10.1007/978-1-349-24002-9_9%5Cnhttp://qje.oxfordjournals.org/lookup/doi/10.2307/1879431},
  doi = {10.2307/1879431},
  abstract = {I. Introduction, 488.–II. The model with automobiles as an example, 489.–III. Examples and applications, 492.–IV. Counteracting institutions, 499.–V. Conclusion, 500},
  number = {3},
  journaltitle = {The Quarterly Journal of Economics},
  date = {1970},
  pages = {488},
  author = {Akerlof, George A.},
  file = {/home/dimitri/Nextcloud/Zotero/storage/ZCJF64SE/Akerlof - 1970 - The Market for Lemons Quality Uncertainty and the Market Mechanism.pdf},
  eprinttype = {pmid},
  eprint = {1879431}
}

@book{bjorkSuccessfulSATEncoding2009,
  title = {Successful {{SAT}} Encoding Techniques},
  url = {http://scholar.google.com/scholar?hl=en&btnG=Search&q=intitle:Successful+SAT+Encoding+Techniques#0},
  date = {2009},
  keywords = {sat,sat encodings,satisfiability},
  author = {Björk, M}
}

@article{eenExtensibleSATsolver2004,
  title = {An {{Extensible SAT}}-Solver},
  issn = {03029743},
  url = {http://link.springer.com/chapter/10.1007/978-3-540-24605-3_37},
  doi = {10.1007/978-3-540-24605-3_37},
  abstract = {In this article, we present a small, complete, and efficient SAT-solver in the style of conflict-driven learning, as exemplified by Chaff. We aim to give sufficient details about implementation to enable the reader to construct his or her own solver in a very short time. This will allow users of SAT-solvers to make domain specific extensions or adaptions of current state-of-the-art SAT-techniques, to meet the needs of a particular application area. The presented solver is designed with this in mind, and includes among other things a mechanism for adding arbitrary boolean constraints. It also supports solving a series of related SAT-problems efficiently by an incremental SAT-interface.},
  journaltitle = {Theory and Applications of Satisfiability Testing},
  date = {2004},
  pages = {502--518},
  keywords = {Algorithm Analysis and Problem Complexity,Artificial Intelligence (incl. Robotics),Mathematical Logic and Formal Languages,Mathematical Logic and Foundations,Numeric Computing},
  author = {Eén, Niklas and Sörensson, Niklas},
  file = {/home/dimitri/Nextcloud/Zotero/storage/JU6DKXX4/Unknown - Unknown - MiniSat.pdf}
}

@article{meftahSERBNanosatelliteDedicated2016,
  title = {{{SERB}}, a Nano-Satellite Dedicated to the {{Earth}}-{{Sun}} Relationship},
  abstract = {The Solar irradiance and Earth Radiation Budget (SERB) mission is an innovative proof-of-concept nano-satellite, with three ambitious scientific objectives. The nano-satellite aims at measuring on the same platform the absolute value of the total solar irradiance (TSI) and its variability, the ultraviolet (UV) solar spectral vari-ability, and the different components of the Earth radiation budget. SERB is a joint project between CNES (Centre National d Etudes Spatiales) Ecole polytechnique, and LATMOS (LaboratoireAtmosp eres, Milieux, Observations Spatiales) scheduled for a launch in 2020–2021. It is a three-unit CubeSat (X-CubeSat II), de-veloped by students fro Ecole polytechnique. Critical components of instrumental payloads of future large missions (coatings, UV filters, etc.) can acquire the technical maturity by flying in a CubeSat. Nano-satellites also represent an excellent alternative for instrumentation testing, allowing for longer flights than rockets. More-over, specific scientific experiments can be performed by nano-satellites. This paper is intended to present the SERB mission and its scientific objectives.},
  date = {2016},
  keywords = {Earth radiation budget,Nano-satellites,Sun-Earth relationship,total solar irradiance,ultraviolet solar irradiance},
  author = {Meftah, Mustapha and Bamas, Etienne and Cambournac, Pierre and Cherabier, Philippe and Demarets, Romain and Denis, Gaspard and Dion, Axel and Duroselle, Raphaël and Duveiller, Florence and Eichner, Laetitia and Lozeve, Dimitri and Mestdagh, Guillaume and Ogier, Antoine and Oliverio, Romane and Receveur, Thibault and Souchet, Camille and Gilbert, Pierre and Poiet, Germain and Hauchecorne, Alain and Keckhut, Philippe and Sarkissian, Alain},
  file = {/home/dimitri/Nextcloud/Zotero/storage/TRN6HJ3H/Meftah et al. - Unknown - SERB, a nano-satellite dedicated to the Earth-Sun relationship.pdf}
}

@article{bohmProgrammingSymposium1974,
  title = {Programming {{Symposium}}},
  volume = {19},
  issn = {1098-6596},
  url = {http://www.springerlink.com/index/10.1007/3-540-06859-7},
  doi = {10.1007/3-540-06859-7},
  abstract = {The type structure of programming languages has been the subject of an active development characterized by continued controversy over basic principles. In this paper, we formalize a view of these principles somewhat similar to that of J. H. Morris. We introduce an extension of the typed lambda calculus which permits user-defined types and polymorphic functions, and show that the semantics of this language satisfies a representation theorem which embodies our notion of a `correct' type structure.},
  journaltitle = {Programming Symposium, Proceedings Colloque sur la Programmation, Paris, France, April 9-11, 1974},
  date = {1974},
  pages = {266--279},
  keywords = {types},
  author = {Böhm, Corrado and Dezani-Ciancaglini, Mariangiola and Ronchi Della Rocca, Simona},
  file = {/home/dimitri/Nextcloud/Zotero/storage/BUVHWJQU/Reynolds - 1974 - Towards a theory of type structure.pdf},
  eprinttype = {pmid},
  eprint = {25246403}
}

@article{hoareAxiomaticBasisComputer1969,
  title = {An {{Axiomatic Basis}} for {{Computer Programming}}},
  volume = {12},
  issn = {00010782},
  doi = {10.1145/363235.363259},
  abstract = {In this paper an attempt is made to explore the logical foundations of computer programming by use of techniques which were first applied in the study of geometry and have later been extended to other branches of mathematics. This involves the elucidation of sets of axioms and rules of inference which can be used in proofs of the properties of computer programs. Examples are given of such axioms and rules, and a formal proof of a simple theorem is displayed. Finally, it is argued that important advantages, both theoretical and practical, may follow from a pursuance of these topics.},
  number = {10},
  journaltitle = {Communications of the ACM},
  date = {1969},
  pages = {576--580},
  keywords = {0,20,21,22,23,24,4,5,and phrases,axiomatic method,cr category,design,formal language definition,machine-independent programming,program documentation,programming language,proofs of programs,theory of programming},
  author = {Hoare, C. A. R.},
  file = {/home/dimitri/Nextcloud/Zotero/storage/CQIK4EBT/Hoare - 1969 - An Axiomatic Basis for Computer Programming.pdf}
}

@article{landinNext700Programming1966,
  title = {The {{Next}} 700 {{Programming Languages}}},
  volume = {9},
  issn = {0001-0782},
  url = {http://doi.acm.org/10.1145/365230.365257%5Cnhttp://dl.acm.org/ft_gateway.cfm?id=365257&type=pdf},
  doi = {10.1145/365230.365257},
  abstract = {A family of unimplemented computing languages is described that is intended to span differences of application area by a unified framework. This framework dictates the rules about the uses of user-coined names, and the conventions about characterizing functional relationships. Within this framework the design of a specific language splits into two independent parts. One is the choice of written appearances of programs (or more generally, their physical representation). The other is the choice of the abstract entities (such as numbers, character-strings, list of them, functional relations among them) that can be referred to in the language.\$\textbackslash{}backslash\$nThe system is biased towards “expressions” rather than “statements.” It includes a nonprocedural (purely functional) subsystem that aims to expand the class of users' needs that can be met by a single print-instruction, without sacrificing the important properties that make conventional right-hand-side expressions easy to construct and understand.},
  number = {3},
  journaltitle = {Commun. ACM},
  date = {1966},
  pages = {157--166},
  author = {Landin, P. J.},
  file = {/home/dimitri/Nextcloud/Zotero/storage/QZBMQRZC/Landin - 1966 - The Next 700 Programming Languages.pdf}
}

@article{plotkinCallbynameCallbyvalueLcalculus1975,
  title = {Call-by-Name, Call-by-Value and the λ-Calculus},
  volume = {1},
  issn = {03043975},
  doi = {10.1016/0304-3975(75)90017-1},
  abstract = {This paper examines the old question of the relationship between ISWIM and the λ-calculus, using the distinction between call-by-value and call-by-name. It is held that the relationship should be mediated by a standardisation theorem. Since this leads to difficulties, a new λ-calculus is introduced whose standardisation theorem gives a good correspondence with ISWIM as given by the SECD machine, but without the letrec feature. Next a call-by-name variant of ISWIM is introduced which is in an analogous correspondence withthe usual λ-calculus. The relation between call-by-value and call-by-name is then studied by giving simulations of each language by the other and interpretations of each calculus in the other. These are obtained as another application of the continuation technique. Some emphasis is placed throughout on the notion of operational equality (or contextual equality). If terms can be proved equal in a calculus they are operationally equal in the corresponding language. Unfortunately, operational equality is not preserved by either of the simulations. © 1975.},
  number = {2},
  journaltitle = {Theoretical Computer Science},
  date = {1975},
  pages = {125--159},
  author = {Plotkin, G. D.}
}

@article{milnerTheoryTypePolymorphism1978,
  title = {A Theory of Type Polymorphism in Programming},
  volume = {17},
  issn = {10902724},
  doi = {10.1016/0022-0000(78)90014-4},
  abstract = {The aim of this work is largely a practical one. A widely employed style of programming, particularly in structure-processing languages which impose no discipline of types, entails defining procedures which work well on objects of a wide variety. We present a formal type discipline for such polymorphic procedures in the context of a simple programming language, and a compile time type-checking algorithm W which enforces the discipline. A Semantic Soundness Theorem (based on a formal semantics for the language) states that well-type programs cannot "go wrong" and a Syntactic Soundness Theorem states that if W accepts a program then it is well typed. We also discuss extending these results to richer languages; a type-checking algorithm based on W is in fact already implemented and working, for the metalanguage ML in the Edinburgh LCF system. ?? 1978.},
  number = {3},
  journaltitle = {Journal of Computer and System Sciences},
  date = {1978},
  pages = {348--375},
  author = {Milner, Robin},
  file = {/home/dimitri/Nextcloud/Zotero/storage/ALDIBIYT/Milner - 1978 - A theory of type polymorphism in programming.pdf}
}

@book{goldbergWhatEveryComputer1991,
  title = {What Every Computer Scientist Should Know about Floating-Point Arithmetic},
  volume = {23},
  isbn = {0000000000000},
  url = {http://portal.acm.org/citation.cfm?doid=103162.103163},
  abstract = {Floating-point arithmetic is considered as esoteric subject by many people. This is rather surprising, because floating-point is ubiquitous in computer systems: Almost every language has a floating-point datatype; computers from PCs to supercomputers have floating-point accelerators; most compilers will be called upon to compile floating-point algorithms from time to time; and virtually every operating system must respond to floating-point exceptions such as overflow. This paper presents a tutorial on the aspects of floating-point that have a direct impact on designers of computer systems. It begins with background on floating-point representation and rounding error, continues with a discussion of the IEEE floating point standard, and concludes with examples of how computer system builders can better support floating point.},
  pagetotal = {5–48},
  number = {1},
  date = {1991},
  keywords = {and phrases,denormalized,exception,floating-point,number},
  author = {Goldberg, David},
  file = {/home/dimitri/Nextcloud/Zotero/storage/IGXKPQHC/Goldberg - 1991 - What every computer scientist should know about floating-point arithmetic.pdf},
  doi = {10.1145/103162.103163},
  eprinttype = {pmid},
  eprint = {15809826}
}

@article{turingComputableNumbersApplication1938,
  title = {On Computable Numbers, with an Application to the Entscheidungsproblem. a Correction},
  volume = {s2-43},
  issn = {1460244X},
  doi = {10.1112/plms/s2-43.6.544},
  abstract = {Turing, A. M. "On Computable Numbers with an Application to the Entscheidungsproblem." ,},
  number = {1},
  journaltitle = {Proceedings of the London Mathematical Society},
  date = {1938},
  pages = {544--546},
  author = {Turing, A. M.},
  file = {/home/dimitri/Nextcloud/Zotero/storage/4MTC42Q2/Turing - 1938 - On computable numbers, with an application to the entscheidungsproblem. a correction.pdf},
  eprinttype = {pmid},
  eprint = {25246403}
}

@incollection{karpReducibilityCombinatorialProblems2010,
  title = {Reducibility among Combinatorial Problems},
  isbn = {978-3-540-68274-5},
  url = {http://www.springerlink.com/index/10.1007/978-1-4684-2001-2_9%5Cnpapers3://publication/doi/10.1007/978-1-4684-2001-2_9},
  abstract = {A large class of computational problems involve the determination of properties of graphs, digraphs, integers, arrays of integers, finite families of finite sets, boolean formulas and elements of other countable domains. Through simple encodings from such domains into the set of words over a finite alphabet these problems can be converted into language recognition problems, and we can inquire into their computational complexity. It is reasonable to consider such a problem satisfactorily solved when an algorithm for its solution is found which terminates within a number of steps bounded by a polynomial in the length of the input. We show that a large number of classic unsolved problems of covering, matching, packing, routing, assignment and sequencing are equivalent, in the sense that either each of them possesses a polynomial-bounded algorithm or none of them does.},
  number = {Chapter 9},
  booktitle = {50 {{Years}} of {{Integer Programming}} 1958-2008: {{From}} the {{Early Years}} to the {{State}}-of-the-{{Art}}},
  date = {2010},
  pages = {219--241},
  author = {Karp, Richard M.},
  file = {/home/dimitri/Nextcloud/Zotero/storage/4IAHTPB5/Karp - 1972 - Reducibility among Combinatorial Problems BT - (null).pdf},
  doi = {10.1007/978-3-540-68279-0_8},
  eprinttype = {pmid},
  eprint = {15890271}
}

@article{alonSpaceComplexityApproximating1999,
  title = {The {{Space Complexity}} of {{Approximating}} the {{Frequency Moments}}},
  volume = {58},
  issn = {00220000},
  url = {http://www.sciencedirect.com/science/article/pii/S0022000097915452},
  doi = {10.1006/jcss.1997.1545},
  abstract = {The frequency moments of a sequence containingmielements of typei, 1⩽i⩽n, are the numbersFk=∑ni=1mki. We consider the space complexity of randomized algorithms that approximate the numbersFk, when the elements of the sequence are given one by one and cannot be stored. Surprisingly, it turns out that the numbersF0,F1, andF2can be approximated in logarithmic space, whereas the approximation ofFkfork⩾6 requiresn\$Ømega\$(1)space. Applications to data bases are mentioned as well.},
  number = {1},
  journaltitle = {Journal of Computer and System Sciences},
  date = {1999},
  pages = {137--147},
  author = {Alon, Noga and Matias, Yossi and Szegedy, Mario},
  file = {/home/dimitri/Nextcloud/Zotero/storage/M7Z3C5AL/Alon, Matias, Szegedy - 1999 - The Space Complexity of Approximating the Frequency Moments.pdf}
}

@article{shannonMathematicalTheoryCommunication1948,
  title = {A Mathematical Theory of Communication},
  volume = {27},
  issn = {15591662},
  url = {http://cm.bell-labs.com/cm/ms/what/shannonday/paper.html},
  doi = {10.1145/584091.584093},
  abstract = {THE recent development of various methods of modulation such as PCM and PPM which exchange bandwidth for signal-to-noise ratio has intensified the interest in a general theory of communication. A basis for such a theory is contained in the important papers of Nyquist1 and Hartley2 on this subject. In the present paper we will extend the theory to include a number of new factors, in particular the effect of noise in the channel, and the savings possible due to the statistical structure of the original message and due to the nature of the final destination of the information. The fundamental problem of communication is that of reproducing at one point either exactly or approximately a message selected at another point. Frequently the messages have meaning; that is they refer to or are correlated according to some system with certain physical or conceptual entities. These semantic aspects of communication are irrelevant to the engineering problem. The significant aspect is that the actual message is one selected from a set of possible messages. The system must be designed to operate for each possible selection, not just the one which will actually be chosen since this is unknown at the time of design. If the number of messages in the set is finite then this number or any monotonic function of this number can be regarded as a measure of the information produced when one message is chosen from the set, all choices being equally likely. As was pointed out by Hartley the most natural choice is the logarithmic function. Although this definition must be generalized considerably when we consider the influence of the statistics of the message and when we have a continuous range of messages, we will in all cases use an essentially logarithmic measure.},
  number = {1},
  journaltitle = {The Bell System Technical Journal},
  date = {1948},
  pages = {379--423},
  author = {Shannon, C E and Weaver, W and Press, University of Illinois},
  file = {/home/dimitri/Nextcloud/Zotero/storage/UZ83CFWA/Shannon, Weaver - 1948 - A Mathematical Theory of Communication.pdf},
  eprinttype = {pmid},
  eprint = {19312579}
}

@article{haastadOptimalInapproximabilityResults2001,
  title = {Some Optimal Inapproximability Results},
  volume = {48},
  issn = {00045411},
  doi = {10.1145/502090.502098},
  abstract = {We prove optimal, up to an arbitrary epsilon \textbackslash{}textgreater 0, inapproximability results for Max-Ek-Sat for k greater than or equal to 3, maximizing the number of satisfied linear equations in an over-determined system of linear equations modulo a prime p and Set Splitting. As a consequence of these results we get improved lower bounds for the efficient approximability of many optimization problems studied previously. In particular, for Max-E2-Sat, Max-Cut, Max-di-Cut, and Vertex cover.},
  number = {4},
  journaltitle = {Journal of the ACM},
  date = {2001},
  pages = {798--859},
  author = {H\textbackslash{}a astad, Johan}
}

@article{hanksStructuredPropositionsTypes2011,
  title = {Structured Propositions as Types},
  volume = {120},
  issn = {00264423},
  doi = {10.1093/mind/fzr011},
  abstract = {Powerful insights arise from linking two fields of study previ- ously thought separate. Examples include Descartes's coordinates, which links geometry to algebra, Planck's Quantum Theory, which links particles to waves, and Shannon's Information Theory, which links thermodynamics to communication. Such a synthesis is of- fered by the principle of Propositions as Types, which links logic to computation. At first sight it appears to be a simple coincidence— almost a pun—but it turns out to be remarkably robust, inspiring the design of automated proof assistants and programming languages, and continuing to influence the forefronts of computing},
  number = {477},
  journaltitle = {Mind},
  date = {2011},
  pages = {11--52},
  author = {Hanks, Peter W.},
  file = {/home/dimitri/Nextcloud/Zotero/storage/Z6P75AZQ/Hanks - 2011 - Structured propositions as types.pdf}
}

@article{thompsonReflectionsTrustingTrust1995,
  title = {Reflections on Trusting Trust Revisited},
  volume = {46},
  issn = {00010782},
  doi = {10.1145/777313.777347},
  abstract = {I thank the ACM for this award. I can't help but feel that I am receiving this honor for timing and serendip- ity as much as technical merit. UNIX1 swept into popu- larity with an industry-wide change from central main- frames to autonomous minis. I suspect that Daniel Bob- row [1] would be here instead of me if he could not afford a PDP-10 and had had to "settle" for a PDP-11. Moreover, the current state of UNIX is the result of the labors of a large number of people. There is an old adage, "Dance with the one that brought you," which means that I should talk about UNIX. I have not worked on mainstream UNIX in many years, yet I continue to get undeserved credit for the work of others. Therefore, I am not going to talk about UNIX, but I want to thank everyone who has contrib- uted. That brings me to Dennis Ritchie. Our collaboration has been a thing of beauty. In the ten years that we have worked together, I can recall only one case of miscoordination of work. On that occasion, I discovered that we both had written the same 20-line assembly language program. I compared the sources and was as- tounded to find that they matched character-for-char- acter. The result of our work together has been far greater than the work that we each contributed. I am a programmer. On my 1040 form, that is what I put down as my occupation. As a programmer, I write programs. I would like to present to you the cutest program I ever wrote. I will do this in three stages and try to bring it together at the end.},
  number = {6},
  journaltitle = {Communications of the ACM},
  date = {1995},
  pages = {112},
  author = {Thompson, Ken},
  file = {/home/dimitri/Nextcloud/Zotero/storage/IQBKUKA7/Thompson - 2003 - Reflections on trusting trust revisited.pdf}
}

@article{hooryExpanderGraphsTheir2006,
  title = {Expander Graphs and Their Applications},
  volume = {43},
  issn = {02730979},
  doi = {10.1090/S0273-0979-06-01126-8},
  abstract = {A major consideration we had in writing this survey was to make it accessible to mathematicians as well as to computer scientists, since expander graphs, the protagonists of our story, come up in numerous and often surprising contexts in both fields. But, perhaps, we should start with a few words about graphs in general. They are, of course, one of the prime objects of study in Discrete Mathematics. However, graphs are among the most ubiquitous models of both natural and human-made structures. In the natural and social sciences they model relations among species, societies, companies, etc. In computer science, they represent networks of commu-nication, data organization, computational devices as well as the flow of computa-tion, and more. In mathematics, Cayley graphs are useful in Group Theory. Graphs carry a natural metric and are therefore useful in Geometry, and though they are " just " one-dimensional complexes, they are useful in certain parts of Topology, e.g. Knot Theory. In statistical physics, graphs can represent local connections between interacting parts of a system, as well as the dynamics of a physical process on such systems. The study of these models calls, then, for the comprehension of the significant structural properties of the relevant graphs. But are there nontrivial structural properties which are universally important? Expansion of a graph requires that it is simultaneously sparse and highly connected. Expander graphs were first de-fined by Bassalygo and Pinsker, and their existence first proved by Pinsker in the early '70s. The property of being an expander seems significant in many of these mathematical, computational and physical contexts. It is not surprising that ex-panders are useful in the design and analysis of communication networks. What is less obvious is that expanders have surprising utility in other computational settings such as in the theory of error correcting codes and the theory of pseudorandom-ness. In mathematics, we will encounter e.g. their role in the study of metric embeddings, and in particular in work around the Baum-Connes Conjecture. Ex-pansion is closely related to the convergence rates of Markov Chains, and so they play a key role in the study of Monte-Carlo algorithms in statistical mechanics and in a host of practical computational applications. The list of such interesting and fruitful connections goes on and on with so many applications we will not even},
  number = {4},
  journaltitle = {Bulletin of the American Mathematical Society},
  date = {2006},
  pages = {439--561},
  author = {Hoory, Shlomo and Linial, Nathan and Wigderson, Avi},
  file = {/home/dimitri/Nextcloud/Zotero/storage/NHWWD2CT/Hoory, Linial, Wigderson - 2006 - Expander graphs and their applications.pdf},
  eprinttype = {pmid},
  eprint = {1000183077}
}

@article{fichHundredsImpossibilityResults2003,
  title = {Hundreds of Impossibility Results for Distributed Computing},
  volume = {16},
  issn = {01782770},
  doi = {10.1007/s00446-003-0091-y},
  abstract = {We survey results from distributed computing that show tasks to be impossible, either outright or within given resource bounds, in various models. The parameters of the models considered include synchrony, fault-tolerance, different communication media, and randomization. The resource bounds refer to time, space and message complexity. These results are useful in understanding the inherent difficulty of individual problems and in studying the power of different models of distributed computing.},
  number = {2-3},
  journaltitle = {Distributed Computing},
  date = {2003},
  pages = {121--163},
  keywords = {Complexity lower bounds,Distributed computing,Impossibility results},
  author = {Fich, Faith and Ruppert, Eric},
  file = {/home/dimitri/Nextcloud/Zotero/storage/KGCYWGPW/Fich, Ruppert - 2003 - Hundreds of impossibility results for distributed computing.pdf}
}

@article{wadlerCallbyvalueDualCallbyname2003,
  title = {Call-by-Value Is Dual to Call-by-Name},
  volume = {38},
  issn = {03621340},
  doi = {10.1145/944746.944723},
  abstract = {The rules of classical logic may be formulated in pairs corresponding to De Morgan duals: rules about \& are dual to rules about V. A line of work, including that of Filinski (1989), Griffin (1990), Parigot (1992), Danos, Joinet, and Schellinx (1995), Selinger (1998,2001), and Curien and Herbelin (2000), has led to the startling conclusion that call-by-value is the de Morgan dual of call-by-name.This paper presents a dual calculus that corresponds to the classical sequent calculus of Gentzen (1935) in the same way that the lambda calculus of Church (1932,1940) corresponds to the intuitionistic natural deduction of Gentzen (1935). The paper includes crisp formulations of call-by-value and call-by-name that are obviously dual; no similar formulations appear in the literature. The paper gives a CPS translation and its inverse, and shows that the translation is both sound and complete, strengthening a result in Curien and Herbelin (2000).Note. This paper uses color to clarify the relation of types and terms, and of source and target calculi. If the URL below is not in blue, please download the color version, which can be found in the ACM Digital Library archive for ICFP 2003, athttp://portal.acm.org/proceedings/icfp/archive, or bygoogling 'wadler dual'.},
  number = {9},
  journaltitle = {ACM SIGPLAN Notices},
  date = {2003},
  pages = {189--201},
  keywords = {curry-howard correspondence,de morgan dual,duction,lambda,lambda calculus,logic,natural de-,sequent calculus},
  author = {Wadler, Philip},
  file = {/home/dimitri/Nextcloud/Zotero/storage/CTNFUAYP/Wadler - 2003 - Call-by-value is dual to call-by-name.pdf}
}

@article{edmondsPathsTreesFlowers1963,
  title = {Paths, {{Trees}}, and {{Flowers}}},
  volume = {17},
  issn = {1496-4279},
  doi = {10.4153/CJM-1965-045-4},
  abstract = {A graph G for purposes here is a finite set of elements called vertices and a finite set of elements called edges such that each edge meets exactly two vertices, called the end-points of the edge. An edge is said to join its end-points. Jack Edmonds has been one of the creators of the field of combinatorial optimization and polyhedral combinatorics. His 1965 paper Paths, Trees, and Flowers 1 was one of the first papers to suggest the possibility of establishing a mathematical theory of efficient combinatorial algorithms . . . from the award citation of the 1985 John von Neumann Theory Prize, awarded annually since 1975 by the Institute for Operations Research and the Management Sciences. In 1962, the Chinese mathematician Mei-Ko Kwan proposed a method that could be used to minimize the lengths of routes walked by mail carriers. Much earlier, in 1736, the eminent Swiss mathematician Leonhard Euler, who had elevated calculus to unprecedented heights, had investigated whether there existed a walk across all seven bridges that connected two islands in the river Pregel with the rest of the city of Konigsberg on the adjacent shores, a walk that would cross each bridge exactly once.},
  journaltitle = {Journal canadien de mathématiques},
  date = {1963},
  pages = {449--467},
  author = {Edmonds, Jack},
  file = {/home/dimitri/Nextcloud/Zotero/storage/ZILVRD4Q/Edmonds - 1965 - Paths, trees, and flowers.pdf}
}

@article{nasaSmallSpacecraftTechnology2014,
  title = {Small {{Spacecraft Technology State}} of the {{Art}}},
  url = {http://www.nasa.gov/sites/default/files/files/Small_Spacecraft_Technology_State_of_the_Art_2014.pdf},
  doi = {NASA/TP–2014–216648},
  abstract = {This report provides an overview of the current state of the art of small spacecraft technology. It was commissioned by NASA's Small Space- craft Technology Program (SSTP) in mid-2013 in response to the rapid growth in interest in using small spacecraft for many types of missions in Earth orbit and beyond. For the sake of this assessment, small spacecraft are defined to be spacecraft with a mass less than 180 kg. This report provides a summary of the state of the art for each of the following small spacecraft technology domains: Complete spacecraft, Power, Propulsion, Attitude Determination and Control, Structures, Ma- terials and Mechanisms, Thermal Control, Command and Data Han- dling, Communications, Integration, Launch and Deployment, and Ground Data Systems and Operations. Due to the high popularity of cubesats, particular emphasis is placed on the state-of-the-art of cubesat-related technology.},
  issue = {February},
  date = {2014},
  pages = {1--197},
  keywords = {nanosat,smallsat,spacecraft,technology},
  author = {{NASA}},
  file = {/home/dimitri/Nextcloud/Zotero/storage/ATW8MZMG/Design, Staff, Field - 2014 - Small Spacecraft Technology State of the Art.pdf}
}

@article{hansenEarthEnergyImbalance2011,
  title = {Earth's Energy Imbalance and Implications},
  volume = {11},
  issn = {16807316},
  doi = {10.5194/acp-11-13421-2011},
  abstract = {Improving observations of ocean heat content show that Earth is absorbing more energy from the sun than it is radiating to space as heat, even during the recent solar minimum. The inferred planetary energy imbalance, 0.59 \$\textbackslash{}backslash\$pm 0.15 W/m2 during the 6-year period 2005-2010, confirms the dominant role of the human-made greenhouse effect in driving global climate change. Observed surface temperature change and ocean heat gain together constrain the net climate forcing and ocean mixing rates. We conclude that most climate models mix heat too efficiently into the deep ocean and as a result underestimate the negative forcing by human-made aerosols. Aerosol climate forcing today is inferred to be 1.6 \$\textbackslash{}backslash\$pm 0.3 W/m2, implying substantial aerosol indirect climate forcing via cloud changes. Continued failure to quantify the specific origins of this large forcing is untenable, as knowledge of changing aerosol effects is needed to understand future climate change. We conclude that recent slowdown of ocean heat uptake was caused by a delayed rebound effect from Mount Pinatubo aerosols and a deep prolonged solar minimum. Observed sea level rise during the Argo float era is readily accounted for by ice melt and ocean thermal expansion, but the ascendency of ice melt leads us to anticipate acceleration of the rate of sea level rise this decade.},
  number = {24},
  journaltitle = {Atmospheric Chemistry and Physics},
  date = {2011},
  pages = {13421--13449},
  author = {Hansen, J. and Sato, M. and Kharecha, P. and Von Schuckmann, K.},
  file = {/home/dimitri/Nextcloud/Zotero/storage/GU7GQGNT/Hansen et al. - 2011 - Earth's energy imbalance and implications.pdf},
  eprinttype = {pmid},
  eprint = {74271824}
}

@article{foukalVariationsSolarLuminosity2006,
  title = {Variations in Solar Luminosity and Their Effect on the {{Earth}}'s Climate.},
  volume = {443},
  issn = {0028-0836},
  doi = {10.1038/nature05072},
  abstract = {Variations in the Sun's total energy output (luminosity) are caused by changing dark (sunspot) and bright structures on the solar disk during the 11-year sunspot cycle. The variations measured from spacecraft since 1978 are too small to have contributed appreciably to accelerated global warming over the past 30 years. In this Review, we show that detailed analysis of these small output variations has greatly advanced our understanding of solar luminosity change, and this new understanding indicates that brightening of the Sun is unlikely to have had a significant influence on global warming since the seventeenth century. Additional climate forcing by changes in the Sun's output of ultraviolet light, and of magnetized plasmas, cannot be ruled out. The suggested mechanisms are, however, too complex to evaluate meaningfully at present.},
  number = {7108},
  journaltitle = {Nature},
  date = {2006},
  pages = {161--166},
  author = {Foukal, P and Fröhlich, C and Spruit, H and Wigley, T M L},
  file = {/home/dimitri/Nextcloud/Zotero/storage/9G86JWU2/Foukal et al. - 2006 - Variations in solar luminosity and their effect on the Earth's climate.pdf},
  eprinttype = {pmid},
  eprint = {16971941}
}

@article{vieiraEvolutionSolarIrradiance2011,
  title = {Evolution of the Solar Irradiance during the {{Holocene}}},
  volume = {531},
  issn = {0004-6361},
  url = {http://arxiv.org/abs/1103.4958},
  doi = {10.1051/0004-6361/201015843},
  abstract = {Aims. We present a physically consistent reconstruction of the total solar irradiance for the Holocene. Methods. We extend the SATIRE models to estimate the evolution of the total (and partly spectral) solar irradiance over the Holocene. The basic assumption is that the variations of the solar irradiance are due to the evolution of the dark and bright magnetic features on the solar surface. The evolution of the decadally averaged magnetic flux is computed from decadal values of cosmogenic isotope concentrations recorded in natural archives employing a series of physics-based models connecting the processes from the modulation of the cosmic ray flux in the heliosphere to their record in natural archives. We then compute the total solar irradiance (TSI) as a linear combination of the jth and jth + 1 decadal values of the open magnetic flux. Results. Reconstructions of the TSI over the Holocene, each valid for a di\_erent paleomagnetic time series, are presented. Our analysis suggests that major sources of uncertainty in the TSI in this model are the heritage of the uncertainty of the TSI since 1610 reconstructed from sunspot data and the uncertainty of the evolution of the Earth's magnetic dipole moment. The analysis of the distribution functions of the reconstructed irradiance for the last 3000 years indicates that the estimates based on the virtual axial dipole moment are significantly lower at earlier times than the reconstructions based on the virtual dipole moment. Conclusions. We present the first physics-based reconstruction of the total solar irradiance over the Holocene, which will be of interest for studies of climate change over the last 11500 years. The reconstruction indicates that the decadally averaged total solar irradiance ranges over approximately 1.5 W/m2 from grand maxima to grand minima.},
  number = {A6},
  journaltitle = {Astronomy and Astrophysics},
  date = {2011},
  pages = {1--20},
  keywords = {activity,faculae,plages,solar-terrestrial relations,sun,sunspots,surface magnetism,uv radiation},
  author = {Vieira, Luis Eduardo A. and Solanki, Sami K. and Krivova, Natalie A. and Usoskin, Ilya},
  file = {/home/dimitri/Nextcloud/Zotero/storage/G39FWCYD/Vieira et al. - 2011 - Evolution of the solar irradiance during the Holocene.pdf}
}

@article{irbahHowEarthAtmospheric2012,
  title = {How {{Earth}} Atmospheric Radiations May Affect Astronomical Observations from Low-Orbit Satellites},
  volume = {8442},
  issn = {0277-786X},
  doi = {10.1117/12.925747},
  abstract = {Telescopes are placed on spacecrafts to avoid the effects of the Earth\$\textbackslash{}backslash\$natmosphere on astronomical observations (turbulence, extinction ... ).\$\textbackslash{}backslash\$nAtmospheric effects however may subsist when satellites are launched in\$\textbackslash{}backslash\$nlow orbits, typically mean altitudes of the order of 700 km. We will\$\textbackslash{}backslash\$npresent first in this paper how we are able to estimate the mean Earth\$\textbackslash{}backslash\$nradiation flux when we consider temperature housekeeping data recorded\$\textbackslash{}backslash\$nwith a specific space solar mission having this orbit property. We will\$\textbackslash{}backslash\$nshow after how some solar parameters extracted from images recorded with\$\textbackslash{}backslash\$nthe on-board telescope are correlated with the Earth atmospheric\$\textbackslash{}backslash\$nradiation flux. We will also present how we find the limits of the South\$\textbackslash{}backslash\$nAtlantic Anomaly from affected images.},
  issue = {September 2015},
  journaltitle = {Space Telescopes and Instrumentation 2012: Optical, Infrared, and Millimeter Wave},
  date = {2012},
  author = {Irbah, a and Meftah, M and Hauchecorne, a and Cisse, E M and Lin, M and Rouze, M and Team, Picard},
  file = {/home/dimitri/Nextcloud/Zotero/storage/8VDUU9C5/Irbah et al. - 2012 - How Earth atmospheric radiations may affect astronomical observations from low-orbit satellites.pdf}
}

@article{kiehlEarthAnnualGlobal1997,
  title = {Earth's {{Annual Global Mean Energy Budget}}},
  volume = {78},
  issn = {00030007},
  doi = {10.1175/1520-0477(1997)078<0197:EAGMEB>2.0.CO;2},
  abstract = {The purpose of this paper is to put forward a new estimate, in the context of previous assessments, of the annual global mean energy budget. A description is provided of the source of each component to this budget. The top-of-atmosphere shortwave and longwave flux of energy is constrained by satellite observations. Partitioning of the radiative energy throughout the atmosphere is achieved through the use of detailed radiation models for both the longwave and shortwave spectral regions. Spectral features of shortwave and longwave fluxes at both the top and surface of the earth's system are presented. The longwave radiative forcing of the climate system for both clear (125 W m-2) and cloudy (155 W m-2) conditions are discussed. The authors find that for the clear sky case the contribution due to water vapor to the total longwave radiative forcing is 75 W m-2, while for carbon dioxide it is 32 W m-2. Clouds alter these values, and the effects of clouds on both the longwave and shortwave budget are addressed. In particular, the shielding effect by clouds on absorption and emission by water vapor is as large as the direct cloud forcing. Because the net surface heat budget must balance, the radiative fluxes constrain the sum of the sensible and latent heat fluxes, which can also be estimated independently.},
  number = {2},
  journaltitle = {Bulletin of the American Meteorological Society},
  date = {1997},
  pages = {197--208},
  author = {Kiehl, Jeffery T. and Trenberth, Kevin E.},
  file = {/home/dimitri/Nextcloud/Zotero/storage/4TKY3LG6/Kiehl, Trenberth - 1997 - Earth's Annual Global Mean Energy Budget.pdf}
}

@article{haighInfluenceSolarSpectral2010,
  title = {An Influence of Solar Spectral Variations on Radiative Forcing of Climate.},
  volume = {467},
  issn = {0028-0836},
  url = {http://dx.doi.org/10.1038/nature09426},
  doi = {10.1038/nature09426},
  abstract = {The thermal structure and composition of the atmosphere is determined fundamentally by the incoming solar irradiance. Radiation at ultraviolet wavelengths dissociates atmospheric molecules, initiating chains of chemical reactions-specifically those producing stratospheric ozone-and providing the major source of heating for the middle atmosphere, while radiation at visible and near-infrared wavelengths mainly reaches and warms the lower atmosphere and the Earth's surface. Thus the spectral composition of solar radiation is crucial in determining atmospheric structure, as well as surface temperature, and it follows that the response of the atmosphere to variations in solar irradiance depends on the spectrum. Daily measurements of the solar spectrum between 0.2 µm and 2.4 µm, made by the Spectral Irradiance Monitor (SIM) instrument on the Solar Radiation and Climate Experiment (SORCE) satellite since April 2004, have revealed that over this declining phase of the solar cycle there was a four to six times larger decline in ultraviolet than would have been predicted on the basis of our previous understanding. This reduction was partially compensated in the total solar output by an increase in radiation at visible wavelengths. Here we show that these spectral changes appear to have led to a significant decline from 2004 to 2007 in stratospheric ozone below an altitude of 45 km, with an increase above this altitude. Our results, simulated with a radiative-photochemical model, are consistent with contemporaneous measurements of ozone from the Aura-MLS satellite, although the short time period makes precise attribution to solar effects difficult. We also show, using the SIM data, that solar radiative forcing of surface climate is out of phase with solar activity. Currently there is insufficient observational evidence to validate the spectral variations observed by SIM, or to fully characterize other solar cycles, but our findings raise the possibility that the effects of solar variability on temperature throughout the atmosphere may be contrary to current expectations.},
  number = {7316},
  journaltitle = {Nature},
  date = {2010},
  pages = {696--699},
  author = {Haigh, Joanna D and Winning, Ann R and Toumi, Ralf and Harder, Jerald W},
  file = {/home/dimitri/Nextcloud/Zotero/storage/9Z2CEUJ9/Haigh et al. - 2010 - An influence of solar spectral variations on radiative forcing of climate.pdf},
  eprinttype = {pmid},
  eprint = {20930841}
}

@article{meftahSOVAPPicardSpaceborne2014,
  title = {{{SOVAP}}/{{Picard}}, a {{Spaceborne Radiometer}} to {{Measure}} the {{Total Solar Irradiance}}},
  volume = {289},
  issn = {00380938},
  doi = {10.1007/s11207-013-0443-0},
  number = {5},
  journaltitle = {Solar Physics},
  date = {2014},
  pages = {1885--1899},
  keywords = {Instrumental effects,Instrumentation and data management,Solar cycle observations,Solar irradiance},
  author = {Meftah, M. and Dewitte, S. and Irbah, A. and Chevalier, A. and Conscience, C. and Crommelynck, D. and Janssen, E. and Mekaoui, S.},
  file = {/home/dimitri/Nextcloud/Zotero/storage/QSDC3PBC/Meftah et al. - 2014 - SOVAPPicard, a Spaceborne Radiometer to Measure the Total Solar Irradiance.pdf}
}

@article{foukalNewLookSolar2012,
  title = {A {{New Look}} at {{Solar Irradiance Variation}}},
  volume = {279},
  issn = {00380938},
  doi = {10.1007/s11207-012-0017-6},
  number = {2},
  journaltitle = {Solar Physics},
  date = {2012},
  pages = {365--381},
  keywords = {Solar irradiance,Solar activity},
  author = {Foukal, Peter},
  file = {/home/dimitri/Nextcloud/Zotero/storage/J8BD5QDU/Foukal - 2012 - A New Look at Solar Irradiance Variation.pdf}
}

@article{meftahNanosatelliteStudySun2014,
  title = {A Nano-Satellite to Study the {{Sun}} and the {{Earth}}},
  volume = {9085},
  issn = {1996756X},
  url = {http://proceedings.spiedigitallibrary.org/proceeding.aspx?doi=10.1117/12.2050134},
  doi = {10.1117/12.2050134},
  date = {2014},
  pages = {90850Y},
  keywords = {total solar irradiance,earth radiation budget,nano-satellites,sun-earth relationships},
  author = {Meftah, M. and Irbah, a. and Hauchecorne, a. and Damé, L. and Sarkissian, a. and Keckhut, P. and Lagage, P.-O. and Dewitte, S. and Chevalier, a.},
  file = {/home/dimitri/Nextcloud/Zotero/storage/G95WYD84/Meftah et al. - 2014 - A nano-satellite to study the Sun and the Earth.pdf}
}

@book{smithEssentialsStatisticalInference2005,
  title = {Essentials of Statistical Inference},
  isbn = {978-0-521-83971-6},
  url = {http://www.worldcat.org/oclc/254683556?referer=xid},
  publisher = {{Cambridge Univ. Press}},
  date = {2005},
  author = {Smith, G. A. Young ; R. L.}
}

@book{oudotPersistenceTheoryQuiver2015,
  location = {{Providence, Rhode Island}},
  title = {Persistence Theory: From Quiver Representations to Data Analysis},
  isbn = {978-1-4704-2545-6},
  shorttitle = {Persistence Theory},
  pagetotal = {218},
  number = {volume 209},
  series = {Mathematical Surveys and Monographs},
  publisher = {{American Mathematical Society}},
  date = {2015},
  keywords = {Algebraic topology,Algebraic topology -- Applied homological algebra and category theory -- Simplicial sets and complexes,Associative rings and algebras -- Representation theory of rings and algebras -- Representations of quivers and partially ordered sets,Computer science -- Computing methodologies and applications -- Computer graphics; computational geometry,Homology theory,Statistics -- Data analysis},
  author = {Oudot, Steve Y.},
  file = {/home/dimitri/Nextcloud/Zotero/storage/ALZW577G/Steve_Oudot_Persistence_Theory.pdf}
}

@article{carlssonTopologyData2009,
  langid = {english},
  title = {Topology and Data},
  volume = {46},
  issn = {0273-0979},
  url = {http://www.ams.org/journal-getitem?pii=S0273-0979-09-01249-X},
  doi = {10.1090/S0273-0979-09-01249-X},
  number = {2},
  journaltitle = {Bulletin of the American Mathematical Society},
  urldate = {2017-11-03},
  date = {2009-01-29},
  pages = {255-308},
  author = {Carlsson, Gunnar},
  file = {/home/dimitri/Nextcloud/Zotero/storage/WYT52FA5/carlsson2009.pdf}
}

@article{chazalIntroductionTopologicalData2017,
  title = {An Introduction to {{Topological Data Analysis}}: Fundamental and Practical Aspects for Data Scientists},
  shorttitle = {An Introduction to {{Topological Data Analysis}}},
  journaltitle = {arXiv preprint arXiv:1710.04019},
  date = {2017},
  author = {Chazal, Frédéric and Michel, Bertrand},
  file = {/home/dimitri/Nextcloud/Zotero/storage/CH8YWVM3/chazal2017.pdf}
}

@article{xuHierarchicalSegmentationUsing2017,
  title = {Hierarchical {{Segmentation Using Tree}}-{{Based Shape Spaces}}},
  volume = {39},
  issn = {0162-8828, 2160-9292},
  url = {http://ieeexplore.ieee.org/document/7452658/},
  doi = {10.1109/TPAMI.2016.2554550},
  number = {3},
  journaltitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  urldate = {2017-11-03},
  date = {2017-03-01},
  pages = {457-469},
  author = {Xu, Yongchao and Carlinet, Edwin and Geraud, Thierry and Najman, Laurent},
  file = {/home/dimitri/Nextcloud/Zotero/storage/X49E35AC/xu2016.pdf}
}

@article{tiernyGeneralizedTopologicalSimplification2012,
  title = {Generalized Topological Simplification of Scalar Fields on Surfaces},
  volume = {18},
  number = {12},
  journaltitle = {IEEE transactions on visualization and computer graphics},
  date = {2012},
  pages = {2005--2013},
  author = {Tierny, Julien and Pascucci, Valerio},
  file = {/home/dimitri/Nextcloud/Zotero/storage/ID96MTE2/tierny2012.pdf}
}

@article{tiernyLoopSurgeryVolumetric2009,
  title = {Loop Surgery for Volumetric Meshes: {{Reeb}} Graphs Reduced to Contour Trees},
  volume = {15},
  shorttitle = {Loop Surgery for Volumetric Meshes},
  number = {6},
  journaltitle = {IEEE Transactions on Visualization and Computer Graphics},
  date = {2009},
  author = {Tierny, Julien and Gyulassy, Attila and Simon, Eddie and Pascucci, Valerio},
  file = {/home/dimitri/Nextcloud/Zotero/storage/9VGB22UH/tierny2009.pdf}
}

@inproceedings{monasseScalespaceLevelLines1999,
  title = {Scale-Space from a Level Lines Tree},
  volume = {99},
  booktitle = {Scale-{{Space}}},
  publisher = {{Springer}},
  date = {1999},
  pages = {175--186},
  author = {Monasse, Pascal and Guichard, Frédéric},
  file = {/home/dimitri/Nextcloud/Zotero/storage/GXHUMD2G/Scale-Space_from_a_Level_Lines_Tree.pdf}
}

@article{robinsTheoryAlgorithmsConstructing2011,
  title = {Theory and {{Algorithms}} for {{Constructing Discrete Morse Complexes}} from {{Grayscale Digital Images}}},
  volume = {33},
  issn = {0162-8828},
  url = {http://ieeexplore.ieee.org/document/5766002/},
  doi = {10.1109/TPAMI.2011.95},
  number = {8},
  journaltitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  urldate = {2017-11-03},
  date = {2011-08},
  pages = {1646-1658},
  author = {Robins, V and Wood, P J and Sheppard, A P},
  file = {/home/dimitri/Nextcloud/Zotero/storage/D4PMIRGY/robins2011.pdf}
}

@article{rieckPersistentHomologyEvaluation2015,
  langid = {english},
  title = {Persistent {{Homology}} for the {{Evaluation}} of {{Dimensionality Reduction Schemes}}},
  volume = {34},
  issn = {01677055},
  url = {http://doi.wiley.com/10.1111/cgf.12655},
  doi = {10.1111/cgf.12655},
  number = {3},
  journaltitle = {Computer Graphics Forum},
  urldate = {2017-11-03},
  date = {2015-06},
  pages = {431-440},
  author = {Rieck, B. and Leitte, H.},
  file = {/home/dimitri/Nextcloud/Zotero/storage/4VEXZ4DG/rieck2015.pdf}
}

@inproceedings{reininghausStableMultiscaleKernel2015,
  title = {A Stable Multi-Scale Kernel for Topological Machine Learning},
  booktitle = {Proceedings of the {{IEEE}} Conference on Computer Vision and Pattern Recognition},
  date = {2015},
  pages = {4741--4748},
  keywords = {1-Wasserstein distance,3D shape classification,3D shape retrieval,image classification,image retrieval,image texture,learning (artificial intelligence),multiscale kernel,persistence diagrams,shape recognition,texture recognition,topological machine learning,Yttrium},
  author = {Reininghaus, Jan and Huber, Stefan and Bauer, Ulrich and Kwitt, Roland},
  file = {/home/dimitri/Nextcloud/Zotero/storage/H6VIHWWS/reininghaus2015.pdf;/home/dimitri/Nextcloud/Zotero/storage/K3FZI79D/Reininghaus et al. - 2015 - A stable multi-scale kernel for topological machin.pdf;/home/dimitri/Nextcloud/Zotero/storage/R29X3VFL/7299106.html}
}

@book{harveyUnderstandingHighdimensionalData2012,
  title = {Understanding High-Dimensional Data Using {{Reeb}} Graphs},
  publisher = {{The Ohio State University}},
  date = {2012},
  author = {Harvey, William John},
  file = {/home/dimitri/Nextcloud/Zotero/storage/M4SPU65W/osu1342614959.pdf}
}

@inproceedings{liPersistenceBasedStructuralRecognition2014,
  title = {Persistence-{{Based Structural Recognition}}},
  isbn = {978-1-4799-5118-5},
  url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6909654},
  doi = {10.1109/CVPR.2014.257},
  publisher = {{IEEE}},
  urldate = {2017-11-03},
  date = {2014-06},
  pages = {2003-2010},
  author = {Li, Chunyuan and Ovsjanikov, Maks and Chazal, Frederic},
  file = {/home/dimitri/Nextcloud/Zotero/storage/9JSHF2C4/li2014.pdf}
}

@article{krimDiscoveringWholeCoarse2016,
  title = {Discovering the {{Whole}} by the {{Coarse}}: {{A}} Topological Paradigm for Data Analysis},
  volume = {33},
  issn = {1053-5888},
  url = {http://ieeexplore.ieee.org/document/7426571/},
  doi = {10.1109/MSP.2015.2510703},
  shorttitle = {Discovering the {{Whole}} by the {{Coarse}}},
  number = {2},
  journaltitle = {IEEE Signal Processing Magazine},
  urldate = {2017-11-03},
  date = {2016-03},
  pages = {95-104},
  author = {Krim, Hamid and Gentimis, Thanos and Chintakunta, Harish},
  file = {/home/dimitri/Nextcloud/Zotero/storage/379FA7KH/krim2016.pdf}
}

@collection{pascucciTopologicalMethodsData2011,
  location = {{Berlin, Heidelberg}},
  title = {Topological {{Methods}} in {{Data Analysis}} and {{Visualization}}},
  isbn = {978-3-642-15013-5 978-3-642-15014-2},
  url = {http://link.springer.com/10.1007/978-3-642-15014-2},
  series = {Mathematics and {{Visualization}}},
  publisher = {{Springer Berlin Heidelberg}},
  urldate = {2017-11-03},
  date = {2011},
  editor = {Pascucci, Valerio and Tricoche, Xavier and Hagen, Hans and Tierny, Julien},
  file = {/home/dimitri/Nextcloud/Zotero/storage/IMYEDN4S/(Mathematics and Visualization) Kirk E. Jordan, Lance E. Miller (auth.), Valerio Pascucci, Xavier Tricoche, Hans Hagen, Julien Tierny (eds.)-Topological Methods in Data Analysis and Visualization_ The.pdf},
  doi = {10.1007/978-3-642-15014-2}
}

@book{edelsbrunnerComputationalTopologyIntroduction2010,
  location = {{Providence, R.I}},
  title = {Computational Topology: An Introduction},
  isbn = {978-0-8218-4925-5},
  shorttitle = {Computational Topology},
  pagetotal = {241},
  publisher = {{American Mathematical Society}},
  date = {2010},
  keywords = {Algorithms,Computational complexity,Data processing,Geometry,Topology},
  author = {Edelsbrunner, Herbert and Harer, J.},
  file = {/home/dimitri/Nextcloud/Zotero/storage/FWGR5NJ3/Herbert Edelsbrunner, John L. Harer-Computational Topology_ An Introduction-American Mathematical Society (2009).pdf},
  note = {OCLC: ocn427757156}
}

@article{monasseFastComputationContrastinvariant2000,
  title = {Fast Computation of a Contrast-Invariant Image Representation},
  volume = {9},
  issn = {10577149},
  url = {http://ieeexplore.ieee.org/document/841532/},
  doi = {10.1109/83.841532},
  number = {5},
  journaltitle = {IEEE Transactions on Image Processing},
  urldate = {2017-11-03},
  date = {2000-05},
  pages = {860-872},
  author = {Monasse, P. and Guichard, F.},
  file = {/home/dimitri/Nextcloud/Zotero/storage/3UDY8L47/monasse2000.pdf}
}

@book{mitzenmacherProbabilityComputingIntroduction2005,
  location = {{New York}},
  title = {Probability and Computing: An Introduction to Randomized Algorithms and Probabilistic Analysis},
  isbn = {978-0-521-83540-4},
  shorttitle = {Probability and Computing},
  pagetotal = {352},
  publisher = {{Cambridge University Press}},
  date = {2005},
  keywords = {Algorithms,Probabilities,Stochastic analysis},
  author = {Mitzenmacher, Michael and Upfal, Eli},
  file = {/home/dimitri/Nextcloud/Zotero/storage/JSQ7NR7J/Probability_and_Computing.pdf}
}

@book{nielsenQuantumComputationQuantum2010,
  location = {{Cambridge ; New York}},
  title = {Quantum Computation and Quantum Information},
  edition = {10th anniversary ed},
  isbn = {978-1-107-00217-3},
  pagetotal = {676},
  publisher = {{Cambridge University Press}},
  date = {2010},
  keywords = {Quantum computers},
  author = {Nielsen, Michael A. and Chuang, Isaac L.},
  file = {/home/dimitri/Nextcloud/Zotero/storage/YAWHG824/Quantum Computation And Quantum Information - 10th Anniversary Edition.pdf}
}

@book{okasakiPurelyFunctionalData1998,
  location = {{Cambridge, U.K. ; New York}},
  title = {Purely Functional Data Structures},
  isbn = {978-0-521-63124-2},
  pagetotal = {220},
  publisher = {{Cambridge University Press}},
  date = {1998},
  keywords = {Data structures (Computer science),Functional programming languages},
  author = {Okasaki, Chris},
  file = {/home/dimitri/Nextcloud/Zotero/storage/6K4RBEHR/Purely Functional Data Structures.pdf}
}

@article{algorithmicMultiagentSystems2008,
  title = {Multiagent {{Systems}}},
  date = {2008},
  author = {Algorithmic, Game-Theoretic},
  file = {/home/dimitri/Nextcloud/Zotero/storage/UZ2KLXJI/multiagentsystems.pdf}
}

@book{lawvereConceptualMathematicsFirst1997,
  location = {{Cambridge ; New York, NY, USA}},
  title = {Conceptual Mathematics: A First Introduction to Categories},
  isbn = {978-0-521-47249-4},
  shorttitle = {Conceptual Mathematics},
  pagetotal = {358},
  publisher = {{Cambridge University Press}},
  date = {1997},
  keywords = {Categories (Mathematics)},
  author = {Lawvere, F. W. and Schanuel, S. H.},
  file = {/home/dimitri/Nextcloud/Zotero/storage/KIDDH48T/kategori.pdf}
}

@book{sipserIntroductionTheoryComputation2012,
  location = {{Boston, MA}},
  title = {Introduction to the Theory of Computation},
  edition = {3rd Ed},
  isbn = {978-1-133-18779-0},
  publisher = {{Course Technology Cengage Learning}},
  date = {2012},
  author = {Sipser, Michael},
  file = {/home/dimitri/Nextcloud/Zotero/storage/IZ9MFCXP/Introduction to the Theory of Computation by Michael Sipser, Third Edition, Course Technology.pdf}
}

@collection{cormenIntroductionAlgorithms2009,
  location = {{Cambridge, Mass}},
  title = {Introduction to Algorithms},
  edition = {3rd ed},
  isbn = {978-0-262-03384-8 978-0-262-53305-8},
  pagetotal = {1292},
  publisher = {{MIT Press}},
  date = {2009},
  keywords = {Computer algorithms,Computer programming},
  editor = {Cormen, Thomas H.},
  file = {/home/dimitri/Nextcloud/Zotero/storage/CF8RIR53/Introduction to Algorithms 3rd Edition.pdf},
  note = {OCLC: ocn311310321}
}

@collection{porterHandbookKnowledgeRepresentation2008,
  location = {{Amsterdam ; Boston}},
  title = {Handbook of Knowledge Representation},
  edition = {1st ed},
  isbn = {978-0-444-52211-5},
  pagetotal = {1005},
  series = {Foundations of Artificial Intelligence},
  publisher = {{Elsevier}},
  date = {2008},
  keywords = {Knowledge representation (Information theory),Représentation des connaissances},
  editor = {Porter, Bruce and Lifschitz, Vladimir and Van Harmelen, Frank},
  file = {/home/dimitri/Nextcloud/Zotero/storage/BG7E5NIB/handbook_of_kr.pdf},
  note = {OCLC: ocn171550939}
}

@book{brownleeCleverAlgorithmsNatureinspired2012,
  langid = {english},
  location = {{s.l.}},
  title = {Clever Algorithms: Nature-Inspired Programming Recipes},
  edition = {Revision 2},
  isbn = {978-1-4467-8506-5},
  shorttitle = {Clever Algorithms},
  pagetotal = {425},
  publisher = {{LuLu.com}},
  date = {2012},
  author = {Brownlee, Jason},
  file = {/home/dimitri/Nextcloud/Zotero/storage/WPB66L35/CleverAlgorithms.pdf},
  note = {OCLC: 826547864}
}

@article{heardBeautifulCodeCompelling2008,
  title = {Beautiful {{Code}}, {{Compelling Evidence}}},
  journaltitle = {Functional Programming For Information Visualization and Visual Analytics},
  date = {2008},
  author = {Heard, J. R.},
  file = {/home/dimitri/Nextcloud/Zotero/storage/PGHKEZ48/BeautifulCode.pdf}
}

@book{krishnaAuctionTheory2009,
  title = {Auction Theory},
  publisher = {{Academic press}},
  date = {2009},
  author = {Krishna, Vijay},
  file = {/home/dimitri/Nextcloud/Zotero/storage/SBM9LED5/auctiontheory.pdf}
}

@book{harelAlgorithmicsSpiritComputing2004,
  location = {{Harlow, Essex, England ; New York}},
  title = {Algorithmics: The Spirit of Computing},
  edition = {3rd ed},
  isbn = {978-0-321-11784-7},
  shorttitle = {Algorithmics},
  pagetotal = {513},
  publisher = {{Addison Wesley : Pearson Education}},
  date = {2004},
  keywords = {Computer algorithms},
  author = {Harel, David and Feldman, Yishai A.},
  file = {/home/dimitri/Nextcloud/Zotero/storage/N7YPTAJV/Algorithmics The Spirit of Computing.pdf}
}

@article{dunningTimeSeriesDatabases2015,
  title = {Time Series Databases},
  journaltitle = {New Ways to Store and},
  date = {2015},
  author = {Dunning, Ted and Friedman, Ellen},
  file = {/home/dimitri/Nextcloud/Zotero/storage/74VPN68E/timeseriesdatabases_newwaystostoreandaccessdata.pdf}
}

@report{cuiFinitestateDirichletAllocation2006,
  title = {Finite-State {{Dirichlet}} Allocation: {{Learned}} Priors on Finite-State Models},
  shorttitle = {Finite-State {{Dirichlet}} Allocation},
  institution = {{Technical Report 53, Center for Language and Speech Processing, Johns Hopkins University}},
  date = {2006},
  author = {Cui, Jia and Eisner, Jason}
}

@book{downeyThinkStats2014,
  location = {{Sebastopol, CA}},
  title = {Think Stats},
  edition = {Second edition},
  isbn = {978-1-4919-0733-7 978-1-4919-0736-8},
  abstract = {Teaches the entire exploratory data analysis process using a single case study.--},
  pagetotal = {206},
  publisher = {{O'Reilly Media}},
  date = {2014},
  keywords = {Computer programs,MATHEMATICS / Probability & Statistics / General,Programmed instructional materials,Quantitative research,Statistics,Study and teaching},
  author = {Downey, Allen},
  file = {/home/dimitri/Nextcloud/Zotero/storage/IIG23KBC/thinkstats.pdf},
  note = {OCLC: ocn881657156}
}

@book{starkProbabilityStatisticsRandom2012,
  title = {Probability, Statistics, and Random Processes for Engineers},
  publisher = {{Pearson}},
  date = {2012},
  author = {Stark, Henry and Woods, John William}
}

@book{adlerNutshell2012,
  location = {{Beijing}},
  title = {R in a Nutshell},
  edition = {Second edition},
  isbn = {978-1-4493-1208-4},
  pagetotal = {699},
  publisher = {{O'Reilly}},
  date = {2012},
  keywords = {Data processing,Mathematical statistics,R (Computer program language)},
  author = {Adler, Joseph},
  file = {/home/dimitri/Nextcloud/Zotero/storage/P7V3QYRC/rinanutshell.pdf}
}

@inproceedings{kaskiDataExplorationUsing1997,
  title = {Data Exploration Using Self-Organizing Maps},
  booktitle = {Acta Polytechnica Scandinavica: {{Mathematics}}, Computing and Management in Engineering Series No. 82},
  publisher = {{Citeseer}},
  date = {1997},
  author = {Kaski, Samuel}
}

@book{dunningPracticalMachineLearning2014,
  location = {{Sebastopol, California}},
  title = {Practical Machine Learning: Innovations in Recommendation},
  edition = {First edition},
  isbn = {978-1-4919-1538-7},
  shorttitle = {Practical Machine Learning},
  pagetotal = {48},
  publisher = {{O'Reilly Media, Inc}},
  date = {2014},
  keywords = {Machine learning,Development},
  author = {Dunning, Ted and Friedman, B. Ellen},
  file = {/home/dimitri/Nextcloud/Zotero/storage/AEKV7BMC/practicalmachinelearning_innovationsinrecommendation.pdf}
}

@book{dunningPracticalMachineLearning2014a,
  location = {{Beijin}},
  title = {Practical Machine Learning: A New Look at Anomaly Detection},
  edition = {First edition},
  isbn = {978-1-4919-1160-0},
  shorttitle = {Practical Machine Learning},
  pagetotal = {58},
  publisher = {{O'Reilly}},
  date = {2014},
  keywords = {Machine learning,Anomaly detection (Computer security)},
  author = {Dunning, Ted and Friedman, B. Ellen},
  file = {/home/dimitri/Nextcloud/Zotero/storage/HKANK5B5/practicalmachinelearning_anewlookatanomalydetection.pdf}
}

@book{karauLearningSpark2015,
  location = {{Beijing ; Sebastopol}},
  title = {Learning {{Spark}}},
  edition = {First edition},
  isbn = {978-1-4493-5862-4},
  abstract = {This book introduces Apache Spark, the open source cluster computing system that makes data analytics fast to write and fast to run. You'll learn how to express parallel jobs with just a few lines of code, and cover applications from simple batch jobs to stream processing and machine learning.--},
  pagetotal = {254},
  publisher = {{O'Reilly}},
  date = {2015},
  keywords = {Computer programs,Big data,Data mining,Spark (Electronic resource : Apache Software Foundation)},
  author = {Karau, Holden and Konwinski, Andy and Wendell, Patrick and Zaharia, Matei},
  file = {/home/dimitri/Nextcloud/Zotero/storage/SBM7CTCN/learningspark.pdf},
  note = {OCLC: ocn844872440}
}

@book{miltonHeadFirstData2009,
  location = {{Beijing ; Sebastopol, CA}},
  title = {Head First Data Analysis},
  edition = {1st ed},
  isbn = {978-0-596-15393-9},
  abstract = {A guide for data managers and analyzers. It shares guidelines for identifying patterns, predicting future outcomes, and presenting findings to others},
  pagetotal = {445},
  series = {Head First Series},
  publisher = {{O'Reilly}},
  date = {2009},
  keywords = {Data processing,Data mining,Business,Knowledge management},
  author = {Milton, Michael},
  file = {/home/dimitri/Nextcloud/Zotero/storage/IZIXW4P3/headfirstdataanalysis.pdf},
  note = {OCLC: ocn433010793}
}

@book{whiteHadoopDefinitiveGuide2015,
  location = {{Beijing}},
  title = {Hadoop: The Definitive Guide},
  edition = {Fourth edition},
  isbn = {978-1-4919-0163-2},
  shorttitle = {Hadoop},
  pagetotal = {727},
  publisher = {{O'Reilly}},
  date = {2015},
  keywords = {Apache Hadoop,File organization (Computer science),Hadoop},
  author = {White, Tom},
  file = {/home/dimitri/Nextcloud/Zotero/storage/UV3MKRBZ/hadoop_thedefinitiveguide.pdf},
  note = {OCLC: ocn904818464}
}

@book{robinsonGraphDatabases2015,
  location = {{Beijing}},
  title = {Graph Databases},
  edition = {Second edition},
  isbn = {978-1-4919-3089-2},
  pagetotal = {218},
  publisher = {{O'Reilly}},
  date = {2015},
  keywords = {Data processing,Database design,Database management,Databases,Graph theory},
  author = {Robinson, Ian and Webber, James and Eifrem, Emil},
  file = {/home/dimitri/Nextcloud/Zotero/storage/U5RJT4XS/graphdatabases.pdf},
  note = {OCLC: ocn911172345}
}

@book{schuttDoingDataScience2013,
  location = {{Beijing ; Sebastopol}},
  title = {Doing Data Science},
  edition = {First edition},
  isbn = {978-1-4493-5865-5},
  pagetotal = {375},
  publisher = {{O'Reilly Media}},
  date = {2013},
  keywords = {Data structures (Computer science),Big data,Data mining,Database management,Cyberinfrastructure,Information science},
  author = {Schutt, Rachel and O'Neil, Cathy},
  file = {/home/dimitri/Nextcloud/Zotero/storage/Y9S9QVFH/doingdatascience.pdf},
  note = {OCLC: ocn827841776}
}

@book{janssensDataScienceCommand2014,
  location = {{Sebastopol, CA}},
  title = {Data Science at the Command Line},
  edition = {First edition},
  isbn = {978-1-4919-4785-2},
  abstract = {Discusses more than 80 command-line tools to obtain, scrub, explore, and model data},
  pagetotal = {191},
  publisher = {{O'Reilly}},
  date = {2014},
  keywords = {Database management,Information science,Command languages (Computer science),Electronic data processing},
  author = {Janssens, Jeroen},
  file = {/home/dimitri/Nextcloud/Zotero/storage/U9RN3L7S/datascienceatthecommandline.pdf},
  note = {OCLC: ocn894638627}
}

@article{seltzerRelationalDatabases2008,
  title = {Beyond Relational Databases},
  volume = {51},
  number = {7},
  journaltitle = {Communications of the ACM},
  date = {2008},
  pages = {52--58},
  author = {Seltzer, Margo}
}

@book{karauHighPerformanceSpark2017,
  langid = {english},
  title = {High Performance {{Spark}} Best Practices for Scaling and Optimizing {{Apache Spark}}},
  isbn = {978-1-4919-4317-5 978-1-4919-4315-1},
  url = {http://search.ebscohost.com/login.aspx?direct=true&scope=site&db=nlebk&db=nlabk&AN=1526486},
  publisher = {{Oreilly \& Associates Inc}},
  urldate = {2017-11-03},
  date = {2017},
  author = {Karau, Holden},
  file = {/home/dimitri/Nextcloud/Zotero/storage/8EQG8S3N/highperformancespark.pdf},
  note = {OCLC: 988282120}
}

@book{downeyThinkBayes2013,
  location = {{Sebastopol, CA}},
  title = {Think {{Bayes}}},
  edition = {First edition},
  isbn = {978-1-4493-7078-7},
  pagetotal = {190},
  publisher = {{O'Reilly}},
  date = {2013},
  keywords = {Data processing,Bayesian statistical decision theory,Python (Computer program language)},
  author = {Downey, Allen},
  file = {/home/dimitri/Nextcloud/Zotero/storage/3BSPSVFG/thinkbayes.pdf},
  note = {OCLC: ocn858900827}
}

@book{carpenterCassandraDefinitiveGuide2016,
  location = {{Sebastopol, CA}},
  title = {Cassandra: The Definitive Guide},
  edition = {Second edition},
  isbn = {978-1-4919-3366-4},
  shorttitle = {Cassandra},
  pagetotal = {342},
  publisher = {{O'Reilly Media, Inc}},
  date = {2016},
  keywords = {Database management,Distributed databases,Open source software},
  author = {Carpenter, Jeff and Hewitt, Eben},
  file = {/home/dimitri/Nextcloud/Zotero/storage/DWIN9YSG/cassandra_thedefinitiveguide.pdf},
  note = {OCLC: ocn913137517}
}

@book{kirkThoughtfulMachineLearning2017,
  location = {{Beijing ; Boston}},
  title = {Thoughtful Machine Learning with {{Python}}: A Test-Driven Approach},
  edition = {First edition},
  isbn = {978-1-4919-2413-6},
  shorttitle = {Thoughtful Machine Learning with {{Python}}},
  pagetotal = {201},
  publisher = {{O'Reilly}},
  date = {2017},
  keywords = {Machine learning,Python (Computer program language)},
  author = {Kirk, Matthew},
  file = {/home/dimitri/Nextcloud/Zotero/storage/CSDF3XWI/thoughtfulmachinelearningwithpython.pdf},
  note = {OCLC: ocn908375399}
}

@book{cookPracticalMachineLearning2017,
  langid = {english},
  location = {{Beijing Boston Farnham Sebastopol Tokyo}},
  title = {Practical Machine Learning with {{H2O}}: Powerful, Scalable Techniques for Deep Learning and {{AI}}},
  edition = {First edition},
  isbn = {978-1-4919-6460-6},
  shorttitle = {Practical Machine Learning with {{H2O}}},
  pagetotal = {281},
  publisher = {{O'Reilly Media}},
  date = {2017},
  author = {Cook, Darren},
  file = {/home/dimitri/Nextcloud/Zotero/storage/E2SPNL3R/Cook - 2017 - Practical machine learning with H2O powerful, sca.pdf;/home/dimitri/Nextcloud/Zotero/storage/IXRVGR2M/practicalmachinelearningwithh2o.pdf},
  note = {OCLC: 992150370}
}

@book{murrayInteractiveDataVisualization2013,
  location = {{Sebastopol, CA}},
  title = {Interactive Data Visualization for the Web},
  edition = {1st ed},
  isbn = {978-1-4493-3973-9 978-1-4493-6108-2},
  abstract = {Author Scott Murray teaches you the fundamental concepts and methods of D3, a JavaScript library that lets you express data visually in a web browser --},
  pagetotal = {255},
  publisher = {{O'Reilly Media}},
  date = {2013},
  keywords = {Interactive computer graphics,JavaScript (Computer program language)},
  author = {Murray, Scott},
  file = {/home/dimitri/Nextcloud/Zotero/storage/97I9GH7W/Interactive Data Visualization For The Web.pdf},
  note = {OCLC: ocn838647635}
}

@book{dewarGettingStartedD32012,
  langid = {english},
  location = {{Beijing}},
  title = {Getting {{Started}} with {{D3}}: Creating Data-Driven Documents},
  edition = {1. ed},
  isbn = {978-1-4493-2879-5},
  shorttitle = {Getting {{Started}} with {{D3}}},
  pagetotal = {57},
  publisher = {{O'Reilly}},
  date = {2012},
  author = {Dewar, Mike},
  file = {/home/dimitri/Nextcloud/Zotero/storage/2PNLBL2S/Getting_Started_with_D3.pdf},
  note = {OCLC: 844085072}
}

@book{zhuDataVisualizationD32013,
  langid = {english},
  location = {{Birmingham}},
  title = {Data Visualization with {{D3}}.Js Cookbook: Over 70 Recipes to Create Dynamic Data-Driven Visualization with {{D3}}.Js},
  isbn = {978-1-78216-216-2},
  shorttitle = {Data Visualization with {{D3}}.Js Cookbook},
  pagetotal = {321},
  series = {Quick Answers to Common Problems},
  publisher = {{Packt Publ}},
  date = {2013},
  author = {Zhu, Nick},
  file = {/home/dimitri/Nextcloud/Zotero/storage/CUXMQNU7/Data Visualization with D3.js Cookbook.pdf},
  note = {OCLC: 865074944}
}

@book{kornerDataVisualizationD32015,
  langid = {english},
  location = {{Birmingham Mumbai}},
  title = {Data Visualization with {{D3}} and {{AngularJS}}: Build Dynamic and Interactive Visualizations from Real-World Data with {{D3}} on {{AngularJS}}},
  isbn = {978-1-78439-848-4},
  shorttitle = {Data Visualization with {{D3}} and {{AngularJS}}},
  pagetotal = {253},
  series = {Community Experience Distilled},
  publisher = {{Packt Publishing}},
  date = {2015},
  author = {Körner, Christoph},
  file = {/home/dimitri/Nextcloud/Zotero/storage/JKIICYF4/Data Visualization with D3 and AngularJS (2015).pdf;/home/dimitri/Nextcloud/Zotero/storage/VNTT9D8Z/Körner - 2015 - Data visualization with D3 and AngularJS build dy.pdf}
}

@book{castilloMasteringD3Js2014,
  langid = {english},
  location = {{Birmingham, UK}},
  title = {Mastering {{D3}}.Js Bring Your Data to Life by Creating and Deploying Complex Data Visualizations with {{D3}}.Js},
  isbn = {978-1-78328-628-7 978-1-78328-627-0},
  url = {http://0proquest.safaribooksonline.com/9781783286270},
  publisher = {{Packt Pub.}},
  urldate = {2017-11-03},
  date = {2014},
  author = {Castillo, Pablo Navarro},
  file = {/home/dimitri/Nextcloud/Zotero/storage/9SY8T3FU/Mastering D3.js (2014).pdf},
  note = {OCLC: 929767632}
}

@book{mankiwMacroeconomics2013,
  location = {{New York, NY}},
  title = {Macroeconomics},
  edition = {8th ed},
  isbn = {978-1-4292-4002-4},
  pagetotal = {625},
  publisher = {{Worth}},
  date = {2013},
  keywords = {Macroeconomics},
  author = {Mankiw, N. Gregory},
  file = {/home/dimitri/Nextcloud/Zotero/storage/4DFZXWND/Mankiw Macroeconomics 8th c2013 txtbk.pdf}
}

@book{blanchardMACROECONOMICS2013,
  location = {{Boston}},
  title = {{{MACROECONOMICS}}},
  edition = {Sixth Edition},
  isbn = {978-0-13-306163-5},
  pagetotal = {553},
  publisher = {{Pearson}},
  date = {2013},
  keywords = {Macroeconomics},
  author = {Blanchard, Olivier and Johnson, David R.},
  file = {/home/dimitri/Nextcloud/Zotero/storage/IYAQM9YJ/Macroeconomics (6th Edition) - Blanchard, Olivier.pdf}
}

@book{samuelsonEconomics2010,
  location = {{Boston}},
  title = {Economics},
  edition = {19th ed},
  isbn = {978-0-07-351129-0},
  pagetotal = {715},
  series = {The {{McGraw}}-{{Hill}} Series Economics},
  publisher = {{McGraw-Hill Irwin}},
  date = {2010},
  keywords = {Economics},
  author = {Samuelson, Paul A. and Nordhaus, William D.},
  file = {/home/dimitri/Nextcloud/Zotero/storage/5225RYTH/Economics 19e - Paul Samuelson, William Nordhaus.pdf}
}

@book{yiuDefinitiveGuideARM2013,
  title = {The {{Definitive Guide}} to {{ARM}}® {{Cortex}}®-{{M3}} and {{Cortex}}®-{{M4 Processors}}},
  publisher = {{Newnes}},
  date = {2013},
  author = {Yiu, Joseph},
  file = {/home/dimitri/Nextcloud/Zotero/storage/M3XDTIC8/Yiu J. - The Definitive Guide to ARM Cortex-M3 and Cortex-M4 Processors, 3rd Edition - 2014.pdf}
}

@collection{brownRadioElectronicsCookbook2001,
  location = {{Oxford}},
  title = {Radio and Electronics Cookbook},
  isbn = {978-0-7506-5214-8},
  pagetotal = {319},
  publisher = {{Newnes}},
  date = {2001},
  keywords = {Electronics,Radio},
  editor = {Brown, George and Radio Society of Great Britain},
  file = {/home/dimitri/Nextcloud/Zotero/storage/WVKDLLM9/RSGB-Radio and Electronics Cookbook-Newnes (2001).pdf},
  note = {OCLC: ocm45488561}
}

@book{sedraMicroelectronicCircuits2015,
  location = {{New York ; Oxford}},
  title = {Microelectronic Circuits},
  edition = {Seventh edition},
  isbn = {978-0-19-933913-6 978-0-19-933918-1},
  pagetotal = {1},
  series = {The {{Oxford}} Series in Electrical and Computer Engineering},
  publisher = {{Oxford University Press}},
  date = {2015},
  keywords = {Electronic circuits,Integrated circuits},
  author = {Sedra, Adel S. and Smith, Kenneth C.},
  file = {/home/dimitri/Nextcloud/Zotero/storage/U35NKUV9/microelectroniccircuits.pdf}
}

@book{whiteMakingEmbeddedSystems2012,
  langid = {english},
  location = {{Beijing}},
  title = {Making Embedded Systems: Design Patterns for Great Software},
  edition = {1. ed},
  isbn = {978-1-4493-0214-6},
  shorttitle = {Making Embedded Systems},
  pagetotal = {310},
  publisher = {{O'Reilly}},
  date = {2012},
  author = {White, Elecia},
  file = {/home/dimitri/Nextcloud/Zotero/storage/GKTHE67G/making_embedded_systems.pdf},
  note = {OCLC: 815881891}
}

@book{hartmanMakeWearableElectronics2014,
  location = {{Sebastopol, CA}},
  title = {Make: Wearable Electronics},
  edition = {First edition},
  isbn = {978-1-4493-3651-6},
  shorttitle = {Make},
  abstract = {Make: Wearable Electronics is intended for those with an interest in physical computing who are looking to create interfaces or systems that live on the body. Perfect for makers new to wearable tech, this book introduces you to the tools, materials, and techniques for creating interactive electronic circuits and embedding them in clothing and other things you can wear. Each chapter features experiments to get you comfortable with the technology and then invites you to build upon that knowledge with your own projects. Fully illustrated with step-by-step instructions and images of amazing creations made by artists and professional designers, this book offers a concrete understanding of electronic circuits and how you can use them to bring your wearable projects from concept to prototype},
  pagetotal = {257},
  publisher = {{Maker Media}},
  date = {2014},
  keywords = {Design and construction,Human-computer interaction,Wearable computers,Wearable technology},
  author = {Hartman, Kate and Jepson, Brian and Dvorak, Emma and Demarest, Rebecca},
  file = {/home/dimitri/Nextcloud/Zotero/storage/PQ5J3ZDC/Make Wearable Electronics Design.pdf},
  note = {OCLC: ocn890200431}
}

@book{plattMakeElectronicsLearning2010,
  langid = {english},
  location = {{Beijing}},
  title = {Make: Electronics: Learning through Discovery},
  edition = {1. ed},
  isbn = {978-0-596-15374-8},
  shorttitle = {Make},
  pagetotal = {334},
  series = {Make: Makezine.Com},
  publisher = {{O'Reilly}},
  date = {2010},
  author = {Platt, Charles},
  file = {/home/dimitri/Nextcloud/Zotero/storage/2GNBLD3H/Make Electronics Learning Through Discovery.pdf},
  note = {OCLC: 837320134}
}

@book{purdumArduinoProjectsAmateur2014,
  langid = {english},
  title = {Arduino {{Projects}} for {{Amateur Radio}}},
  isbn = {978-0-07-183406-3},
  url = {https://www.overdrive.com/search?q=D77BFE02-97DC-446D-BBCC-E378D0086FBF},
  abstract = {BOOST YOUR HAM RADIO'S CAPABILITIES USING LOW-COST ARDUINO MICROCONTROLLER BOARDS! Do you want to increase the functionality and value of your ham radio without spending a lot of money? This book will show you how! Arduino Projects for Amateur Radio is filled with step-by-step microcontroller projects you can accomplish on your own⁰́₄no programming experience necessary. After getting you set up on an Arduino board, veteran ham radio operators Jack Purdum (W8TEE) and Dennis Kidder (W6DQ) start with a simple LCD display and move up to projects that can add hundreds of dollars' worth of upgrades to existing equipment. This practical guide provides detailed instructions, helpful diagrams, lists of low-cost parts and suppliers, and hardware and software tips that make building your own equipment even more enjoyable. Downloadable code for all of the projects in the book is also available. Do-it-yourself projects include: LCD shield Station timer General purpose panel meter Dummy load and watt meter CW automatic keyer Morse code decoder PS2 keyboard CW encoder Universal relay shield Flexible sequencer Rotator controller Directional watt and SWR meter Simple frequency counter DDS VFO Portable solar power source},
  urldate = {2017-11-03},
  date = {2014},
  author = {Purdum, Jack and Kidder, Dennis},
  file = {/home/dimitri/Nextcloud/Zotero/storage/AH9DFTRB/Jack Purdum, Dennis Kidder-Arduino Projects for Amateur Radio-McGraw-Hill_TAB Electronics (2014).pdf},
  note = {OCLC: 896873984}
}

@book{bakosEmbeddedSystemsARM2016,
  location = {{Amsterdam}},
  title = {Embedded Systems: {{ARM}} Programming and Optimization},
  isbn = {978-0-12-800342-8},
  shorttitle = {Embedded Systems},
  pagetotal = {300},
  publisher = {{Morgan Kaufmann}},
  date = {2016},
  keywords = {Embedded computer systems,Linux,Microprocessors,Operating systems (Computers),Programming},
  author = {Bakos, Jason D.},
  file = {/home/dimitri/Nextcloud/Zotero/storage/959DW2EC/Embedded Systems - ARM® Programming and Optimization.pdf},
  note = {OCLC: ocn925305432}
}

@article{radioBEGINNERHANDBOOKAMATEUR2000,
  title = {{{THE BEGINNER}}’{{S HANDBOOK OF AMATEUR RADIO}}},
  date = {2000},
  author = {RADIO, AMATEUR},
  file = {/home/dimitri/Nextcloud/Zotero/storage/HZRGYKAM/Clay Laster-The Beginner's Handbook of Amateur Radio 4th Edition-McGraw-Hill_TAB Electronics (2000).pdf}
}

@collection{yuanThisChinaFirst2010,
  location = {{Great Barrington, Mass}},
  title = {This Is {{China}}: The First 5,000 Years},
  edition = {1st ed},
  isbn = {978-1-933782-20-1 978-1-933782-76-8},
  shorttitle = {This Is {{China}}},
  pagetotal = {134},
  number = {2},
  series = {This World of Ours},
  publisher = {{Berkshire Pub. Group}},
  date = {2010},
  keywords = {China,Civilization,History},
  editor = {Yuan, Haiwang},
  file = {/home/dimitri/Nextcloud/Zotero/storage/H3GJ44C9/Haiwang_Yuan,_Ronald_G._Knapp,_Margot_E._Landman,_Gregory_Veeck_This_Is_China_The_First_5,000_Years_This_World_of_Ours__.pdf},
  note = {OCLC: ocn540644070}
}

@book{bermanLawRevolutionFormation1983,
  location = {{Cambridge, Mass}},
  title = {Law and Revolution: The Formation of the {{Western}} Legal Tradition},
  isbn = {978-0-674-51774-5},
  shorttitle = {Law and Revolution},
  pagetotal = {657},
  publisher = {{Harvard University Press}},
  date = {1983},
  keywords = {History,Law},
  author = {Berman, Harold J.},
  file = {/home/dimitri/Nextcloud/Zotero/storage/8XP5J8Q9/Harold J. Berman-Law and Revolution, The Formation of the Western Legal Tradition-Harvard University Press (1983).pdf}
}

@book{ebreyJianQiaoChaTuZhongGuoShiCambridgeIllustrated2001,
  langid = {Chinese},
  location = {{济南市}},
  title = {剑桥插图中国史 = The Cambridge illustrated history of China},
  isbn = {978-7-80603-493-4},
  publisher = {{山東畫報出版社}},
  date = {2001},
  author = {Ebrey, Patricia Buckley},
  file = {/home/dimitri/Nextcloud/Zotero/storage/JRG47KQN/Patricia_Buckley_Ebrey,_Kwang-ching_Liu_The_Cambridge_Illustrated_History_of_China.pdf},
  note = {OCLC: 48175720}
}

@book{gowersPrincetonCompanionMathematics2010,
  title = {The {{Princeton}} Companion to Mathematics},
  publisher = {{Princeton University Press}},
  date = {2010},
  author = {Gowers, Timothy and Barrow-Green, June and Leader, Imre},
  file = {/home/dimitri/Nextcloud/Zotero/storage/CBXJ7JYN/Gowers et al. - 2010 - The Princeton companion to mathematics.pdf}
}

@book{fieldSymmetryChaosSearch2009,
  location = {{Philadelphia, PA}},
  title = {Symmetry in Chaos: A Search for Pattern in Mathematics, Art, and Nature},
  edition = {2nd ed},
  isbn = {978-0-89871-672-6},
  shorttitle = {Symmetry in Chaos},
  pagetotal = {199},
  publisher = {{Society for Industrial and Applied Mathematics}},
  date = {2009},
  keywords = {Chaotic behavior in systems,Symmetry},
  author = {Field, Mike and Golubitsky, Martin},
  file = {/home/dimitri/Nextcloud/Zotero/storage/H632DHEZ/[Michael_Field,_Martin_Golubitsky]_Symmetry_in_Cha(BookFi.org).pdf},
  note = {OCLC: ocn298324451}
}

@book{wilsonFourColorsSuffice2002,
  location = {{Princeton, NJ}},
  title = {Four Colors Suffice: How the Map Problem Was Solved},
  isbn = {978-0-691-11533-7},
  shorttitle = {Four Colors Suffice},
  pagetotal = {262},
  publisher = {{Princeton University Press}},
  date = {2002},
  keywords = {History,Four-color problem,Mathematical recreations},
  author = {Wilson, Robin J.},
  file = {/home/dimitri/Nextcloud/Zotero/storage/MSVFMCDM/mcs.pdf},
  note = {OCLC: ocm51730619}
}

@book{rovelliCovariantLoopQuantum2014,
  title = {Covariant {{Loop Quantum Gravity}}: {{An Elementary Introduction}} to {{Quantum Gravity}} and {{Spinfoam Theory}}},
  shorttitle = {Covariant {{Loop Quantum Gravity}}},
  publisher = {{Cambridge University Press}},
  date = {2014},
  author = {Rovelli, Carlo and Vidotto, Francesca},
  file = {/home/dimitri/Nextcloud/Zotero/storage/ENSGWLQR/IntroductionLQG.pdf}
}

@article{rentelnFoolproofSamplingMathematical2005,
  title = {Foolproof: {{A}} Sampling of Mathematical Folk Humor},
  volume = {52},
  shorttitle = {Foolproof},
  number = {1},
  journaltitle = {Notices of the AMS},
  date = {2005},
  pages = {24--34},
  author = {Renteln, Paul and Dundes, Alan},
  file = {/home/dimitri/Nextcloud/Zotero/storage/P68LVWSV/fea-dundes.pdf}
}

@book{allsoppUnauthorisedAccessPhysical2009,
  location = {{Chichester, West Sussex, U.K}},
  title = {Unauthorised Access: Physical Penetration Testing for {{IT}} Security Teams},
  isbn = {978-0-470-74761-2},
  shorttitle = {Unauthorised Access},
  pagetotal = {287},
  publisher = {{Wiley}},
  date = {2009},
  keywords = {Computer networks,Computer security,Computer software,Penetration testing (Computer security),Reliability,Security measures,Testing},
  author = {Allsopp, Wil},
  file = {/home/dimitri/Nextcloud/Zotero/storage/HI5VZWMJ/Unauthorised Access - Physical Penetration Testing For IT Security Teams[NepsterJay].pdf},
  note = {OCLC: ocn419866397}
}

@book{katzIntroductionModernCryptography2008,
  location = {{Boca Raton}},
  title = {Introduction to Modern Cryptography},
  isbn = {978-1-58488-551-1},
  pagetotal = {534},
  series = {Chapman \& {{Hall}}/{{CRC}} Cryptography and Network Security},
  publisher = {{Chapman \& Hall/CRC}},
  date = {2008},
  keywords = {Computer security,Cryptography},
  author = {Katz, Jonathan and Lindell, Yehuda},
  file = {/home/dimitri/Nextcloud/Zotero/storage/R2HZCQWJ/Introduction to Modern Cryptography - J. Katz, Y. Lindell (Chapman and Hall, 2008) WW.pdf},
  note = {OCLC: ocn137325053}
}

@book{stallingsDataComputerCommunications2007,
  location = {{Upper Saddle River, N.J}},
  title = {Data and Computer Communications},
  edition = {8th ed},
  isbn = {978-0-13-243310-5},
  pagetotal = {878},
  publisher = {{Pearson/Prentice Hall}},
  date = {2007},
  keywords = {Computer networks,Data transmission systems},
  author = {Stallings, William}
}

@book{stallingsCryptographyNetworkSecurity2011,
  location = {{Boston}},
  title = {Cryptography and Network Security: Principles and Practice},
  edition = {5th ed},
  isbn = {978-0-13-609704-4},
  shorttitle = {Cryptography and Network Security},
  pagetotal = {719},
  publisher = {{Prentice Hall}},
  date = {2011},
  keywords = {Computer networks,Computer security,Security measures,Coding theory,Data encryption (Computer science)},
  author = {Stallings, William},
  file = {/home/dimitri/Nextcloud/Zotero/storage/YCHE3R57/Cryptography and Network Security - Prins and Pract. 5th ed - W. Stallings (Pearson, 2011) BBS.pdf}
}

@article{wilsonFacialTrustworthinessPredicts2015,
  langid = {english},
  title = {Facial {{Trustworthiness Predicts Extreme Criminal}}-{{Sentencing Outcomes}}},
  volume = {26},
  issn = {0956-7976, 1467-9280},
  url = {http://journals.sagepub.com/doi/10.1177/0956797615590992},
  doi = {10.1177/0956797615590992},
  number = {8},
  journaltitle = {Psychological Science},
  urldate = {2017-11-03},
  date = {2015-08},
  pages = {1325-1331},
  author = {Wilson, John Paul and Rule, Nicholas O.},
  file = {/home/dimitri/Nextcloud/Zotero/storage/QQ4RPDRN/wilson2015.pdf}
}

@article{suchardUnderstandingGPUProgramming2010,
  langid = {english},
  title = {Understanding {{GPU Programming}} for {{Statistical Computation}}: {{Studies}} in {{Massively Parallel Massive Mixtures}}},
  volume = {19},
  issn = {1061-8600, 1537-2715},
  url = {http://www.tandfonline.com/doi/abs/10.1198/jcgs.2010.10016},
  doi = {10.1198/jcgs.2010.10016},
  shorttitle = {Understanding {{GPU Programming}} for {{Statistical Computation}}},
  number = {2},
  journaltitle = {Journal of Computational and Graphical Statistics},
  urldate = {2017-11-03},
  date = {2010-01},
  pages = {419-438},
  author = {Suchard, Marc A. and Wang, Quanli and Chan, Cliburn and Frelinger, Jacob and Cron, Andrew and West, Mike},
  file = {/home/dimitri/Nextcloud/Zotero/storage/ZP7HW8VL/Suchard2010.pdf}
}

@article{plummerCutsBayesianGraphical2015,
  langid = {english},
  title = {Cuts in {{Bayesian}} Graphical Models},
  volume = {25},
  issn = {0960-3174, 1573-1375},
  url = {http://link.springer.com/10.1007/s11222-014-9503-z},
  doi = {10.1007/s11222-014-9503-z},
  number = {1},
  journaltitle = {Statistics and Computing},
  urldate = {2017-11-03},
  date = {2015-01},
  pages = {37-43},
  author = {Plummer, Martyn},
  file = {/home/dimitri/Nextcloud/Zotero/storage/B4CGX4HS/paperplummer.pdf}
}

@book{ramseyStatisticalSleuthCourse2013,
  location = {{Australia ; Boston}},
  title = {The Statistical Sleuth: A Course in Methods of Data Analysis},
  edition = {3rd ed},
  isbn = {978-1-133-49067-8},
  shorttitle = {The Statistical Sleuth},
  pagetotal = {760},
  publisher = {{Brooks/Cole, Cengage Learning}},
  date = {2013},
  keywords = {Mathematical statistics},
  author = {Ramsey, Fred L. and Schafer, Daniel W.},
  file = {/home/dimitri/Nextcloud/Zotero/storage/V3U8U8W4/Fred_Ramsey,_Daniel_W._Schafer_The_Statistical_Sleuth_A_Course_in_Methods_of_Data_Analysis.pdf},
  note = {OCLC: ocn794592462}
}

@book{dobsonIntroductionGeneralizedLinear2008,
  location = {{Boca Raton}},
  title = {An Introduction to Generalized Linear Models},
  edition = {3rd ed},
  isbn = {978-1-58488-950-2},
  pagetotal = {307},
  series = {Chapman \& {{Hall}}/{{CRC}} Texts in Statistical Science Series},
  publisher = {{CRC Press}},
  date = {2008},
  keywords = {Linear models (Statistics)},
  author = {Dobson, Annette J. and Barnett, Adrian G.},
  file = {/home/dimitri/Nextcloud/Zotero/storage/Z7SRXTTB/Annette_J._Dobson,_Adrian_Barnett_An_Introduction_to_Generalized_Linear_Models.pdf},
  note = {OCLC: ocn213602344}
}

@book{davisonStatisticalModels2003,
  langid = {english},
  location = {{Leiden}},
  title = {Statistical {{Models}}.},
  isbn = {978-0-511-67299-6},
  url = {http://public.eblib.com/choice/publicfullrecord.aspx?p=487306},
  abstract = {Gives an integrated development of models and likelihood that blends theory and practice.},
  publisher = {{Cambridge University Press}},
  urldate = {2017-11-03},
  date = {2003},
  author = {Davison, A. C},
  file = {/home/dimitri/Nextcloud/Zotero/storage/HWH2K8NK/A._C._Davison_Statistical_Models_Cambridge_Series_in_Statistical_and_Probabilistic_Mathematics.pdf},
  note = {OCLC: 609856537}
}

@book{farawayExtendingLinearModel2016,
  langid = {english},
  title = {Extending the Linear Model with {{R}}: Generalized Linear, Mixed Effects and Nonparametric Regression Models},
  isbn = {978-1-4987-2098-4},
  url = {http://ebookcentral.proquest.com/lib/ubstgallen-ebooks/detail.action?docID=4711494},
  shorttitle = {Extending the Linear Model with {{R}}},
  urldate = {2017-11-03},
  date = {2016},
  author = {Faraway, Julian James},
  file = {/home/dimitri/Nextcloud/Zotero/storage/IWH3AYYW/Faraway,_Julian_James_Extending_the_linear_model_with_R_generalized_linear,_mixed_effects_and_nonparametric_regression_models.pdf},
  note = {OCLC: 961996425}
}

@book{farawayLinearModels2015,
  langid = {english},
  title = {Linear Models with {{R}}},
  isbn = {978-1-4398-8734-9},
  url = {http://www.crcnetbase.com/isbn/9781439887349},
  abstract = {Part of the core of statistics, linear models are used to make predictions and explain the relationship between the response and the predictors. Understanding linear models is crucial to a broader competence in the practice of statistics. Linear Models with R, Second Edition explains how to use linear models in physical science, engineering, social science, and business applications.},
  urldate = {2017-11-03},
  date = {2015},
  author = {Faraway, Julian James},
  file = {/home/dimitri/Nextcloud/Zotero/storage/QD2DDJ73/Julian_J_Faraway_Linear_Models_with_R.pdf},
  note = {OCLC: 908262161}
}

@book{hojsgaardGraphicalModels2012,
  langid = {english},
  location = {{Boston, MA}},
  title = {Graphical {{Models}} with {{R}}},
  isbn = {978-1-4614-2298-3 978-1-4614-2299-0},
  url = {http://link.springer.com/10.1007/978-1-4614-2299-0},
  publisher = {{Springer US}},
  urldate = {2017-11-03},
  date = {2012},
  author = {Højsgaard, Søren and Edwards, David and Lauritzen, Steffen},
  file = {/home/dimitri/Nextcloud/Zotero/storage/BBZ74BGJ/Søren_Højsgaard,_David_Edwards,_Steffen_Lauritzen_auth._Graphical_Models_with_R.pdf},
  doi = {10.1007/978-1-4614-2299-0}
}

@book{pearlCausalityModelsReasoning2000,
  location = {{Cambridge, U.K. ; New York}},
  title = {Causality: Models, Reasoning, and Inference},
  isbn = {978-0-521-89560-6 978-0-521-77362-1},
  shorttitle = {Causality},
  pagetotal = {384},
  publisher = {{Cambridge University Press}},
  date = {2000},
  keywords = {Probabilities,Causation},
  author = {Pearl, Judea},
  file = {/home/dimitri/Nextcloud/Zotero/storage/2VS2UUJA/Judea_Pearl_Causality_Models,_Reasoning_and_Inference.pdf}
}

@book{kollerProbabilisticGraphicalModels2009,
  location = {{Cambridge, MA}},
  title = {Probabilistic Graphical Models: Principles and Techniques},
  isbn = {978-0-262-01319-2},
  shorttitle = {Probabilistic Graphical Models},
  pagetotal = {1231},
  series = {Adaptive Computation and Machine Learning},
  publisher = {{MIT Press}},
  date = {2009},
  keywords = {Bayesian statistical decision theory,Graphic methods,Graphical modeling (Statistics)},
  author = {Koller, Daphne and Friedman, Nir},
  file = {/home/dimitri/Nextcloud/Zotero/storage/SBKAT6SU/Daphne_Koller,_Nir_Friedman_Probabilistic_Graphical_Models_Principles_and_Techniques.pdf}
}

@book{lauritzenGraphicalModels1996,
  location = {{Oxford : New York}},
  title = {Graphical Models},
  isbn = {978-0-19-852219-5},
  pagetotal = {298},
  number = {17},
  series = {Oxford Statistical Science Series},
  publisher = {{Clarendon Press ; Oxford University Press}},
  date = {1996},
  keywords = {Graphical modeling (Statistics)},
  author = {Lauritzen, Steffen L.},
  file = {/home/dimitri/Nextcloud/Zotero/storage/ICJW23VF/Steffen_L._Lauritzen_Graphical_models.djvu}
}

@book{durrettRandomGraphDynamics2007,
  langid = {english},
  location = {{Cambridge; New York}},
  title = {Random Graph Dynamics},
  isbn = {978-0-511-34983-6 978-0-521-86656-9 978-0-511-54659-4},
  url = {http://site.ebrary.com/id/10408675},
  publisher = {{Cambridge University Press}},
  urldate = {2017-11-03},
  date = {2007},
  author = {Durrett, Richard},
  file = {/home/dimitri/Nextcloud/Zotero/storage/6TBNWFMY/Rick_Durrett_Random_graph_dynamics.pdf},
  note = {OCLC: 182819757}
}

@book{newmanNetworksIntroduction2010,
  location = {{Oxford ; New York}},
  title = {Networks: An Introduction},
  isbn = {978-0-19-920665-0},
  shorttitle = {Networks},
  abstract = {"The scientific study of networks, including computer networks, social networks, and biological networks, has received an enormous amount of interest in the last few years. The rise of the Internet and the wide availability of inexpensive computers have made it possible to gather and analyze network data on a large scale, and the development of a variety of new theoretical tools has allowed us to extract new knowledge from many different kinds of networks. The study of networks is broadly interdisciplinary and important developments have occurred in many fields, including mathematics, physics, computer and information sciences, biology, and the social sciences. This book brings together for the first time the most important breakthroughs in each of these fields and presents them in a coherent fashion, highlighting the strong interconnections between work in different areas. Subjects covered include the measurement and structure of networks in many branches of science, methods for analyzing network data, including methods developed in physics, statistics, and sociology, the fundamentals of graph theory, computer algorithms, and spectral methods, mathematical models of networks, including random graph models and generative models, and theories of dynamical processes taking place on networks"--},
  pagetotal = {772},
  publisher = {{Oxford University Press}},
  date = {2010},
  keywords = {Engineering systems,Network analysis (Planning),Social systems,System analysis,Systems biology},
  author = {Newman, M. E. J.},
  file = {/home/dimitri/Nextcloud/Zotero/storage/FDMM48IV/Mark_Newman_Networks_An_Introduction.pdf},
  note = {OCLC: ocn456837194}
}

@book{chenNormalApproximationStein2011,
  location = {{Berlin, Heidelberg}},
  title = {Normal {{Approximation}} by {{Stein}}’s {{Method}}},
  isbn = {978-3-642-15006-7 978-3-642-15007-4},
  url = {http://link.springer.com/10.1007/978-3-642-15007-4},
  series = {Probability and {{Its Applications}}},
  publisher = {{Springer Berlin Heidelberg}},
  urldate = {2017-11-03},
  date = {2011},
  author = {Chen, Louis H.Y. and Goldstein, Larry and Shao, Qi-Man},
  file = {/home/dimitri/Nextcloud/Zotero/storage/DYURNDFV/Louis_H.Y._Chen,_Larry_Goldstein,_Qi-Man_Shao_auth._Normal_Approximation_by_Stein’s_Method.pdf},
  doi = {10.1007/978-3-642-15007-4}
}

@book{kolaczykStatisticalAnalysisNetwork2014,
  location = {{New York, NY}},
  title = {Statistical {{Analysis}} of {{Network Data}} with {{R}}},
  volume = {65},
  isbn = {978-1-4939-0982-7 978-1-4939-0983-4},
  url = {http://link.springer.com/10.1007/978-1-4939-0983-4},
  series = {Use {{R}}!},
  publisher = {{Springer New York}},
  urldate = {2017-11-03},
  date = {2014},
  author = {Kolaczyk, Eric D. and Csárdi, Gábor},
  file = {/home/dimitri/Nextcloud/Zotero/storage/ULWE5ZDG/Eric_D._Kolaczyk,_Gábor_Csárdi_auth._Statistical_Analysis_of_Network_Data_with_R.pdf},
  doi = {10.1007/978-1-4939-0983-4}
}

@article{barbourCentralLimitTheorem1989,
  title = {A Central Limit Theorem for Decomposable Random Variables with Applications to Random Graphs},
  volume = {47},
  number = {2},
  journaltitle = {Journal of Combinatorial Theory, Series B},
  date = {1989},
  pages = {125--145},
  author = {Barbour, Andrew D. and Karoński, Michal and Ruciński, Andrzej},
  file = {/home/dimitri/Nextcloud/Zotero/storage/3AG5G92T/barbour1989.pdf}
}

@collection{barbourIntroductionSteinMethod2005,
  location = {{Singapore : Hackensack, N.J}},
  title = {An Introduction to {{Stein}}'s Method},
  isbn = {978-981-256-280-7 978-981-256-330-9},
  pagetotal = {225},
  number = {v. 4},
  series = {Lecture Notes Series / {{Institute}} for {{Mathematical Sciences}}, {{National University}} of {{Singapore}}},
  publisher = {{Singapore University Press ; World Scientific}},
  date = {2005},
  keywords = {Probabilities,Approximation theory,Distribution (Probability theory)},
  editor = {Barbour, A. D. and Chen, Louis H. Y.},
  file = {/home/dimitri/Nextcloud/Zotero/storage/7WWRMRU5/A._D._Barbour,_Louis_H._Y._Chen_An_Introduction_to_Steins_Method.pdf},
  note = {OCLC: ocm61225778}
}

@book{hofstadRandomGraphsComplex2017,
  location = {{Cambridge}},
  title = {Random Graphs and Complex Networks},
  isbn = {978-1-107-17287-6},
  number = {43},
  series = {Cambridge Series in Statistical and Probabilistic Mathematics},
  publisher = {{Cambridge University Press}},
  date = {2017},
  keywords = {Random graphs},
  author = {family=Hofstad, given=Remco, prefix=van der, useprefix=false},
  file = {/home/dimitri/Nextcloud/Zotero/storage/D8JSABD6/NotesRGCN.pdf}
}

@article{arratiaPoissonApproximationChenStein1990,
  langid = {english},
  title = {Poisson {{Approximation}} and the {{Chen}}-{{Stein Method}}},
  volume = {5},
  issn = {0883-4237},
  url = {http://projecteuclid.org/euclid.ss/1177012015},
  doi = {10.1214/ss/1177012015},
  number = {4},
  journaltitle = {Statistical Science},
  urldate = {2017-11-03},
  date = {1990-11},
  pages = {403-424},
  author = {Arratia, Richard and Goldstein, Larry and Gordon, Louis},
  file = {/home/dimitri/Nextcloud/Zotero/storage/TP9SZJBL/pacs.pdf}
}

@book{youngEssentialsStatisticalInference2005,
  langid = {english},
  location = {{Cambridge, UK; New York}},
  title = {Essentials of Statistical Inference: {{G}}.{{A}}. {{Young}}, {{R}}.{{L}}. {{Smith}}.},
  isbn = {978-0-511-12616-1 978-0-511-12402-0 978-0-521-83971-6 978-0-511-75539-2},
  url = {http://site.ebrary.com/id/10428993},
  shorttitle = {Essentials of Statistical Inference},
  abstract = {Aimed at advanced undergraduate and graduate students in mathematics and related disciplines, this book presents the concepts and results underlying the Bayesian, frequentist and Fisherian approaches, with particular emphasis on the contrasts between them. Computational ideas are explained, as well as basic mathematical theory. Written in a lucid and informal style, this concise text provides both basic material on the main approaches to inference, as well as more advanced material on developments in statistical theory, including: material on Bayesian computation, such as MCMC, higher-order likelihood theory, predictive inference, bootstrap methods and conditional inference. It contains numerous extended examples of the application of formal inference techniques to real data, as well as historical commentary on the development of the subject. Throughout, the text concentrates on concepts, rather than mathematical detail, while maintaining appropriate levels of formality. Each chapter ends with a set of accessible problems.},
  publisher = {{Cambridge University Press}},
  urldate = {2017-11-03},
  date = {2005},
  author = {Young, G. A and Smith, Richard L},
  file = {/home/dimitri/Nextcloud/Zotero/storage/5IL5IVNR/G._A._Young,_R._L._Smith_Essentials_of_Statistical_Inference.pdf},
  note = {OCLC: 61410200}
}

@book{coxPrinciplesStatisticalInference2006,
  langid = {english},
  location = {{Cambridge; New York}},
  title = {Principles of Statistical Inference},
  isbn = {978-0-511-34950-8},
  publisher = {{Cambridge University Press}},
  date = {2006},
  author = {Cox, David Roxbee},
  file = {/home/dimitri/Nextcloud/Zotero/storage/XKB7NUPG/D._R._Cox_Principles_of_statistical_inference.pdf},
  note = {OCLC: 288913845}
}

@book{lieroIntroductionTheoryStatistical2016,
  langid = {english},
  title = {Introduction to the {{Theory}} of {{Statistical Inference}}},
  isbn = {978-1-4665-0320-5},
  url = {https://nls.ldls.org.uk/welcome.html?ark:/81055/vdc_100045022665.0x000001},
  urldate = {2017-11-03},
  date = {2016},
  author = {Liero, Hannelore and Zwanzig, Silvelyn},
  file = {/home/dimitri/Nextcloud/Zotero/storage/LGP8CVUG/Liero,_Hannelore\;_Zwanzig,_Silvelyn_Introduction_to_the_Theory_of_Statistical_Inference.pdf},
  note = {OCLC: 991528384}
}

@book{casellaStatisticalInference2002,
  location = {{Australia ; Pacific Grove, CA}},
  title = {Statistical Inference},
  edition = {2nd ed},
  isbn = {978-0-534-24312-8},
  pagetotal = {660},
  publisher = {{Thomson Learning}},
  date = {2002},
  keywords = {Probabilities,Mathematical statistics},
  author = {Casella, George and Berger, Roger L.},
  file = {/home/dimitri/Nextcloud/Zotero/storage/RBQEIYUM/George_Casella,_Roger_L._Berger_Statistical_Inference.pdf}
}

@inproceedings{matejkaSameStatsDifferent2017,
  langid = {english},
  title = {Same {{Stats}}, {{Different Graphs}}: {{Generating Datasets}} with {{Varied Appearance}} and {{Identical Statistics}} through {{Simulated Annealing}}},
  isbn = {978-1-4503-4655-9},
  url = {http://dl.acm.org/citation.cfm?doid=3025453.3025912},
  doi = {10.1145/3025453.3025912},
  shorttitle = {Same {{Stats}}, {{Different Graphs}}},
  publisher = {{ACM Press}},
  urldate = {2017-11-03},
  date = {2017},
  pages = {1290-1294},
  author = {Matejka, Justin and Fitzmaurice, George},
  file = {/home/dimitri/Nextcloud/Zotero/storage/FBB9CIM5/SameStats-DifferentGraphs.pdf}
}

@article{creswellGenerativeAdversarialNetworks2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1710.07035},
  primaryClass = {cs},
  title = {Generative {{Adversarial Networks}}: {{An Overview}}},
  url = {http://arxiv.org/abs/1710.07035},
  shorttitle = {Generative {{Adversarial Networks}}},
  abstract = {Generative adversarial networks (GANs) provide a way to learn deep representations without extensively annotated training data. They achieve this through deriving backpropagation signals through a competitive process involving a pair of networks. The representations that can be learned by GANs may be used in a variety of applications, including image synthesis, semantic image editing, style transfer, image super-resolution and classification. The aim of this review paper is to provide an overview of GANs for the signal processing community, drawing on familiar analogies and concepts where possible. In addition to identifying different methods for training and constructing GANs, we also point to remaining challenges in their theory and application.},
  urldate = {2017-11-03},
  date = {2017-10-19},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  author = {Creswell, Antonia and White, Tom and Dumoulin, Vincent and Arulkumaran, Kai and Sengupta, Biswa and Bharath, Anil A.},
  file = {/home/dimitri/Nextcloud/Zotero/storage/MP87U6RA/Creswell et al. - 2017 - Generative Adversarial Networks An Overview.pdf;/home/dimitri/Nextcloud/Zotero/storage/4TWDAP5A/1710.html}
}

@article{kipfSemiSupervisedClassificationGraph2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1609.02907},
  primaryClass = {cs, stat},
  title = {Semi-{{Supervised Classification}} with {{Graph Convolutional Networks}}},
  url = {http://arxiv.org/abs/1609.02907},
  abstract = {We present a scalable approach for semi-supervised learning on graph-structured data that is based on an efficient variant of convolutional neural networks which operate directly on graphs. We motivate the choice of our convolutional architecture via a localized first-order approximation of spectral graph convolutions. Our model scales linearly in the number of graph edges and learns hidden layer representations that encode both local graph structure and features of nodes. In a number of experiments on citation networks and on a knowledge graph dataset we demonstrate that our approach outperforms related methods by a significant margin.},
  urldate = {2017-11-03},
  date = {2016-09-09},
  keywords = {Computer Science - Learning,Statistics - Machine Learning},
  author = {Kipf, Thomas N. and Welling, Max},
  file = {/home/dimitri/Nextcloud/Zotero/storage/5CA43RDN/Kipf and Welling - 2016 - Semi-Supervised Classification with Graph Convolut.pdf;/home/dimitri/Nextcloud/Zotero/storage/9WK5J7Q8/1609.html}
}

@online{khanAmazonAntitrustParadox2017,
  title = {Amazon's {{Antitrust Paradox}}},
  url = {https://www.yalelawjournal.org/note/amazons-antitrust-paradox},
  abstract = {Amazon is the titan of twenty-first century commerce. In addition to being a retailer, it is now a marketing platform, a delivery and logistics network, a payment service, a credit lender, an auction house, a major book publisher, a producer of television and films, a fashion designer, a hardware manufacturer, and a leading host of cloud server space.},
  urldate = {2017-11-03},
  date = {2017},
  author = {Khan, Lina M.},
  file = {/home/dimitri/Nextcloud/Zotero/storage/PBSBPS93/e.710.Khan.805_zuvfyyeh.pdf;/home/dimitri/Nextcloud/Zotero/storage/L3ZBK7MN/amazons-antitrust-paradox.html}
}

@book{garthwaiteStatisticalInference2002,
  location = {{Oxford ; New York}},
  title = {Statistical Inference},
  edition = {2nd ed},
  isbn = {978-0-19-857226-8},
  pagetotal = {328},
  series = {Oxford Science Publications},
  publisher = {{Oxford University Press}},
  date = {2002},
  keywords = {Mathematical statistics},
  author = {Garthwaite, Paul H. and Jolliffe, I. T. and Jones, Byron}
}

@article{rafteryBayesFactorsBIC1999,
  langid = {english},
  title = {Bayes {{Factors}} and {{BIC}}: {{Comment}} on “{{A Critique}} of the {{Bayesian Information Criterion}} for {{Model Selection}}”},
  volume = {27},
  issn = {0049-1241, 1552-8294},
  url = {http://journals.sagepub.com/doi/10.1177/0049124199027003005},
  doi = {10.1177/0049124199027003005},
  shorttitle = {Bayes {{Factors}} and {{BIC}}},
  number = {3},
  journaltitle = {Sociological Methods \& Research},
  urldate = {2017-11-21},
  date = {1999-02},
  pages = {411-427},
  author = {Raftery, Adrian E.},
  file = {/home/dimitri/Nextcloud/Zotero/storage/3UDW8LR7/raftery1999.pdf}
}

@article{kassBayesFactors1995,
  title = {Bayes {{Factors}}},
  volume = {90},
  issn = {0162-1459},
  url = {http://www.tandfonline.com/doi/abs/10.1080/01621459.1995.10476572},
  doi = {10.1080/01621459.1995.10476572},
  abstract = {In a 1935 paper and in his book Theory of Probability, Jeffreys developed a methodology for quantifying the evidence in favor of a scientific theory. The centerpiece was a number, now called the Bayes factor, which is the posterior odds of the null hypothesis when the prior probability on the null is one-half. Although there has been much discussion of Bayesian hypothesis testing in the context of criticism of P-values, less attention has been given to the Bayes factor as a practical tool of applied statistics. In this article we review and discuss the uses of Bayes factors in the context of five scientific applications in genetics, sports, ecology, sociology, and psychology. We emphasize the following points: • From Jeffreys' Bayesian viewpoint, the purpose of hypothesis testing is to evaluate the evidence in favor of a scientific theory. • Bayes factors offer a way of evaluating evidence in favor of a null hypothesis. • Bayes factors provide a way of incorporating external information into the evaluation of evidence about a hypothesis. • Bayes factors are very general and do not require alternative models to be nested. • Several techniques are available for computing Bayes factors, including asymptotic approximations that are easy to compute using the output from standard packages that maximize likelihoods. • In “nonstandard” statistical models that do not satisfy common regularity conditions, it can be technically simpler to calculate Bayes factors than to derive non-Bayesian significance tests. • The Schwarz criterion (or BIC) gives a rough approximation to the logarithm of the Bayes factor, which is easy to use and does not require evaluation of prior distributions. • When one is interested in estimation or prediction, Bayes factors may be converted to weights to be attached to various models so that a composite estimate or prediction may be obtained that takes account of structural or model uncertainty. • Algorithms have been proposed that allow model uncertainty to be taken into account when the class of models initially considered is very large. • Bayes factors are useful for guiding an evolutionary model-building process. • It is important, and feasible, to assess the sensitivity of conclusions to the prior distributions used.},
  number = {430},
  journaltitle = {Journal of the American Statistical Association},
  urldate = {2017-11-21},
  date = {1995-06-01},
  pages = {773-795},
  keywords = {Bayesian hypothesis tests,BIC,Importance sampling,Laplace method,Markov chain Monte Carlo,Model selection,Monte Carlo integration,Posterior model probabilities,Posterior odds,Quadrature,Schwarz criterion,Sensitivity analysis,Strength of evidence},
  author = {Kass, Robert E. and Raftery, Adrian E.},
  file = {/home/dimitri/Nextcloud/Zotero/storage/758TAR3G/kass1995.pdf}
}

@article{goodfellowNIPS2016Tutorial2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1701.00160},
  primaryClass = {cs},
  title = {{{NIPS}} 2016 {{Tutorial}}: {{Generative Adversarial Networks}}},
  url = {http://arxiv.org/abs/1701.00160},
  shorttitle = {{{NIPS}} 2016 {{Tutorial}}},
  abstract = {This report summarizes the tutorial presented by the author at NIPS 2016 on generative adversarial networks (GANs). The tutorial describes: (1) Why generative modeling is a topic worth studying, (2) how generative models work, and how GANs compare to other generative models, (3) the details of how GANs work, (4) research frontiers in GANs, and (5) state-of-the-art image models that combine GANs with other methods. Finally, the tutorial contains three exercises for readers to complete, and the solutions to these exercises.},
  urldate = {2017-11-23},
  date = {2016-12-31},
  keywords = {Computer Science - Learning},
  author = {Goodfellow, Ian},
  file = {/home/dimitri/Nextcloud/Zotero/storage/S2ZIVLS3/Goodfellow - 2016 - NIPS 2016 Tutorial Generative Adversarial Network.pdf;/home/dimitri/Nextcloud/Zotero/storage/QB8BN9AE/1701.html}
}

@article{liDeepReinforcementLearning2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1701.07274},
  primaryClass = {cs},
  title = {Deep {{Reinforcement Learning}}: {{An Overview}}},
  url = {http://arxiv.org/abs/1701.07274},
  shorttitle = {Deep {{Reinforcement Learning}}},
  abstract = {We give an overview of recent exciting achievements of deep reinforcement learning (RL). We discuss six core elements, six important mechanisms, and twelve applications. We start with background of machine learning, deep learning and reinforcement learning. Next we discuss core RL elements, including value function, in particular, Deep Q-Network (DQN), policy, reward, model, planning, and exploration. After that, we discuss important mechanisms for RL, including attention and memory, unsupervised learning, transfer learning, multi-agent RL, hierarchical RL, and learning to learn. Then we discuss various applications of RL, including games, in particular, AlphaGo, robotics, natural language processing, including dialogue systems, machine translation, and text generation, computer vision, neural architecture design, business management, finance, healthcare, Industry 4.0, smart grid, intelligent transportation systems, and computer systems. We mention topics not reviewed yet, and list a collection of RL resources. After presenting a brief summary, we close with discussions.},
  urldate = {2017-11-23},
  date = {2017-01-25},
  keywords = {Computer Science - Learning},
  author = {Li, Yuxi},
  file = {/home/dimitri/Nextcloud/Zotero/storage/9HV9VAGU/Li - 2017 - Deep Reinforcement Learning An Overview.pdf;/home/dimitri/Nextcloud/Zotero/storage/VSLYQLYR/1701.html}
}

@article{arulkumaranBriefSurveyDeep2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1708.05866},
  title = {A {{Brief Survey}} of {{Deep Reinforcement Learning}}},
  volume = {34},
  issn = {1053-5888},
  url = {http://arxiv.org/abs/1708.05866},
  doi = {10.1109/MSP.2017.2743240},
  abstract = {Deep reinforcement learning is poised to revolutionise the field of AI and represents a step towards building autonomous systems with a higher level understanding of the visual world. Currently, deep learning is enabling reinforcement learning to scale to problems that were previously intractable, such as learning to play video games directly from pixels. Deep reinforcement learning algorithms are also applied to robotics, allowing control policies for robots to be learned directly from camera inputs in the real world. In this survey, we begin with an introduction to the general field of reinforcement learning, then progress to the main streams of value-based and policy-based methods. Our survey will cover central algorithms in deep reinforcement learning, including the deep \$Q\$-network, trust region policy optimisation, and asynchronous advantage actor-critic. In parallel, we highlight the unique advantages of deep neural networks, focusing on visual understanding via reinforcement learning. To conclude, we describe several current areas of research within the field.},
  number = {6},
  journaltitle = {IEEE Signal Processing Magazine},
  urldate = {2017-11-23},
  date = {2017-11},
  pages = {26-38},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Learning,Statistics - Machine Learning,Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  author = {Arulkumaran, Kai and Deisenroth, Marc Peter and Brundage, Miles and Bharath, Anil Anthony},
  file = {/home/dimitri/Nextcloud/Zotero/storage/3LAVBW42/Arulkumaran et al. - 2017 - A Brief Survey of Deep Reinforcement Learning.pdf;/home/dimitri/Nextcloud/Zotero/storage/79YX4467/Arulkumaran et al. - 2017 - A Brief Survey of Deep Reinforcement Learning.pdf;/home/dimitri/Nextcloud/Zotero/storage/KTWCE5QP/1708.html;/home/dimitri/Nextcloud/Zotero/storage/YDQQ6UAA/1708.html}
}

@article{kleinerScalableBootstrapMassive2014,
  langid = {english},
  title = {A Scalable Bootstrap for Massive Data},
  volume = {76},
  issn = {1467-9868},
  url = {http://onlinelibrary.wiley.com/doi/10.1111/rssb.12050/abstract},
  doi = {10.1111/rssb.12050},
  abstract = {The bootstrap provides a simple and powerful means of assessing the quality of estimators. However, in settings involving large data sets—which are increasingly prevalent—the calculation of bootstrap-based quantities can be prohibitively demanding computationally. Although variants such as subsampling and the m out of n bootstrap can be used in principle to reduce the cost of bootstrap computations, these methods are generally not robust to specification of tuning parameters (such as the number of subsampled data points), and they often require knowledge of the estimator's convergence rate, in contrast with the bootstrap. As an alternative, we introduce the ‘bag of little bootstraps’ (BLB), which is a new procedure which incorporates features of both the bootstrap and subsampling to yield a robust, computationally efficient means of assessing the quality of estimators. The BLB is well suited to modern parallel and distributed computing architectures and furthermore retains the generic applicability and statistical efficiency of the bootstrap. We demonstrate the BLB's favourable statistical performance via a theoretical analysis elucidating the procedure's properties, as well as a simulation study comparing the BLB with the bootstrap, the m out of n bootstrap and subsampling. In addition, we present results from a large-scale distributed implementation of the BLB demonstrating its computational superiority on massive data, a method for adaptively selecting the BLB's tuning parameters, an empirical study applying the BLB to several real data sets and an extension of the BLB to time series data.},
  number = {4},
  journaltitle = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  shortjournal = {J. R. Stat. Soc. B},
  urldate = {2017-11-24},
  date = {2014-09-01},
  pages = {795-816},
  keywords = {Bootstrap,Computational efficiency,Estimator quality assessment,Massive data,Resampling},
  author = {Kleiner, Ariel and Talwalkar, Ameet and Sarkar, Purnamrita and Jordan, Michael I.},
  file = {/home/dimitri/Nextcloud/Zotero/storage/WBLDAB8H/Kleiner_et_al-2014-Journal_of_the_Royal_Statistical_Society-_Series_B_(Statistical_Methodology).pdf;/home/dimitri/Nextcloud/Zotero/storage/Y4MEUPYU/abstract.html}
}

@article{cinnerBrightSpotsWorld2016,
  langid = {english},
  title = {Bright Spots among the World’s Coral Reefs},
  volume = {535},
  issn = {1476-4687},
  url = {https://www.nature.com/articles/nature18607},
  doi = {10.1038/nature18607},
  abstract = {{$<$}p{$>$}The health of the world's coral reefs, which provide goods and services for millions of people, is declining. Effective management of these ecosystems requires an understanding of the underlying drivers of reef decline. In a study that spans the gap between ecology and the social sciences, Joshua Cinner and colleagues develop a Bayesian hierarchical model, using data from more than 2,500 reefs worldwide, to predict reef fish biomass based on various socioeconomic drivers and environmental conditions. They identify 15 bright spots  sites where reef biomass is significantly higher than expected. The bright spots are found not only among iconic remote and pristine areas, but also where there are strong sociocultural institutions and high levels of local engagement. On the basis of this analysis, the authors argue for a refocus of coral reef conservation efforts away from locating and protecting remote, pristine sites, towards unlocking potential solutions from sites that have successfully confronted the coral reef crisis.{$<$}/p{$>$}},
  number = {7612},
  journaltitle = {Nature},
  urldate = {2017-11-30},
  date = {2016-07},
  pages = {416},
  author = {Cinner, Joshua E. and Huchery, Cindy and MacNeil, M. Aaron and Graham, Nicholas A. J. and McClanahan, Tim R. and Maina, Joseph and Maire, Eva and Kittinger, John N. and Hicks, Christina C. and Mora, Camilo and Allison, Edward H. and D’Agata, Stephanie and Hoey, Andrew and Feary, David A. and Crowder, Larry and Williams, Ivor D. and Kulbicki, Michel and Vigliola, Laurent and Wantiez, Laurent and Edgar, Graham and Stuart-Smith, Rick D. and Sandin, Stuart A. and Green, Alison L. and Hardt, Marah J. and Beger, Maria and Friedlander, Alan and Campbell, Stuart J. and Holmes, Katherine E. and Wilson, Shaun K. and Brokovich, Eran and Brooks, Andrew J. and Cruz-Motta, Juan J. and Booth, David J. and Chabanet, Pascale and Gough, Charlie and Tupper, Mark and Ferse, Sebastian C. A. and Sumaila, U. Rashid and Mouillot, David},
  file = {/home/dimitri/Nextcloud/Zotero/storage/9T5N7LBU/cinner2016.pdf;/home/dimitri/Nextcloud/Zotero/storage/4PQWK299/nature18607.html}
}

@article{hoareEmperorOldClothes1981,
  title = {The {{Emperor}}'s {{Old Clothes}}},
  volume = {24},
  issn = {0001-0782},
  url = {http://doi.acm.org/10.1145/358549.358561},
  doi = {10.1145/358549.358561},
  number = {2},
  journaltitle = {Commun. ACM},
  urldate = {2017-12-31},
  date = {1981-02},
  pages = {75--83},
  keywords = {history of programming languages,lessons for the future,programming languages},
  author = {Hoare, Charles Antony Richard},
  file = {/home/dimitri/Nextcloud/Zotero/storage/2FP4K8RZ/Hoare - 1981 - The Emperor's Old Clothes.pdf}
}

@article{kimStatespaceMultitaperTimefrequency2018,
  langid = {english},
  title = {State-Space Multitaper Time-Frequency Analysis},
  volume = {115},
  issn = {0027-8424, 1091-6490},
  url = {http://www.pnas.org/content/115/1/E5},
  doi = {10.1073/pnas.1702877115},
  abstract = {Time series are an important data class that includes recordings ranging from radio emissions, seismic activity, global positioning data, and stock prices to EEG measurements, vital signs, and voice recordings. Rapid growth in sensor and recording technologies is increasing the production of time series data and the importance of rapid, accurate analyses. Time series data are commonly analyzed using time-varying spectral methods to characterize their nonstationary and often oscillatory structure. Current methods provide local estimates of data features. However, they do not offer a statistical inference framework that applies to the entire time series. The important advances that we report are state-space multitaper (SS-MT) methods, which provide a statistical inference framework for time-varying spectral analysis of nonstationary time series. We model nonstationary time series as a sequence of second-order stationary Gaussian processes defined on nonoverlapping intervals. We use a frequency-domain random-walk model to relate the spectral representations of the Gaussian processes across intervals. The SS-MT algorithm efficiently computes spectral updates using parallel 1D complex Kalman filters. An expectation–maximization algorithm computes static and dynamic model parameter estimates. We test the framework in time-varying spectral analyses of simulated time series and EEG recordings from patients receiving general anesthesia. Relative to standard multitaper (MT), SS-MT gave enhanced spectral resolution and noise reduction ({$<$}mml:math{$><$}mml:mo{$>><$}/mml:mo{$><$}/mml:math{$>>$}10 dB) and allowed statistical comparisons of spectral properties among arbitrary time series segments. SS-MT also extracts time-domain estimates of signal components. The SS-MT paradigm is a broadly applicable, empirical Bayes’ framework for statistical inference that can help ensure accurate, reproducible findings from nonstationary time series analyses.},
  number = {1},
  journaltitle = {Proceedings of the National Academy of Sciences},
  shortjournal = {PNAS},
  urldate = {2018-01-07},
  date = {2018-02-01},
  pages = {E5-E14},
  keywords = {big data,complex Kalman filter,nonparametric spectral analysis,spectral representation theorem,statistical inference},
  author = {Kim, Seong-Eun and Behr, Michael K. and Ba, Demba and Brown, Emery N.},
  file = {/home/dimitri/Nextcloud/Zotero/storage/X5QZT5KF/Kim et al. - 2018 - State-space multitaper time-frequency analysis.pdf;/home/dimitri/Nextcloud/Zotero/storage/FFHG3RN5/E5.html},
  eprinttype = {pmid},
  eprint = {29255032}
}

@article{davidProblemCalissons1989,
  eprinttype = {jstor},
  eprint = {2325150},
  title = {The {{Problem}} of the {{Calissons}}},
  volume = {96},
  issn = {0002-9890},
  doi = {10.2307/2325150},
  number = {5},
  journaltitle = {The American Mathematical Monthly},
  date = {1989},
  pages = {429-431},
  author = {David, Guy and Tomei, Carlos},
  file = {/home/dimitri/Nextcloud/Zotero/storage/6PQPYJFP/david1989.pdf}
}

@article{wilsonMixingTimesLozenge2004,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {math/0102193},
  title = {Mixing Times of Lozenge Tiling and Card Shuffling {{Markov}} Chains},
  volume = {14},
  issn = {1050-5164},
  url = {http://arxiv.org/abs/math/0102193},
  doi = {10.1214/aoap/1075828054},
  abstract = {We show how to combine Fourier analysis with coupling arguments to bound the mixing times of a variety of Markov chains. The mixing time is the number of steps a Markov chain takes to approach its equilibrium distribution. One application is to a class of Markov chains introduced by Luby, Randall, and Sinclair to generate random tilings of regions by lozenges. For an L X L region we bound the mixing time by O(L\^4 log L), which improves on the previous bound of O(L\^7), and we show the new bound to be essentially tight. In another application we resolve a few questions raised by Diaconis and Saloff-Coste, by lower bounding the mixing time of various card-shuffling Markov chains. Our lower bounds are within a constant factor of their upper bounds. When we use our methods to modify a path-coupling analysis of Bubley and Dyer, we obtain an O(n\^3 log n) upper bound on the mixing time of the Karzanov-Khachiyan Markov chain for linear extensions.},
  number = {1},
  journaltitle = {The Annals of Applied Probability},
  urldate = {2018-01-14},
  date = {2004-02},
  pages = {274-325},
  keywords = {60J10; 60C05,Mathematics - Probability},
  author = {Wilson, David Bruce},
  file = {/home/dimitri/Nextcloud/Zotero/storage/5PSJDJAG/Wilson - 2004 - Mixing times of lozenge tiling and card shuffling .pdf;/home/dimitri/Nextcloud/Zotero/storage/V2I9FLAM/0102193.html}
}

@article{stolzPersistentHomologyTimedependent2017,
  title = {Persistent Homology of Time-Dependent Functional Networks Constructed from Coupled Time Series},
  volume = {27},
  issn = {1054-1500},
  url = {http://aip.scitation.org/doi/full/10.1063/1.4978997},
  doi = {10.1063/1.4978997},
  abstract = {We use topological data analysis to study “functional networks” that we           construct from time-series data from both experimental and synthetic sources. We use           persistent homology with a weight rank clique filtration to gain insights into these           functional networks, and we use persistence landscapes to interpret our results.           Our first example uses time-series output from networks of coupled Kuramoto oscillators. Our second example           consists of biological data in the form of functional magnetic resonance imaging data that           were acquired from human subjects during a simple motor-learning task in which subjects           were monitored for three days during a five-day period. With these examples, we           demonstrate that (1) using persistent homology to study functional networks provides           fascinating insights into their properties and (2) the position of the features in a           filtration can sometimes play a more vital role than persistence in the interpretation of           topological features, even though conventionally the latter is used to distinguish between           signal and noise. We find that persistent homology can detect differences in           synchronization patterns in our data sets over time, giving insight both on changes in           community structure in the networks and on increased synchronization between brain regions that form loops           in a functional network during motor learning. For the motor-learning data, persistence           landscapes also reveal that on average the majority of changes in the network loops take place           on the second of the three days of the learning process.},
  number = {4},
  journaltitle = {Chaos: An Interdisciplinary Journal of Nonlinear Science},
  shortjournal = {Chaos},
  urldate = {2018-01-18},
  date = {2017-04-01},
  pages = {047410},
  author = {Stolz, Bernadette J. and Harrington, Heather A. and Porter, Mason A.},
  file = {/home/dimitri/Nextcloud/Zotero/storage/2W4IQ5TQ/sichaostimeseries-april2017-corrected-v4-4.pdf;/home/dimitri/Nextcloud/Zotero/storage/A2BD6EHP/Stolz et al. - 2017 - Persistent homology of time-dependent functional n.pdf}
}

@article{taylorTopologicalDataAnalysis2015,
  title = {Topological Data Analysis of Contagion Maps for Examining Spreading Processes on Networks},
  volume = {6},
  issn = {2041-1723},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4566922/},
  doi = {10.1038/ncomms8723},
  abstract = {Social and biological contagions are influenced by the spatial embeddedness of networks. Historically, many epidemics spread as a wave across part of the Earth’s surface; however, in modern contagions long-range edges—for example, due to airline transportation or communication media—allow clusters of a contagion to appear in distant locations. Here we study the spread of contagions on networks through a methodology grounded in topological data analysis and nonlinear dimension reduction. We construct “contagion maps” that use multiple contagions on a network to map the nodes as a point cloud. By analyzing the topology, geometry, and dimensionality of manifold structure in such point clouds, we reveal insights to aid in the modeling, forecast, and control of spreading processes. Our approach highlights contagion maps also as a viable tool for inferring low-dimensional structure in networks.},
  journaltitle = {Nature communications},
  shortjournal = {Nat Commun},
  urldate = {2018-01-18},
  date = {2015-07-21},
  pages = {7723},
  author = {Taylor, Dane and Klimm, Florian and Harrington, Heather A. and Kramár, Miroslav and Mischaikow, Konstantin and Porter, Mason A. and Mucha, Peter J.},
  file = {/home/dimitri/Nextcloud/Zotero/storage/BRA55ZPK/Taylor et al. - 2015 - Topological data analysis of contagion maps for ex.pdf},
  eprinttype = {pmid},
  eprint = {26194875},
  pmcid = {PMC4566922}
}

@article{stolzTopologicalShapeBrexit2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1610.00752},
  primaryClass = {physics},
  title = {The {{Topological}} "{{Shape}}" of {{Brexit}}},
  url = {http://arxiv.org/abs/1610.00752},
  abstract = {Persistent homology is a method from computational algebraic topology that can be used to study the "shape" of data. We illustrate two filtrations --- the weight rank clique filtration and the Vietoris--Rips (VR) filtration --- that are commonly used in persistent homology, and we apply these filtrations to a pair of data sets that are both related to the 2016 European Union "Brexit" referendum in the United Kingdom. These examples consider a topical situation and give useful illustrations of the strengths and weaknesses of these methods.},
  urldate = {2018-01-18},
  date = {2016-09-15},
  keywords = {Computer Science - Computational Geometry,Mathematics - Algebraic Topology,Physics - Physics and Society},
  author = {Stolz, Bernadette J. and Harrington, Heather A. and Porter, Mason A.},
  file = {/home/dimitri/Nextcloud/Zotero/storage/9MIPK9ZY/Stolz et al. - 2016 - The Topological Shape of Brexit.pdf;/home/dimitri/Nextcloud/Zotero/storage/EGR5HLE4/1610.html}
}

@article{peruccaTravellersSatisfactionRailway2014,
  title = {Travellers’ {{Satisfaction}} with {{Railway Transport}}: {{A Bayesian Network Approach}}},
  volume = {11},
  issn = {null},
  url = {https://doi.org/10.1080/16843703.2014.11673326},
  doi = {10.1080/16843703.2014.11673326},
  shorttitle = {Travellers’ {{Satisfaction}} with {{Railway Transport}}},
  abstract = {Survey data are often employed as a tool to support planners in defining and reviewing their intervention programs and policies. In the current literature, a broad range of studies have examined the determinants and components of consumers’ satisfaction. Most of these studies have exploited statistical models, such as probit or logit, to determine the categorical dependent variables. Less attention has been paid to the investigation of the structural properties of the data by testing alternative methods and procedures. The present work performs such an analysis using data from a survey on consumer satisfaction from railway transport in 14 EU countries. The data were collected in a large survey (more than 17,000 observations) conducted on behalf of the European Commission. By applying both ordered logistic regression and Bayesian Network (BN) analysis, the research question addressed by this paper is twofold. First of all, the performance of the two methodologies, defined in terms of their predictive capability, is assessed. Secondly, the main policy implications conveyed by the two models are compared. According to the results, BN analysis provides better predictions of the outcomes of customers’ satisfaction with railway transport. Moreover, the choice of the statistical methodology is a relevant issue for policy-making, since the policy messages conveyed by the two models differ significantly.},
  number = {1},
  journaltitle = {Quality Technology \& Quantitative Management},
  urldate = {2018-01-21},
  date = {2014-01-01},
  pages = {71-84},
  keywords = {Bayesian Network,consumers’ satisfaction,ordered logit,railway transport.},
  author = {Perucca, Giovanni and Salini, Silvia},
  file = {/home/dimitri/Nextcloud/Zotero/storage/X2HBAKHP/perucca2014.pdf;/home/dimitri/Nextcloud/Zotero/storage/TYR6NFTJ/16843703.2014.html}
}

@article{scutariBayesianNetworkConstraintBased2014,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1406.7648},
  primaryClass = {cs, stat},
  title = {Bayesian {{Network Constraint}}-{{Based Structure Learning Algorithms}}: {{Parallel}} and {{Optimised Implementations}} in the Bnlearn {{R Package}}},
  url = {http://arxiv.org/abs/1406.7648},
  shorttitle = {Bayesian {{Network Constraint}}-{{Based Structure Learning Algorithms}}},
  abstract = {It is well known in the literature that the problem of learning the structure of Bayesian networks is very hard to tackle: its computational complexity is super-exponential in the number of nodes in the worst case and polynomial in most real-world scenarios. Efficient implementations of score-based structure learning benefit from past and current research in optimisation theory, which can be adapted to the task by using the network score as the objective function to maximise. This is not true for approaches based on conditional independence tests, called constraint-based learning algorithms. The only optimisation in widespread use, backtracking, leverages the symmetries implied by the definitions of neighbourhood and Markov blanket. In this paper we illustrate how backtracking is implemented in recent versions of the bnlearn R package, and how it degrades the stability of Bayesian network structure learning for little gain in terms of speed. As an alternative, we describe a software architecture and framework that can be used to parallelise constraint-based structure learning algorithms (also implemented in bnlearn) and we demonstrate its performance using four reference networks and two real-world data sets from genetics and systems biology. We show that on modern multi-core or multiprocessor hardware parallel implementations are preferable over backtracking, which was developed when single-processor machines were the norm.},
  urldate = {2018-01-24},
  date = {2014-06-30},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Mathematical Software,Statistics - Computation,Statistics - Methodology},
  author = {Scutari, Marco},
  file = {/home/dimitri/Nextcloud/Zotero/storage/I5LZQHRE/Scutari - 2014 - Bayesian Network Constraint-Based Structure Learni.pdf;/home/dimitri/Nextcloud/Zotero/storage/XI86L36F/1406.html}
}

@article{berettaQuantitativeAssessmentEffect2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1704.08676},
  primaryClass = {cs, stat},
  title = {A Quantitative Assessment of the Effect of Different Algorithmic Schemes to the Task of Learning the Structure of {{Bayesian Networks}}},
  url = {http://arxiv.org/abs/1704.08676},
  abstract = {One of the most challenging tasks when adopting Bayesian Networks (BNs) is the one of learning their structure from data. This task is complicated by the huge search space of possible solutions and turned out to be a well-known NP-hard problem and, hence, approximations are required. However, to the best of our knowledge, a quantitative analysis of the performance and characteristics of the different heuristics to solve this problem has never been done before. For this reason, in this work, we provide a detailed study of the different state-of-the-arts methods for structural learning on simulated data considering both BNs with discrete and continuous variables, and with different rates of noise in the data. In particular, we investigate the characteristics of different widespread scores proposed for the inference and the statistical pitfalls within them.},
  urldate = {2018-01-24},
  date = {2017-04-27},
  keywords = {Computer Science - Learning,Statistics - Machine Learning,Computer Science - Artificial Intelligence},
  author = {Beretta, Stefano and Castelli, Mauro and Goncalves, Ivo and Ramazzotti, Daniele},
  file = {/home/dimitri/Nextcloud/Zotero/storage/HDNWQ97Y/Beretta et al. - 2017 - A quantitative assessment of the effect of differe.pdf;/home/dimitri/Nextcloud/Zotero/storage/XFTJEX4L/1704.html}
}

@article{shorPolynomialTimeAlgorithmsPrime1997,
  title = {Polynomial-{{Time Algorithms}} for {{Prime Factorization}} and {{Discrete Logarithms}} on a {{Quantum Computer}}},
  volume = {26},
  issn = {0097-5397},
  url = {http://epubs.siam.org/doi/10.1137/S0097539795293172},
  doi = {10.1137/S0097539795293172},
  abstract = {A digital computer is generally believed to be an efficient universal computing device; that is, it is believed able to simulate any physical computing device with an increase in computation time by at most a polynomial factor. This may not be true when quantum mechanics is taken into consideration. This paper considers factoring integers and finding discrete logarithms, two problems which are generally thought to be hard on a classical computer and which have been used as the basis of several proposed cryptosystems. Efficient randomized algorithms are given for these two problems on a hypothetical quantum computer. These algorithms take a number of steps polynomial in the input size, e.g., the number of digits of the integer to be factored.},
  number = {5},
  journaltitle = {SIAM Journal on Computing},
  shortjournal = {SIAM J. Comput.},
  urldate = {2018-02-02},
  date = {1997-10-01},
  pages = {1484-1509},
  author = {Shor, P.},
  file = {/home/dimitri/Nextcloud/Zotero/storage/XILBIQUK/Shor - 1997 - Polynomial-Time Algorithms for Prime Factorization.pdf}
}

@article{schaubGraphPartitionsCluster2016,
  title = {Graph Partitions and Cluster Synchronization in Networks of Oscillators},
  volume = {26},
  issn = {1054-1500},
  url = {http://aip.scitation.org/doi/full/10.1063/1.4961065},
  doi = {10.1063/1.4961065},
  abstract = {Synchronization over networks depends strongly on the structure of the coupling between the oscillators. When the coupling presents certain regularities, the dynamics can be coarse-grained into clusters by means of External Equitable Partitions of the network graph and their associated quotient graphs. We exploit this graph-theoretical concept to study the phenomenon of cluster synchronization, in which different groups of nodes converge to distinct behaviors. We derive conditions and properties of networks in which such clustered behavior emerges and show that the ensuing dynamics is the result of the localization of the eigenvectors of the associated graph Laplacians linked to the existence of invariant subspaces. The framework is applied to both linear and non-linear models, first for the standard case of networks with positive edges, before being generalized to the case of signed networks with both positive and negative interactions. We illustrate our results with examples of both signed and unsigned graphs for consensus dynamics and for partial synchronization of oscillator networks under the master stability function as well as Kuramoto oscillators.},
  number = {9},
  journaltitle = {Chaos: An Interdisciplinary Journal of Nonlinear Science},
  shortjournal = {Chaos},
  urldate = {2018-02-13},
  date = {2016-08-19},
  pages = {094821},
  author = {Schaub, Michael T. and O'Clery, Neave and Billeh, Yazan N. and Delvenne, Jean-Charles and Lambiotte, Renaud and Barahona, Mauricio},
  file = {/home/dimitri/Nextcloud/Zotero/storage/QDQY8L8M/Schaub et al. - 2016 - Graph partitions and cluster synchronization in ne.pdf;/home/dimitri/Nextcloud/Zotero/storage/JP2SXD5G/1.html}
}

@article{tabourierPredictingLinksEgonetworks2016,
  langid = {english},
  title = {Predicting Links in Ego-Networks Using Temporal Information},
  volume = {5},
  issn = {2193-1127},
  url = {https://epjdatascience.springeropen.com/articles/10.1140/epjds/s13688-015-0062-0},
  doi = {10.1140/epjds/s13688-015-0062-0},
  abstract = {Link prediction appears as a central problem of network science, as it calls for unfolding the mechanisms that govern the micro-dynamics of the network. In this work, we are interested in ego-networks, that is the mere information of interactions of a node to its neighbors, in the context of social relationships. As the structural information is very poor, we rely on another source of information to predict links among egos’ neighbors: the timing of interactions. We define several features to capture different kinds of temporal information and apply machine learning methods to combine these various features and improve the quality of the prediction. We demonstrate the efficiency of this temporal approach on a cellphone interaction dataset, pointing out features which prove themselves to perform well in this context, in particular the temporal profile of interactions and elapsed time between contacts.},
  number = {1},
  journaltitle = {EPJ Data Science},
  urldate = {2018-02-13},
  date = {2016-12},
  pages = {1},
  author = {Tabourier, Lionel and Libert, Anne-Sophie and Lambiotte, Renaud},
  file = {/home/dimitri/Nextcloud/Zotero/storage/ETM66HPY/Tabourier et al. - 2016 - Predicting links in ego-networks using temporal in.pdf;/home/dimitri/Nextcloud/Zotero/storage/IUNKJ9YF/s13688-015-0062-0.html}
}

@article{noulasMiningOpenDatasets2015,
  langid = {english},
  title = {Mining Open Datasets for Transparency in Taxi Transport in Metropolitan Environments},
  volume = {4},
  issn = {2193-1127},
  url = {https://epjdatascience.springeropen.com/articles/10.1140/epjds/s13688-015-0060-2},
  doi = {10.1140/epjds/s13688-015-0060-2},
  abstract = {Uber has recently been introducing novel practices in urban taxi transport. Journey prices can change dynamically in almost real time and also vary geographically from one area to another in a city, a strategy known as surge pricing. In this paper, we explore the power of the new generation of open datasets towards understanding the impact of the new disruption technologies that emerge in the area of public transport. With our primary goal being a more transparent economic landscape for urban commuters, we provide a direct price comparison between Uber and the Yellow Cab company in New York. We discover that Uber, despite its lower standard pricing rates, effectively charges higher fares on average, especially during short in length, but frequent in occurrence, taxi journeys. Building on this insight, we develop a smartphone application, OpenStreetCab, that offers a personalized consultation to mobile users on which taxi provider is cheaper for their journey. Almost five months after its launch, the app has attracted more than three thousand users in a single city. Their journey queries have provided additional insights on the potential savings similar technologies can have for urban commuters, with a highlight being that on average, a user in New York saves 6 U.S. Dollars per taxi journey if they pick the cheapest taxi provider. We run extensive experiments to show how Uber’s surge pricing is the driving factor of higher journey prices and therefore higher potential savings for our application’s users. Finally, motivated by the observation that Uber’s surge pricing is occurring more frequently that intuitively expected, we formulate a prediction task where the aim becomes to predict a geographic area’s tendency to surge. Using exogenous to Uber data, in particular Yellow Cab and Foursquare data, we show how it is possible to estimate customer demand within an area, and by extension surge pricing, with high accuracy.},
  number = {1},
  journaltitle = {EPJ Data Science},
  urldate = {2018-02-13},
  date = {2015-12},
  pages = {23},
  author = {Noulas, Anastasios and Salnikov, Vsevolod and Lambiotte, Renaud and Mascolo, Cecilia},
  file = {/home/dimitri/Nextcloud/Zotero/storage/N6P7THVK/Noulas et al. - 2015 - Mining open datasets for transparency in taxi tran.pdf;/home/dimitri/Nextcloud/Zotero/storage/H3R7HWMH/s13688-015-0060-2.html}
}

@article{kivelaMultilayerNetworks2014,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1309.7233},
  title = {Multilayer {{Networks}}},
  volume = {2},
  issn = {2051-1310, 2051-1329},
  url = {http://arxiv.org/abs/1309.7233},
  doi = {10.1093/comnet/cnu016},
  abstract = {In most natural and engineered systems, a set of entities interact with each other in complicated patterns that can encompass multiple types of relationships, change in time, and include other types of complications. Such systems include multiple subsystems and layers of connectivity, and it is important to take such "multilayer" features into account to try to improve our understanding of complex systems. Consequently, it is necessary to generalize "traditional" network theory by developing (and validating) a framework and associated tools to study multilayer systems in a comprehensive fashion. The origins of such efforts date back several decades and arose in multiple disciplines, and now the study of multilayer networks has become one of the most important directions in network science. In this paper, we discuss the history of multilayer networks (and related concepts) and review the exploding body of work on such networks. To unify the disparate terminology in the large body of recent work, we discuss a general framework for multilayer networks, construct a dictionary of terminology to relate the numerous existing concepts to each other, and provide a thorough discussion that compares, contrasts, and translates between related notions such as multilayer networks, multiplex networks, interdependent networks, networks of networks, and many others. We also survey and discuss existing data sets that can be represented as multilayer networks. We review attempts to generalize single-layer-network diagnostics to multilayer networks. We also discuss the rapidly expanding research on multilayer-network models and notions like community structure, connected components, tensor decompositions, and various types of dynamical processes on multilayer networks. We conclude with a summary and an outlook.},
  number = {3},
  journaltitle = {Journal of Complex Networks},
  urldate = {2018-02-13},
  date = {2014-09-01},
  pages = {203-271},
  keywords = {Physics - Physics and Society,Computer Science - Social and Information Networks},
  author = {Kivelä, Mikko and Arenas, Alexandre and Barthelemy, Marc and Gleeson, James P. and Moreno, Yamir and Porter, Mason A.},
  file = {/home/dimitri/Nextcloud/Zotero/storage/F98JFB2E/Kivelä et al. - 2014 - Multilayer Networks.pdf;/home/dimitri/Nextcloud/Zotero/storage/7WBJRIBQ/1309.html}
}

@article{porterDynamicalSystemsNetworks2014,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1403.7663},
  primaryClass = {cond-mat, physics:nlin, physics:physics},
  title = {Dynamical {{Systems}} on {{Networks}}: {{A Tutorial}}},
  url = {http://arxiv.org/abs/1403.7663},
  shorttitle = {Dynamical {{Systems}} on {{Networks}}},
  abstract = {We give a tutorial for the study of dynamical systems on networks. We focus especially on "simple" situations that are tractable analytically, because they can be very insightful and provide useful springboards for the study of more complicated scenarios. We briefly motivate why examining dynamical systems on networks is interesting and important, and we then give several fascinating examples and discuss some theoretical results. We also briefly discuss dynamical systems on dynamical (i.e., time-dependent) networks, overview software implementations, and give an outlook on the field.},
  urldate = {2018-02-13},
  date = {2014-03-29},
  keywords = {Physics - Physics and Society,Computer Science - Social and Information Networks,Condensed Matter - Disordered Systems and Neural Networks,Condensed Matter - Statistical Mechanics,Nonlinear Sciences - Adaptation and Self-Organizing Systems},
  author = {Porter, Mason A. and Gleeson, James P.},
  file = {/home/dimitri/Nextcloud/Zotero/storage/XBRAHARB/Porter and Gleeson - 2014 - Dynamical Systems on Networks A Tutorial.pdf;/home/dimitri/Nextcloud/Zotero/storage/LF7GCTFE/1403.html}
}

@article{tiernyTopologyToolKit2017,
  langid = {english},
  title = {The {{Topology ToolKit}}},
  url = {https://hal.archives-ouvertes.fr/hal-01499905/document},
  abstract = {This system paper presents the Topology ToolKit (TTK), a software platform designed for topological data analysis in scientific visualization. While topological data analysis has gained in popularity over the last two decades, it has not yet been widely adopted as a standard data analysis tool for end users or developers. TTK aims at addressing this problem by providing a unified, generic, efficient, and robust implementation of key algorithms for the topological analysis of scalar data, including: critical points, integral lines, persistence diagrams, persistence curves, merge trees, contour trees, Morse-Smale complexes, fiber surfaces, continuous scatterplots, Jacobi sets, Reeb spaces, and more. TTK is easily accessible to end users due to a tight integration with ParaView. It is also easily accessible to developers through a variety of bindings (Python, VTK/C++) for fast prototyping or through direct, dependence-free, C++, to ease integration into pre-existing complex systems. While developing TTK, we faced several algorithmic and software engineering challenges, which we document in this paper. In particular, we present an algorithm for the construction of a discrete gradient that complies to the critical points extracted in the piecewise-linear setting. This algorithm guarantees a combinatorial consistency across the topological abstractions supported by TTK, and importantly, a unified implementation of topological data simplification for multi-scale exploration and analysis. We also present a cached triangulation data structure, that supports time efficient and generic traversals, which self-adjusts its memory usage on demand for input simplicial meshes and which implicitly emulates a triangulation for regular grids with no memory overhead. Finally, we describe an original software architecture, which guarantees memory efficient and direct accesses to TTK features, while still allowing for researchers powerful and easy bindings and extensions. TTK is open source (BSD license) and its code, online documentation and video tutorials are available on TTK's website (https://topology-tool-kit.github.io/).},
  journaltitle = {IEEE Transactions on Visualization and Computer Graphics},
  urldate = {2018-02-15},
  date = {2017-10-01},
  author = {Tierny, Julien and Favelier, Guillaume and Levine, Joshua and Gueunet, Charles and Michaux, Michael},
  file = {/home/dimitri/Nextcloud/Zotero/storage/TGURBQBF/Tierny et al. - 2017 - The Topology ToolKit.pdf;/home/dimitri/Nextcloud/Zotero/storage/JAIQUA5K/hal-01499905v2.html}
}

@inproceedings{mariaGudhiLibrarySimplicial2014,
  langid = {english},
  title = {The {{Gudhi Library}}: {{Simplicial Complexes}} and {{Persistent Homology}}},
  isbn = {978-3-662-44198-5},
  url = {https://link.springer.com/chapter/10.1007/978-3-662-44199-2_28},
  doi = {10.1007/978-3-662-44199-2_28},
  shorttitle = {The {{Gudhi Library}}},
  abstract = {We present the main algorithmic and design choices that have been made to represent complexes and compute persistent homology in the Gudhi library. The Gudhi library (Geometric Understanding in Higher Dimensions) is a generic C++ library for computational topology. Its goal is to provide robust, efficient, flexible and easy to use implementations of state-of-the-art algorithms and data structures for computational topology. We present the different components of the software, their interaction and the user interface. We justify the algorithmic and design decisions made in Gudhi and provide benchmarks for the code. The software, which has been developped by the first author, will be available soon at project.inria.fr/gudhi/software/ .},
  eventtitle = {International {{Congress}} on {{Mathematical Software}}},
  booktitle = {Mathematical {{Software}} – {{ICMS}} 2014},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  publisher = {{Springer, Berlin, Heidelberg}},
  urldate = {2018-02-15},
  date = {2014-08-05},
  pages = {167-174},
  author = {Maria, Clément and Boissonnat, Jean-Daniel and Glisse, Marc and Yvinec, Mariette},
  file = {/home/dimitri/Nextcloud/Zotero/storage/3YRXLXZL/978-3-662-44199-2_28.html}
}

@online{oudotINF556TopologicalData2017,
  title = {{{INF556}} -- {{Topological Data Analysis}}},
  url = {http://www.enseignement.polytechnique.fr/informatique/INF556/},
  urldate = {2018-02-16},
  date = {2017},
  author = {Oudot, Steve Y.},
  file = {/home/dimitri/Nextcloud/Zotero/storage/TNRU945Q/INF556.html}
}

@article{casteigtsTimevaryingGraphsDynamic2012,
  title = {Time-Varying Graphs and Dynamic Networks},
  volume = {27},
  issn = {1744-5760},
  url = {https://doi.org/10.1080/17445760.2012.668546},
  doi = {10.1080/17445760.2012.668546},
  abstract = {The past few years have seen intensive research efforts carried out in some apparently unrelated areas of dynamic systems – delay-tolerant networks, opportunistic-mobility networks and social networks – obtaining closely related insights. Indeed, the concepts discovered in these investigations can be viewed as parts of the same conceptual universe, and the formal models proposed so far to express some specific concepts are the components of a larger formal description of this universe. The main contribution of this paper is to integrate the vast collection of concepts, formalisms and results found in the literature into a unified framework, which we call time-varying graphs (TVGs). Using this framework, it is possible to express directly in the same formalism not only the concepts common to all those different areas, but also those specific to each. Based on this definitional work, employing both existing results and original observations, we present a hierarchical classification of TVGs; each class corresponds to a significant property examined in the distributed computing literature. We then examine how TVGs can be used to study the evolution of network properties, and propose different techniques, depending on whether the indicators for these properties are atemporal (as in the majority of existing studies) or temporal. Finally, we briefly discuss the introduction of randomness in TVGs.},
  number = {5},
  journaltitle = {International Journal of Parallel, Emergent and Distributed Systems},
  urldate = {2018-02-21},
  date = {2012-10-01},
  pages = {387-408},
  keywords = {social networks,delay-tolerant networks,distributed computing,dynamic graphs,opportunistic networks,time-varying graphs},
  author = {Casteigts, Arnaud and Flocchini, Paola and Quattrociocchi, Walter and Santoro, Nicola},
  file = {/home/dimitri/Nextcloud/Zotero/storage/IPW9FMKH/1012.0009.pdf;/home/dimitri/Nextcloud/Zotero/storage/R94NRJG7/17445760.2012.html}
}

@article{kuhnDynamicNetworksModels2011,
  title = {Dynamic {{Networks}}: {{Models}} and {{Algorithms}}},
  volume = {42},
  issn = {0163-5700},
  url = {http://doi.acm.org/10.1145/1959045.1959064},
  doi = {10.1145/1959045.1959064},
  shorttitle = {Dynamic {{Networks}}},
  number = {1},
  journaltitle = {SIGACT News},
  urldate = {2018-02-21},
  date = {2011-03},
  pages = {82--96},
  author = {Kuhn, Fabian and Oshman, Rotem},
  file = {/home/dimitri/Nextcloud/Zotero/storage/WEN85Y2C/kuhn2011.pdf}
}

@article{michailIntroductionTemporalGraphs2016,
  title = {An {{Introduction}} to {{Temporal Graphs}}: {{An Algorithmic Perspective}}},
  volume = {12},
  issn = {1542-7951},
  url = {https://doi.org/10.1080/15427951.2016.1177801},
  doi = {10.1080/15427951.2016.1177801},
  shorttitle = {An {{Introduction}} to {{Temporal Graphs}}},
  abstract = {A temporal graph is, informally speaking, a graph that changes with time. When time is discrete and only the relationships between the participating entities may change and not the entities themselves, a temporal graph may be viewed as a sequence G1, G2…, Gl of static graphs over the same (static) set of nodes V. Though static graphs have been extensively studied, for their temporal generalization we are still far from having a concrete set of structural and algorithmic principles. Recent research shows that many graph properties and problems become radically different and usually substantially more difficult when an extra time dimension is added to them. Moreover, there is already a rich and rapidly growing set of modern systems and applications that can be naturally modeled and studied via temporal graphs. This, further motivates the need for the development of a temporal extension of graph theory. We survey here recent results on temporal graphs and temporal graph problems that have appeared in the Computer Science community.},
  number = {4},
  journaltitle = {Internet Mathematics},
  urldate = {2018-02-21},
  date = {2016-07-03},
  pages = {239-280},
  author = {Michail, Othon},
  file = {/home/dimitri/Nextcloud/Zotero/storage/QQU5QN6M/1503.00278.pdf;/home/dimitri/Nextcloud/Zotero/storage/A9KBYDWN/15427951.2016.html}
}

@article{kempeConnectivityInferenceProblems2002,
  title = {Connectivity and {{Inference Problems}} for {{Temporal Networks}}},
  volume = {64},
  issn = {0022-0000},
  url = {http://www.sciencedirect.com/science/article/pii/S0022000002918295},
  doi = {10.1006/jcss.2002.1829},
  abstract = {Many network problems are based on fundamental relationships involving time. Consider, for example, the problems of modeling the flow of information through a distributed network, studying the spread of a disease through a population, or analyzing the reachability properties of an airline timetable. In such settings, a natural model is that of a graph in which each edge is annotated with a time label specifying the time at which its endpoints “communicated.” We will call such a graph a temporal network. To model the notion that information in such a network “flows” only on paths whose labels respect the ordering of time, we call a path time-respecting if the time labels on its edges are non-decreasing. The central motivation for our work is the following question: how do the basic combinatorial and algorithmic properties of graphs change when we impose this additional temporal condition? The notion of a path is intrinsic to many of the most fundamental algorithmic problems on graphs; spanning trees, connectivity, flows, and cuts are some examples. When we focus on time-respecting paths in place of arbitrary paths, many of these problems acquire a character that is different from the traditional setting, but very rich in its own right. We provide results on two types of problems for temporal networks. First, we consider connectivity problems, in which we seek disjoint time-respecting paths between pairs of nodes. The natural analogue of Menger's Theorem for node-disjoint paths fails in general for time-respecting paths; we give a non-trivial characterization of those graphs for which the theorem does hold in terms of an excluded subdivision theorem, and provide a polynomial-time algorithm for connectivity on this class of graphs. (The problem on general graphs is NP-complete.) We then define and study the class of inference problems, in which we seek to reconstruct a partially specified time labeling of a network in a manner consistent with an observed history of information flow.},
  number = {4},
  journaltitle = {Journal of Computer and System Sciences},
  shortjournal = {Journal of Computer and System Sciences},
  urldate = {2018-02-22},
  date = {2002-06-01},
  pages = {820-842},
  author = {Kempe, David and Kleinberg, Jon and Kumar, Amit},
  file = {/home/dimitri/Nextcloud/Zotero/storage/I9CR9UGA/10.1.1.30.6741.pdf;/home/dimitri/Nextcloud/Zotero/storage/87E98N2I/S0022000002918295.html}
}

@inproceedings{mertziosTemporalNetworkOptimization2013,
  langid = {english},
  title = {Temporal {{Network Optimization Subject}} to {{Connectivity Constraints}}},
  isbn = {978-3-642-39211-5 978-3-642-39212-2},
  url = {https://link.springer.com/chapter/10.1007/978-3-642-39212-2_57},
  doi = {10.1007/978-3-642-39212-2_57},
  abstract = {In this work we consider temporal networks, i.e. networks defined by a labeling λ assigning to each edge of an underlying graph G a set of discrete time-labels. The labels of an edge, which are natural numbers, indicate the discrete time moments at which the edge is available. We focus on path problems of temporal networks. In particular, we consider time-respecting paths, i.e. paths whose edges are assigned by λ a strictly increasing sequence of labels. We begin by giving two efficient algorithms for computing shortest time-respecting paths on a temporal network. We then prove that there is a natural analogue of Menger’s theorem holding for arbitrary temporal networks. Finally, we propose two cost minimization parameters for temporal network design. One is the temporality of G, in which the goal is to minimize the maximum number of labels of an edge, and the other is the temporal cost of G, in which the goal is to minimize the total number of labels used. Optimization of these parameters is performed subject to some connectivity constraint. We prove several lower and upper bounds for the temporality and the temporal cost of some very basic graph families such as rings, directed acyclic graphs, and trees.},
  eventtitle = {International {{Colloquium}} on {{Automata}}, {{Languages}}, and {{Programming}}},
  booktitle = {Automata, {{Languages}}, and {{Programming}}},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  publisher = {{Springer, Berlin, Heidelberg}},
  urldate = {2018-02-22},
  date = {2013-07-08},
  pages = {657-668},
  author = {Mertzios, George B. and Michail, Othon and Chatzigiannakis, Ioannis and Spirakis, Paul G.},
  file = {/home/dimitri/Nextcloud/Zotero/storage/AUZGZX8M/1502.04382.pdf;/home/dimitri/Nextcloud/Zotero/storage/8AUNJDZ2/978-3-642-39212-2_57.html}
}

@article{akridaEphemeralNetworksRandom2016,
  title = {Ephemeral Networks with Random Availability of Links: {{The}} Case of Fast Networks},
  volume = {87},
  issn = {0743-7315},
  url = {http://www.sciencedirect.com/science/article/pii/S0743731515001872},
  doi = {10.1016/j.jpdc.2015.10.002},
  shorttitle = {Ephemeral Networks with Random Availability of Links},
  abstract = {We consider here a model of temporal networks, the links of which are available only at certain moments in time, chosen randomly from a subset of the positive integers. We define the notion of the Temporal Diameter of such networks. We also define fast and slow such temporal networks with respect to the expected value of their temporal diameter. We then provide a partial characterization of fast random temporal networks. We also define the critical availability as a measure of periodic random availability of the links of a network, required to make the network fast. We finally give a lower bound as well as an upper bound on the (critical) availability.},
  journaltitle = {Journal of Parallel and Distributed Computing},
  shortjournal = {Journal of Parallel and Distributed Computing},
  urldate = {2018-02-22},
  date = {2016-01-01},
  pages = {109-120},
  keywords = {Availability,Diameter,Random input,Temporal networks},
  author = {Akrida, Eleni C. and Gąsieniec, Leszek and Mertzios, George B. and Spirakis, Paul G.},
  file = {/home/dimitri/Nextcloud/Zotero/storage/RJU2GI5T/10.1.1.721.6341.pdf;/home/dimitri/Nextcloud/Zotero/storage/6NLW8PWX/S0743731515001872.html}
}

@article{bensonSimplicialClosureHigherorder2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1802.06916},
  primaryClass = {cond-mat, physics:physics, stat},
  title = {Simplicial {{Closure}} and {{Higher}}-Order {{Link Prediction}}},
  url = {http://arxiv.org/abs/1802.06916},
  abstract = {Networks provide a powerful formalism for modeling complex systems, by representing the underlying set of pairwise interactions. But much of the structure within these systems involves interactions that take place among more than two nodes at once; for example, communication within a group rather than person-to-person, collaboration among a team rather than a pair of co-authors, or biological interaction between a set of molecules rather than just two. We refer to these type of simultaneous interactions on sets of more than two nodes as higher-order interactions; they are ubiquitous, but the empirical study of them has lacked a general framework for evaluating higher-order models. Here we introduce such a framework, based on link prediction, a fundamental problem in network analysis. The traditional link prediction problem seeks to predict the appearance of new links in a network, and here we adapt it to predict which (larger) sets of elements will have future interactions. We study the temporal evolution of 19 datasets from a variety of domains, and use our higher-order formulation of link prediction to assess the types of structural features that are most predictive of new multi-way interactions. Among our results, we find that different domains vary considerably in their distribution of higher-order structural parameters, and that the higher-order link prediction problem exhibits some fundamental differences from traditional pairwise link prediction, with a greater role for local rather than long-range information in predicting the appearance of new interactions.},
  urldate = {2018-02-27},
  date = {2018-02-19},
  keywords = {Statistics - Machine Learning,Mathematics - Algebraic Topology,Physics - Physics and Society,Computer Science - Social and Information Networks,Condensed Matter - Statistical Mechanics},
  author = {Benson, Austin R. and Abebe, Rediet and Schaub, Michael T. and Jadbabaie, Ali and Kleinberg, Jon},
  file = {/home/dimitri/Nextcloud/Zotero/storage/C5IG7QGL/Benson et al. - 2018 - Simplicial Closure and Higher-order Link Predictio.pdf}
}

@article{mellorTemporalEventGraph2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1706.02128},
  title = {The {{Temporal Event Graph}}},
  issn = {2051-1310, 2051-1329},
  url = {http://arxiv.org/abs/1706.02128},
  doi = {10.1093/comnet/cnx048},
  abstract = {Temporal networks are increasingly being used to model the interactions of complex systems. Most studies require the temporal aggregation of edges (or events) into discrete time steps to perform analysis. In this article we describe a static, lossless, and unique representation of a temporal network, the temporal event graph (TEG). The TEG describes the temporal network in terms of both the inter-event time and two-event temporal motif distributions. By considering these distributions in unison we provide a new method to characterise the behaviour of individuals and collectives in temporal networks as well as providing a natural decomposition of the network. We illustrate the utility of the TEG by providing examples on both synthetic and real temporal networks.},
  journaltitle = {Journal of Complex Networks},
  urldate = {2018-02-27},
  date = {2017-10-06},
  keywords = {Physics - Physics and Society,Computer Science - Social and Information Networks,Nonlinear Sciences - Adaptation and Self-Organizing Systems,Physics - Data Analysis; Statistics and Probability},
  author = {Mellor, Andrew},
  file = {/home/dimitri/Nextcloud/Zotero/storage/6HQ7IV56/Mellor - 2017 - The Temporal Event Graph.pdf;/home/dimitri/Nextcloud/Zotero/storage/QGSF97W4/1706.html}
}

@article{ohComplexContagionsTimers2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1706.04252},
  primaryClass = {nlin, physics:physics},
  title = {Complex {{Contagions}} with {{Timers}}},
  url = {http://arxiv.org/abs/1706.04252},
  abstract = {A great deal of effort has gone into trying to model social influence --- including the spread of behavior, norms, and ideas --- on networks. Most models of social influence tend to assume that individuals react to changes in the states of their neighbors without any time delay, but this is often not true in social contexts, where (for various reasons) different agents can have different response times. To examine such situations, we introduce the idea of a timer into threshold models of social influence. The presence of timers on nodes delays the adoption --- i.e., change of state --- of each agent, which in turn delays the adoptions of its neighbors. With a homogeneous-distributed timer, in which all nodes exhibit the same amount of delay, adoption delays are also homogeneous, so the adoption order of nodes remains the same. However, heterogeneously-distributed timers can change the adoption order of nodes and hence the "adoption paths" through which state changes spread in a network. Using a threshold model of social contagions, we illustrate that heterogeneous timers can either accelerate or decelerate the spread of adoptions compared to an analogous situation with homogeneous timers, and we investigate the relationship of such acceleration or deceleration with respect to timer distribution and network structure. We derive an analytical approximation for the temporal evolution of the fraction of adopters by modifying a pair approximation of the Watts threshold model, and we find good agreement with numerical computations. We also examine our new timer model on networks constructed from empirical data.},
  urldate = {2018-02-27},
  date = {2017-06-13},
  keywords = {Mathematics - Probability,Physics - Physics and Society,Computer Science - Social and Information Networks,Nonlinear Sciences - Adaptation and Self-Organizing Systems,Mathematics - Dynamical Systems},
  author = {Oh, Se-Wook and Porter, Mason A.},
  file = {/home/dimitri/Nextcloud/Zotero/storage/DC3LZPEC/Oh and Porter - 2017 - Complex Contagions with Timers.pdf;/home/dimitri/Nextcloud/Zotero/storage/6FT2IFSL/1706.html}
}

@article{mellorClassifyingConversationDigital2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1801.10527},
  primaryClass = {nlin, physics:physics},
  title = {Classifying {{Conversation}} in {{Digital Communication}}},
  url = {http://arxiv.org/abs/1801.10527},
  abstract = {Many studies of digital communication, in particular of Twitter, use natural language processing (NLP) to find topics, assess sentiment, and describe user behaviour. In finding topics often the relationships between users who participate in the topic are neglected. We propose a novel method of describing and classifying online conversations using only the structure of the underlying temporal network and not the content of individual messages. This method utilises all available information in the temporal network (no aggregation), combining both topological and temporal structure using temporal motifs and inter-event times. This allows us create an embedding of the temporal network in order to describe the behaviour of individuals and collectives over time and examine the structure of conversation over multiple timescales.},
  urldate = {2018-02-27},
  date = {2018-01-31},
  keywords = {Physics - Physics and Society,Computer Science - Social and Information Networks,Nonlinear Sciences - Adaptation and Self-Organizing Systems},
  author = {Mellor, Andrew},
  file = {/home/dimitri/Nextcloud/Zotero/storage/XZ25JRM6/Mellor - 2018 - Classifying Conversation in Digital Communication.pdf;/home/dimitri/Nextcloud/Zotero/storage/YWMLZBQ8/1801.html}
}

@article{peelMultiscaleMixingPatterns2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1708.01236},
  primaryClass = {physics},
  title = {Multiscale Mixing Patterns in Networks},
  url = {http://arxiv.org/abs/1708.01236},
  abstract = {Assortative mixing in networks is the tendency for nodes with the same attributes, or metadata, to link to each other. It is a property often found in social networks manifesting as a higher tendency of links occurring between people with the same age, race, or political belief. Quantifying the level of assortativity or disassortativity (the preference of linking to nodes with different attributes) can shed light on the factors involved in the formation of links and contagion processes in complex networks. It is common practice to measure the level of assortativity according to the assortativity coefficient, or modularity in the case of discrete-valued metadata. This global value is the average level of assortativity across the network and may not be a representative statistic when mixing patterns are heterogeneous. For example, a social network spanning the globe may exhibit local differences in mixing patterns as a consequence of differences in cultural norms. Here, we introduce an approach to localise this global measure so that we can describe the assortativity, across multiple scales, at the node level. Consequently we are able to capture and qualitatively evaluate the distribution of mixing patterns in the network. We find that for many real-world networks the distribution of assortativity is skewed, overdispersed and multimodal. Our method provides a clearer lens through which we can more closely examine mixing patterns in networks.},
  urldate = {2018-02-27},
  date = {2017-08-03},
  keywords = {Physics - Physics and Society,Computer Science - Social and Information Networks,Physics - Data Analysis; Statistics and Probability},
  author = {Peel, Leto and Delvenne, Jean-Charles and Lambiotte, Renaud},
  file = {/home/dimitri/Nextcloud/Zotero/storage/YDCIYN5C/Peel et al. - 2017 - Multiscale mixing patterns in networks.pdf;/home/dimitri/Nextcloud/Zotero/storage/6YUS3U3T/1708.html}
}

@article{cangEvolutionaryHomologyCoupled2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1802.04677},
  primaryClass = {math, q-bio},
  title = {Evolutionary Homology on Coupled Dynamical Systems},
  url = {http://arxiv.org/abs/1802.04677},
  abstract = {Time dependence is a universal phenomenon in nature, and a variety of mathematical models in terms of dynamical systems have been developed to understand the time-dependent behavior of real-world problems. Originally constructed to analyze the topological persistence over spatial scales, persistent homology has rarely been devised for time evolution. We propose the use of a new filtration function for persistent homology which takes as input the adjacent oscillator trajectories of a dynamical system. We also regulate the dynamical system by a weighted graph Laplacian matrix derived from the network of interest, which embeds the topological connectivity of the network into the dynamical system. The resulting topological signatures, which we call evolutionary homology (EH) barcodes, reveal the topology-function relationship of the network and thus give rise to the quantitative analysis of nodal properties. The proposed EH is applied to protein residue networks for protein thermal fluctuation analysis, rendering the most accurate B-factor prediction of a set of 364 proteins. This work extends the utility of dynamical systems to the quantitative modeling and analysis of realistic physical systems.},
  urldate = {2018-04-05},
  date = {2018-02-13},
  keywords = {Mathematics - Algebraic Topology,Mathematics - Dynamical Systems,Quantitative Biology - Quantitative Methods},
  author = {Cang, Zixuan and Munch, Elizabeth and Wei, Guo-Wei},
  file = {/home/dimitri/Nextcloud/Zotero/storage/4TZC2U2K/Cang et al. - 2018 - Evolutionary homology on coupled dynamical systems.pdf;/home/dimitri/Nextcloud/Zotero/storage/6RNFZZ93/Cang et al. - 2018 - Evolutionary homology on coupled dynamical systems.pdf;/home/dimitri/Nextcloud/Zotero/storage/984CQS7D/1802.html;/home/dimitri/Nextcloud/Zotero/storage/IR4MU62L/1802.html}
}

@article{salnikovCooccurrenceSimplicialComplexes2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1803.04410},
  primaryClass = {physics},
  title = {Co-Occurrence Simplicial Complexes in Mathematics: Identifying the Holes of Knowledge},
  url = {http://arxiv.org/abs/1803.04410},
  shorttitle = {Co-Occurrence Simplicial Complexes in Mathematics},
  abstract = {In the last years complex networks tools contributed to provide insights on the structure of research, through the study of collaboration, citation and co-occurrence networks. The network approach focuses on pairwise relationships, often compressing multidimensional data structures and inevitably losing information. In this paper we propose for the first time a simplicial complex approach to word co-occurrences, providing a natural framework for the study of higher-order relations in the space of scientific knowledge. Using topological methods we explore the conceptual landscape of mathematical research, focusing on homological holes, regions with low connectivity in the simplicial structure. We find that homological holes are ubiquitous, which suggests that they capture some essential feature of research practice in mathematics. Holes die when a subset of their concepts appear in the same article, hence their death may be a sign of the creation of new knowledge, as we show with some examples. We find a positive relation between the dimension of a hole and the time it takes to be closed: larger holes may represent potential for important advances in the field because they separate conceptually distant areas. We also show that authors' conceptual entropy is positively related with their contribution to homological holes, suggesting that polymaths tend to be on the frontier of research.},
  urldate = {2018-04-12},
  date = {2018-03-11},
  keywords = {Physics - Physics and Society,Computer Science - Digital Libraries,Mathematics - History and Overview},
  author = {Salnikov, Vsevolod and Cassese, Daniele and Lambiotte, Renaud and Jones, Nick S.},
  file = {/home/dimitri/Nextcloud/Zotero/storage/HVHFGEJV/Salnikov et al. - 2018 - Co-occurrence simplicial complexes in mathematics.pdf;/home/dimitri/Nextcloud/Zotero/storage/NZ7QXYNU/1803.html}
}

@article{otterRoadmapComputationPersistent2017,
  langid = {english},
  title = {A Roadmap for the Computation of Persistent Homology},
  volume = {6},
  issn = {2193-1127},
  url = {https://link.springer.com/article/10.1140/epjds/s13688-017-0109-5},
  doi = {10.1140/epjds/s13688-017-0109-5},
  abstract = {Persistent homology (PH) is a method used in topological data analysis (TDA) to study qualitative features of data that persist across multiple scales. It is robust to perturbations of input data, independent of dimensions and coordinates, and provides a compact representation of the qualitative features of the input. The computation of PH is an open area with numerous important and fascinating challenges. The field of PH computation is evolving rapidly, and new algorithms and software implementations are being updated and released at a rapid pace. The purposes of our article are to (1) introduce theory and computational methods for PH to a broad range of computational scientists and (2) provide benchmarks of state-of-the-art implementations for the computation of PH. We give a friendly introduction to PH, navigate the pipeline for the computation of PH with an eye towards applications, and use a range of synthetic and real-world data sets to evaluate currently available open-source implementations for the computation of PH. Based on our benchmarking, we indicate which algorithms and implementations are best suited to different types of data sets. In an accompanying tutorial, we provide guidelines for the computation of PH. We make publicly available all scripts that we wrote for the tutorial, and we make available the processed version of the data sets used in the benchmarking.},
  number = {1},
  journaltitle = {EPJ Data Science},
  shortjournal = {EPJ Data Sci.},
  urldate = {2018-04-12},
  date = {2017-12-01},
  pages = {17},
  author = {Otter, Nina and Porter, Mason A. and Tillmann, Ulrike and Grindrod, Peter and Harrington, Heather A.},
  file = {/home/dimitri/Nextcloud/Zotero/storage/UJRUWEUA/Otter et al. - 2017 - A roadmap for the computation of persistent homolo.pdf;/home/dimitri/Nextcloud/Zotero/storage/6XMV77X9/s13688-017-0109-5.html}
}

@article{zomorodianComputingPersistentHomology2005,
  langid = {english},
  title = {Computing {{Persistent Homology}}},
  volume = {33},
  issn = {0179-5376, 1432-0444},
  url = {https://link.springer.com/article/10.1007/s00454-004-1146-y},
  doi = {10.1007/s00454-004-1146-y},
  abstract = {We show that the persistent homology of a filtered d-dimensional simplicial complex is simply the standard homology of a particular graded module over a polynomial ring. Our analysis establishes the existence of a simple description of persistent homology groups over arbitrary fields. It also enables us to derive a natural algorithm for computing persistent homology of spaces in arbitrary dimension over any field. This result generalizes and extends the previously known algorithm that was restricted to subcomplexes of S3 and Z2 coefficients. Finally, our study implies the lack of a simple classification over non-fields. Instead, we give an algorithm for computing individual persistent homology groups over an arbitrary principal ideal domain in any dimension.},
  number = {2},
  journaltitle = {Discrete \& Computational Geometry},
  shortjournal = {Discrete Comput Geom},
  urldate = {2018-04-16},
  date = {2005-02-01},
  pages = {249-274},
  author = {Zomorodian, Afra and Carlsson, Gunnar},
  file = {/home/dimitri/Nextcloud/Zotero/storage/TB6ZGPWL/Zomorodian and Carlsson - 2005 - Computing Persistent Homology.pdf;/home/dimitri/Nextcloud/Zotero/storage/HQ6KTAAF/s00454-004-1146-y.html}
}

@software{reininghausDIPHADistributedPersistent2018,
  title = {{{DIPHA}} ({{A Distributed Persistent Homology Algorithm}})},
  url = {https://github.com/DIPHA/dipha},
  organization = {{DIPHA}},
  urldate = {2018-04-16},
  date = {2018-04-03T08:01:57Z},
  author = {Reininghaus, Jan},
  file = {/home/dimitri/Nextcloud/Zotero/storage/VSIEADNZ/dipha.html},
  origdate = {2015-12-25T17:23:32Z}
}

@software{bauerRipserRipserLean2018,
  title = {Ripser: {{Ripser}}: A Lean {{C}}++ Code for the Computation of {{Vietoris}}–{{Rips}} Persistence Barcodes},
  url = {https://github.com/Ripser/ripser},
  shorttitle = {Ripser},
  organization = {{Ripser}},
  urldate = {2018-04-16},
  date = {2018-04-03T08:02:57Z},
  author = {Bauer, Ulrich},
  file = {/home/dimitri/Nextcloud/Zotero/storage/HZRP5QNK/ripser.html},
  origdate = {2015-10-27T21:43:59Z}
}

@book{zomorodianTopologyComputing2009,
  location = {{New York, NY, USA}},
  title = {Topology for {{Computing}}},
  isbn = {978-0-521-13609-9},
  abstract = {Written by a computer scientist for computer scientists, this book teaches topology from a computational point of view, and shows how to solve real problems that have topological aspects involving computers. Such problems arise in many areas, such as computer graphics, robotics, structural biology, and chemistry. The author starts from the basics of topology, assuming no prior exposure to the subject, and moves rapidly up to recent advances in the area, including topological persistence and hierarchical Morse complexes. Algorithms and data structures are presented when appropriate.},
  publisher = {{Cambridge University Press}},
  date = {2009},
  author = {Zomorodian, Afra J.},
  file = {/home/dimitri/Nextcloud/Zotero/storage/4JNUZVQS/Zomorodian - 2009 - Topology for Computing.pdf}
}

@book{jonssonSimplicialComplexesGraphs2008,
  langid = {english},
  location = {{Berlin}},
  title = {Simplicial Complexes of Graphs},
  isbn = {978-3-540-75859-4},
  url = {https://cds.cern.ch/record/1691716},
  abstract = {A graph complex is a finite family of graphs closed under deletion of edges. Graph complexes show up naturally in many different areas of mathematics, including commutative algebra, geometry, and knot theory. Identifying each graph with its edge set, one may view a graph complex as a simplicial complex and hence interpret it as a geometric object. This volume examines topological properties of graph complexes, focusing on homotopy type and homology. Many of the proofs are based on Robin Forman's discrete version of Morse theory. As a byproduct, this volume also provides a loosely defined toolbox for attacking problems in topological combinatorics via discrete Morse theory. In terms of simplicity and power, arguably the most efficient tool is Forman's divide and conquer approach via decision trees; it is successfully applied to a large number of graph and digraph complexes.},
  series = {Lecture {{Notes}} in {{Mathematics}}},
  publisher = {{Springer}},
  urldate = {2018-04-16},
  date = {2008},
  author = {Jonsson, Jakob},
  file = {/home/dimitri/Nextcloud/Zotero/storage/689R2YHC/Jonsson - 2008 - Simplicial complexes of graphs.pdf;/home/dimitri/Nextcloud/Zotero/storage/7CIVG53B/1691716.html},
  doi = {10.1007/978-3-540-75859-4, 10.1007/978-3-540-75859-4}
}

@article{horakPersistentHomologyComplex2009,
  langid = {english},
  title = {Persistent Homology of Complex Networks},
  volume = {2009},
  issn = {1742-5468},
  url = {http://stacks.iop.org/1742-5468/2009/i=03/a=P03034},
  doi = {10.1088/1742-5468/2009/03/P03034},
  abstract = {Long-lived topological features are distinguished from short-lived ones (considered as topological noise) in simplicial complexes constructed from complex networks. A new topological invariant, persistent homology, is determined and presented as a parameterized version of a Betti number. Complex networks with distinct degree distributions exhibit distinct persistent topological features. Persistent topological attributes, shown to be related to the robust quality of networks, also reflect the deficiency in certain connectivity properties of networks. Random networks, networks with exponential connectivity distribution and scale-free networks were considered for homological persistency analysis.},
  number = {03},
  journaltitle = {Journal of Statistical Mechanics: Theory and Experiment},
  shortjournal = {J. Stat. Mech.},
  urldate = {2018-04-16},
  date = {2009},
  pages = {P03034},
  author = {Horak, Danijela and Maletić, Slobodan and Rajković, Milan},
  file = {/home/dimitri/Nextcloud/Zotero/storage/IT5PKTTS/Horak et al. - 2009 - Persistent homology of complex networks.pdf}
}

@software{morozovDionysusLibraryComputing2018,
  title = {Dionysus: {{Library}} for Computing Persistent Homology},
  url = {https://github.com/mrzv/dionysus},
  shorttitle = {Dionysus},
  urldate = {2018-04-16},
  date = {2018-04-11T09:29:54Z},
  author = {Morozov, Dimitriy},
  file = {/home/dimitri/Nextcloud/Zotero/storage/BBVYF9D2/dionysus.html},
  origdate = {2017-07-14T19:02:35Z}
}

@article{turnerFrechetMeansDistributions2014,
  langid = {english},
  title = {Fréchet {{Means}} for {{Distributions}} of {{Persistence Diagrams}}},
  volume = {52},
  issn = {0179-5376, 1432-0444},
  url = {https://link.springer.com/article/10.1007/s00454-014-9604-7},
  doi = {10.1007/s00454-014-9604-7},
  abstract = {Given a distribution ρ\textbackslash{}rho on persistence diagrams and observations X1,…,Xn∼iidρX\_\{1\},\textbackslash{}ldots ,X\_\{n\} \textbackslash{}mathop \{\textbackslash{}sim \}\textbackslash{}limits \^\{iid\} \textbackslash{}rho we introduce an algorithm in this paper that estimates a Fréchet mean from the set of diagrams X1,…,XnX\_\{1\},\textbackslash{}ldots ,X\_\{n\}. If the underlying measure ρ\textbackslash{}rho is a combination of Dirac masses ρ=1m∑mi=1δZi\textbackslash{}rho = \textbackslash{}frac\{1\}\{m\} \textbackslash{}sum \_\{i=1\}\^\{m\} \textbackslash{}delta \_\{Z\_\{i\}\} then we prove the algorithm converges to a local minimum and a law of large numbers result for a Fréchet mean computed by the algorithm given observations drawn iid from ρ\textbackslash{}rho . We illustrate the convergence of an empirical mean computed by the algorithm to a population mean by simulations from Gaussian random fields.},
  number = {1},
  journaltitle = {Discrete \& Computational Geometry},
  shortjournal = {Discrete Comput Geom},
  urldate = {2018-04-20},
  date = {2014-07-01},
  pages = {44-70},
  author = {Turner, Katharine and Mileyko, Yuriy and Mukherjee, Sayan and Harer, John},
  file = {/home/dimitri/Nextcloud/Zotero/storage/WFNRGRL6/Turner et al. - 2014 - Fréchet Means for Distributions of Persistence Dia.pdf;/home/dimitri/Nextcloud/Zotero/storage/TIA4XC3D/s00454-014-9604-7.html}
}

@article{munchProbabilisticFrechetMeans2015,
  langid = {english},
  title = {Probabilistic {{Fréchet}} Means for Time Varying Persistence Diagrams},
  volume = {9},
  issn = {1935-7524},
  url = {https://projecteuclid.org/euclid.ejs/1433195858},
  doi = {10.1214/15-EJS1030},
  abstract = {Project Euclid - mathematics and statistics online},
  number = {1},
  journaltitle = {Electronic Journal of Statistics},
  urldate = {2018-04-20},
  date = {2015},
  pages = {1173-1204},
  author = {Munch, Elizabeth and Turner, Katharine and Bendich, Paul and Mukherjee, Sayan and Mattingly, Jonathan and Harer, John},
  file = {/home/dimitri/Nextcloud/Zotero/storage/HRY5Z3E2/Munch et al. - 2015 - Probabilistic Fréchet means for time varying persi.pdf;/home/dimitri/Nextcloud/Zotero/storage/DEP25GGC/1433195858.html}
}

@article{bubenikStatisticalTopologicalData2015,
  title = {Statistical {{Topological Data Analysis}} Using {{Persistence Landscapes}}},
  volume = {16},
  url = {http://www.jmlr.org/papers/v16/bubenik15a.html},
  journaltitle = {Journal of Machine Learning Research},
  urldate = {2018-04-20},
  date = {2015},
  pages = {77-102},
  author = {Bubenik, Peter},
  file = {/home/dimitri/Nextcloud/Zotero/storage/IQ3T72BZ/Bubenik - 2015 - Statistical Topological Data Analysis using Persis.pdf;/home/dimitri/Nextcloud/Zotero/storage/94GGQHGV/bubenik15a.html}
}

@incollection{kwittStatisticalTopologicalData2015,
  title = {Statistical {{Topological Data Analysis}} - {{A Kernel Perspective}}},
  url = {http://papers.nips.cc/paper/5887-statistical-topological-data-analysis-a-kernel-perspective.pdf},
  booktitle = {Advances in {{Neural Information Processing Systems}} 28},
  publisher = {{Curran Associates, Inc.}},
  urldate = {2018-04-20},
  date = {2015},
  pages = {3070--3078},
  author = {Kwitt, Roland and Huber, Stefan and Niethammer, Marc and Lin, Weili and Bauer, Ulrich},
  editor = {Cortes, C. and Lawrence, N. D. and Lee, D. D. and Sugiyama, M. and Garnett, R.},
  file = {/home/dimitri/Nextcloud/Zotero/storage/9NRWV859/Kwitt et al. - 2015 - Statistical Topological Data Analysis - A Kernel P.pdf;/home/dimitri/Nextcloud/Zotero/storage/G7LF48UM/5887-statistical-topological-data-analysis-a-kernel-perspective.html}
}

@article{petriTopologicalStrataWeighted2013,
  title = {Topological {{Strata}} of {{Weighted Complex Networks}}},
  volume = {8},
  url = {http://adsabs.harvard.edu/abs/2013PLoSO...866506P},
  doi = {10.1371/journal.pone.0066506},
  abstract = {The statistical mechanical approach to complex networks is the dominant paradigm in describing natural and societal complex systems. The study of network properties, and their implications on dynamical processes, mostly focus on locally defined quantities of nodes and edges, such as node degrees, edge weights and --more recently-- correlations between neighboring nodes. However, statistical methods quickly become
cumbersome when dealing with many-body properties and do not capture the precise mesoscopic structure of complex networks. Here we introduce a novel method, based on persistent homology, to detect particular non-local structures, akin to weighted holes within the link-weight network fabric, which are invisible to existing methods. Their
properties divide weighted networks in two broad classes: one is characterized by small hierarchically nested holes, while the second displays larger and longer living inhomogeneities. These classes cannot be reduced to known local or quasilocal network properties, because of the intrinsic non-locality of homological properties, and thus yield a new classification built on high order coordination patterns. Our results show that topology can provide novel insights relevant for many-body interactions in social and spatial networks. Moreover, this new method creates the first bridge between network theory and algebraic topology, which will allow to import the toolset of algebraic methods to complex systems.},
  journaltitle = {PLoS ONE},
  shortjournal = {PLoS ONE},
  urldate = {2018-04-20},
  date = {2013-06-01},
  pages = {e66506},
  author = {Petri, Giovanni and Scolamiero, Martina and Donato, Irene and Vaccarino, Francesco},
  file = {/home/dimitri/Nextcloud/Zotero/storage/X43JU3GL/Topological Strata of Weighted Complex Networks.PDF}
}

@inproceedings{carlssonZigzagPersistentHomology2009,
  location = {{New York, NY, USA}},
  title = {Zigzag {{Persistent Homology}} and {{Real}}-Valued {{Functions}}},
  isbn = {978-1-60558-501-7},
  url = {http://doi.acm.org/10.1145/1542362.1542408},
  doi = {10.1145/1542362.1542408},
  abstract = {We study the problem of computing zigzag persistence of a sequence of homology groups and study a particular sequence derived from the levelsets of a real-valued function on a topological space. The result is a local, symmetric interval descriptor of the function. Our structural results establish a connection between the zigzag pairs in this sequence and extended persistence, and in the process resolve an open question associated with the latter. Our algorithmic results not only provide a way to compute zigzag persistence for any sequence of homology groups, but combined with our structural results give a novel algorithm for computing extended persistence. This algorithm is easily parallelizable and uses (asymptotically) less memory.},
  booktitle = {Proceedings of the {{Twenty}}-Fifth {{Annual Symposium}} on {{Computational Geometry}}},
  series = {{{SCG}} '09},
  publisher = {{ACM}},
  urldate = {2018-04-20},
  date = {2009},
  pages = {247--256},
  keywords = {algorithms,extended persistence,levelset zigzag,Mayer-Vietoris pyramid,zigzag persistent homology},
  author = {Carlsson, Gunnar and family=Silva, given=Vin, prefix=de, useprefix=true and Morozov, Dmitriy},
  file = {/home/dimitri/Nextcloud/Zotero/storage/WNIUXA7Y/Carlsson et al. - 2009 - Zigzag Persistent Homology and Real-valued Functio.pdf}
}

@inproceedings{deyComputingTopologicalPersistence2014,
  location = {{New York, NY, USA}},
  title = {Computing {{Topological Persistence}} for {{Simplicial Maps}}},
  isbn = {978-1-4503-2594-3},
  url = {http://doi.acm.org/10.1145/2582112.2582165},
  doi = {10.1145/2582112.2582165},
  abstract = {Algorithms for persistent homology are well-studied where homomorphisms are induced by inclusion maps. In this paper, we propose a practical algorithm for computing persistence under Z2 coefficients for a (monotone) sequence of general simplicial maps and show how these maps arise naturally in some applications of topological data analysis. A simplicial map can be decomposed into a set of elementary inclusions and vertex collapses--two atomic operations that can be supported efficiently with the notion of simplex annotations for computing persistent homology. A consistent annotation through these atomic operations implies the maintenance of a consistent cohomology basis, hence a homology basis by duality. While the idea of maintaining a cohomology basis through an inclusion is not new, maintaining them through a vertex collapse is new, which constitutes an important atomic operation for simulating simplicial maps. Annotations support the vertex collapse in addition to the usual inclusion quite naturally. Finally, we exhibit an application of this new tool in which we approximate the persistence diagram of a filtration of Rips complexes where vertex collapses are used to tame the blow-up in size.},
  booktitle = {Proceedings of the {{Thirtieth Annual Symposium}} on {{Computational Geometry}}},
  series = {{{SOCG}}'14},
  publisher = {{ACM}},
  urldate = {2018-04-20},
  date = {2014},
  pages = {345:345--345:354},
  keywords = {cohomology,homology,simplicial maps,topological data analysis,Topological persistence},
  author = {Dey, Tamal K. and Fan, Fengtao and Wang, Yusu},
  file = {/home/dimitri/Nextcloud/Zotero/storage/X6R4GKRU/Dey et al. - 2014 - Computing Topological Persistence for Simplicial M.pdf}
}

@article{carlssonTheoryMultidimensionalPersistence2009,
  langid = {english},
  title = {The {{Theory}} of {{Multidimensional Persistence}}},
  volume = {42},
  issn = {0179-5376, 1432-0444},
  url = {https://link.springer.com/article/10.1007/s00454-009-9176-0},
  doi = {10.1007/s00454-009-9176-0},
  abstract = {Persistent homology captures the topology of a filtration—a one-parameter family of increasing spaces—in terms of a complete discrete invariant. This invariant is a multiset of intervals that denote the lifetimes of the topological entities within the filtration. In many applications of topology, we need to study a multifiltration: a family of spaces parameterized along multiple geometric dimensions. In this paper, we show that no similar complete discrete invariant exists for multidimensional persistence. Instead, we propose the rank invariant, a discrete invariant for the robust estimation of Betti numbers in a multifiltration, and prove its completeness in one dimension.},
  number = {1},
  journaltitle = {Discrete \& Computational Geometry},
  shortjournal = {Discrete Comput Geom},
  urldate = {2018-04-30},
  date = {2009-07-01},
  pages = {71-93},
  author = {Carlsson, Gunnar and Zomorodian, Afra},
  file = {/home/dimitri/Nextcloud/Zotero/storage/4EEYB2MK/10.1.1.86.1620.pdf;/home/dimitri/Nextcloud/Zotero/storage/RBN5LKT6/Carlsson and Zomorodian - 2009 - The Theory of Multidimensional Persistence.pdf;/home/dimitri/Nextcloud/Zotero/storage/PT7Q4KVU/10.html}
}

@article{bazziGenerativeBenchmarkModels2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1608.06196},
  primaryClass = {cond-mat, physics:nlin, physics:physics, stat},
  title = {Generative {{Benchmark Models}} for {{Mesoscale Structure}} in {{Multilayer Networks}}},
  url = {http://arxiv.org/abs/1608.06196},
  abstract = {Multilayer networks allow one to represent diverse and interdependent connectivity patterns --- e.g., time-dependence, multiple subsystems, or both --- that arise in many applications and which are difficult or awkward to incorporate into standard network representations. In the study of multilayer networks, it is important to investigate "mesoscale" (i.e., intermediate-scale) structures, such as dense sets of nodes known as "communities" that are connected sparsely to each other, to discover network features that are not apparent at the microscale or the macroscale. A variety of methods and algorithms are available to identify communities in multilayer networks, but they differ in their definitions and/or assumptions of what constitutes a community, and many scalable algorithms provide approximate solutions with little or no theoretical guarantee on the quality of their approximations. Consequently, it is crucial to develop generative models of networks to use as a common test of community-detection tools. In the present paper, we develop a family of benchmarks for detecting mesoscale structures in multilayer networks by introducing a generative model that can explicitly incorporate dependency structure between layers. Our benchmark provides a standardized set of null models, together with an associated set of principles from which they are derived, for studies of mesoscale structures in multilayer networks. We discuss the parameters and properties of our generative model, and we illustrate its use by comparing a variety of community-detection methods.},
  urldate = {2018-04-30},
  date = {2016-08-22},
  keywords = {Physics - Physics and Society,Statistics - Methodology,Computer Science - Social and Information Networks,Condensed Matter - Statistical Mechanics,Nonlinear Sciences - Adaptation and Self-Organizing Systems},
  author = {Bazzi, Marya and Jeub, Lucas G. S. and Arenas, Alex and Howison, Sam D. and Porter, Mason A.},
  file = {/home/dimitri/Nextcloud/Zotero/storage/LRM9HWTC/Bazzi et al. - 2016 - Generative Benchmark Models for Mesoscale Structur.pdf;/home/dimitri/Nextcloud/Zotero/storage/JM7VWEGD/1608.html}
}

@article{sekaraFundamentalStructuresDynamic2016,
  langid = {english},
  title = {Fundamental Structures of Dynamic Social Networks},
  volume = {113},
  issn = {0027-8424, 1091-6490},
  url = {http://www.pnas.org/content/113/36/9977},
  doi = {10.1073/pnas.1602803113},
  abstract = {Social systems are in a constant state of flux, with dynamics spanning from minute-by-minute changes to patterns present on the timescale of years. Accurate models of social dynamics are important for understanding the spreading of influence or diseases, formation of friendships, and the productivity of teams. Although there has been much progress on understanding complex networks over the past decade, little is known about the regularities governing the microdynamics of social networks. Here, we explore the dynamic social network of a densely-connected population of ∼1,000 individuals and their interactions in the network of real-world person-to-person proximity measured via Bluetooth, as well as their telecommunication networks, online social media contacts, geolocation, and demographic data. These high-resolution data allow us to observe social groups directly, rendering community detection unnecessary. Starting from 5-min time slices, we uncover dynamic social structures expressed on multiple timescales. On the hourly timescale, we find that gatherings are fluid, with members coming and going, but organized via a stable core of individuals. Each core represents a social context. Cores exhibit a pattern of recurring meetings across weeks and months, each with varying degrees of regularity. Taken together, these findings provide a powerful simplification of the social network, where cores represent fundamental structures expressed with strong temporal and spatial regularity. Using this framework, we explore the complex interplay between social and geospatial behavior, documenting how the formation of cores is preceded by coordination behavior in the communication networks and demonstrating that social behavior can be predicted with high precision.},
  number = {36},
  journaltitle = {Proceedings of the National Academy of Sciences},
  shortjournal = {PNAS},
  urldate = {2018-04-30},
  date = {2016-09-06},
  pages = {9977-9982},
  keywords = {complex networks,computational social science,human dynamics,human mobility,social systems},
  author = {Sekara, Vedran and Stopczynski, Arkadiusz and Lehmann, Sune},
  file = {/home/dimitri/Nextcloud/Zotero/storage/IV3NN8R3/pnas.1602803113.sapp.pdf;/home/dimitri/Nextcloud/Zotero/storage/XX3SU37E/Sekara et al. - 2016 - Fundamental structures of dynamic social networks.pdf;/home/dimitri/Nextcloud/Zotero/storage/WIREJBWU/9977.html},
  eprinttype = {pmid},
  eprint = {27555584}
}

@article{peelDetectingChangePoints2014,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1403.0989},
  primaryClass = {physics, stat},
  title = {Detecting Change Points in the Large-Scale Structure of Evolving Networks},
  url = {http://arxiv.org/abs/1403.0989},
  abstract = {Interactions among people or objects are often dynamic in nature and can be represented as a sequence of networks, each providing a snapshot of the interactions over a brief period of time. An important task in analyzing such evolving networks is change-point detection, in which we both identify the times at which the large-scale pattern of interactions changes fundamentally and quantify how large and what kind of change occurred. Here, we formalize for the first time the network change-point detection problem within an online probabilistic learning framework and introduce a method that can reliably solve it. This method combines a generalized hierarchical random graph model with a Bayesian hypothesis test to quantitatively determine if, when, and precisely how a change point has occurred. We analyze the detectability of our method using synthetic data with known change points of different types and magnitudes, and show that this method is more accurate than several previously used alternatives. Applied to two high-resolution evolving social networks, this method identifies a sequence of change points that align with known external "shocks" to these networks.},
  urldate = {2018-04-30},
  date = {2014-03-04},
  keywords = {Statistics - Machine Learning,Physics - Physics and Society,Computer Science - Social and Information Networks},
  author = {Peel, Leto and Clauset, Aaron},
  file = {/home/dimitri/Nextcloud/Zotero/storage/4DBDLPT3/Peel and Clauset - 2014 - Detecting change points in the large-scale structu.pdf;/home/dimitri/Nextcloud/Zotero/storage/4IGGSISH/1403.html}
}

@article{pereaSlidingWindowsPersistence2017,
  title = {Sliding Windows and Persistence},
  volume = {141},
  issn = {0001-4966},
  url = {https://asa.scitation.org/doi/abs/10.1121/1.4987655},
  doi = {10.1121/1.4987655},
  number = {5},
  journaltitle = {The Journal of the Acoustical Society of America},
  shortjournal = {The Journal of the Acoustical Society of America},
  urldate = {2018-05-02},
  date = {2017-05-01},
  pages = {3585-3585},
  author = {Perea, Jose and Traile, Chris},
  file = {/home/dimitri/Nextcloud/Zotero/storage/NAPFXSEL/1.html}
}

@article{pereaSlidingWindowsPersistence2015,
  langid = {english},
  title = {Sliding {{Windows}} and {{Persistence}}: {{An Application}} of {{Topological Methods}} to {{Signal Analysis}}},
  volume = {15},
  issn = {1615-3375, 1615-3383},
  url = {https://link.springer.com/article/10.1007/s10208-014-9206-z},
  doi = {10.1007/s10208-014-9206-z},
  shorttitle = {Sliding {{Windows}} and {{Persistence}}},
  abstract = {We develop in this paper a theoretical framework for the topological study of time series data. Broadly speaking, we describe geometrical and topological properties of sliding window embeddings, as seen through the lens of persistent homology. In particular, we show that maximum persistence at the point-cloud level can be used to quantify periodicity at the signal level, prove structural and convergence theorems for the resulting persistence diagrams, and derive estimates for their dependency on window size and embedding dimension. We apply this methodology to quantifying periodicity in synthetic data sets and compare the results with those obtained using state-of-the-art methods in gene expression analysis. We call this new method SW1PerS, which stands for Sliding Windows and 1-Dimensional Persistence Scoring.},
  number = {3},
  journaltitle = {Foundations of Computational Mathematics},
  shortjournal = {Found Comput Math},
  urldate = {2018-05-02},
  date = {2015-06-01},
  pages = {799-838},
  author = {Perea, Jose A. and Harer, John},
  file = {/home/dimitri/Nextcloud/Zotero/storage/2PSQQ8F4/Perea and Harer - 2015 - Sliding Windows and Persistence An Application of.pdf;/home/dimitri/Nextcloud/Zotero/storage/UDRR3GRW/s10208-014-9206-z.html}
}

@article{pereaSW1PerSSlidingWindows2015,
  title = {{{SW1PerS}}: {{Sliding}} Windows and 1-Persistence Scoring; Discovering Periodicity in Gene Expression Time Series Data},
  volume = {16},
  issn = {1471-2105},
  url = {https://doi.org/10.1186/s12859-015-0645-6},
  doi = {10.1186/s12859-015-0645-6},
  shorttitle = {{{SW1PerS}}},
  abstract = {Identifying periodically expressed genes across different processes (e.g. the cell and metabolic cycles, circadian rhythms, etc) is a central problem in computational biology. Biological time series may contain (multiple) unknown signal shapes of systemic relevance, imperfections like noise, damping, and trending, or limited sampling density. While there exist methods for detecting periodicity, their design biases (e.g. toward a specific signal shape) can limit their applicability in one or more of these situations.},
  journaltitle = {BMC Bioinformatics},
  shortjournal = {BMC Bioinformatics},
  urldate = {2018-05-02},
  date = {2015-08-16},
  pages = {257},
  keywords = {Gene expression,Periodicity,Persistent homology,Sliding windows,Time series},
  author = {Perea, Jose A. and Deckard, Anastasia and Haase, Steve B. and Harer, John},
  file = {/home/dimitri/Nextcloud/Zotero/storage/YH7DK289/Perea et al. - 2015 - SW1PerS Sliding windows and 1-persistence scoring.pdf;/home/dimitri/Nextcloud/Zotero/storage/2N8XI4AJ/s12859-015-0645-6.html}
}

@inproceedings{severskyTimeSeriesTopologicalData2016,
  title = {On {{Time}}-{{Series Topological Data Analysis}}: {{New Data}} and {{Opportunities}}},
  doi = {10.1109/CVPRW.2016.131},
  shorttitle = {On {{Time}}-{{Series Topological Data Analysis}}},
  abstract = {This work introduces a new dataset and framework for the exploration of topological data analysis (TDA) techniques applied to time-series data. We examine the end-toend TDA processing pipeline for persistent homology applied to time-delay embeddings of time series - embeddings that capture the underlying system dynamics from which time series data is acquired. In particular, we consider stability with respect to time series length, the approximation accuracy of sparse filtration methods, and the discriminating ability of persistence diagrams as a feature for learning. We explore these properties across a wide range of time-series datasets spanning multiple domains for single source multi-segment signals as well as multi-source single segment signals. Our analysis and dataset captures the entire TDA processing pipeline and includes time-delay embeddings, persistence diagrams, topological distance measures, as well as kernels for similarity learning and classification tasks for a broad set of time-series data sources. We outline the TDA framework and rationale behind the dataset and provide insights into the role of TDA for time-series analysis as well as opportunities for new work.},
  eventtitle = {2016 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition Workshops}} ({{CVPRW}})},
  booktitle = {2016 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition Workshops}} ({{CVPRW}})},
  date = {2016-06},
  pages = {1014-1022},
  keywords = {Topology,topological data analysis,approximation accuracy,approximation theory,Context,data analysis,embedded systems,Kernel,learning feature,Pipelines,signal processing,single source multisegment signals,sparse filtration methods,Support vector machines,system dynamics,TDA,Three-dimensional displays,time series,Time series analysis,time-delay embeddings,time-series data},
  author = {Seversky, L. M. and Davis, S. and Berger, M.},
  file = {/home/dimitri/Nextcloud/Zotero/storage/7BBZHTTF/Seversky et al. - 2016 - On Time-Series Topological Data Analysis New Data.pdf;/home/dimitri/Nextcloud/Zotero/storage/BINURQ6P/7789621.html}
}

@article{umedaTimeSeriesClassification2017,
  title = {Time {{Series Classification}} via {{Topological Data Analysis}}},
  volume = {32},
  issn = {1346-0714},
  url = {http://adsabs.harvard.edu/abs/2017TJSAI..32G..72U},
  doi = {10.1527/tjsai.D-G72},
  abstract = {Not Available},
  journaltitle = {Transactions of the Japanese Society for Artificial Intelligence},
  shortjournal = {Transactions of the Japanese Society for Artificial Intelligence},
  urldate = {2018-05-02},
  date = {2017},
  author = {Umeda, Yuhei},
  file = {/home/dimitri/Nextcloud/Zotero/storage/YK5UQZ4D/Umeda - 2017 - Time Series Classification via Topological Data An.pdf}
}

@article{newmanNetworkStructureRich2018,
  langid = {english},
  title = {Network Structure from Rich but Noisy Data},
  issn = {1745-2481},
  url = {https://www.nature.com/articles/s41567-018-0076-1},
  doi = {10.1038/s41567-018-0076-1},
  abstract = {A technique allows optimal inference of the structure of a network when the available observed data are rich but noisy, incomplete or otherwise unreliable.},
  journaltitle = {Nature Physics},
  urldate = {2018-05-19},
  date = {2018-03-12},
  pages = {1},
  author = {Newman, M. E. J.},
  file = {/home/dimitri/Nextcloud/Zotero/storage/F8AYYMEJ/Newman - 2018 - Network structure from rich but noisy data.pdf;/home/dimitri/Nextcloud/Zotero/storage/GK83I4XY/Newman - 2018 - Network structure from rich but noisy data.pdf;/home/dimitri/Nextcloud/Zotero/storage/MIZRK2YS/s41567-018-0076-1.html;/home/dimitri/Nextcloud/Zotero/storage/ST32RTSC/s41567-018-0076-1.html}
}

@article{newmanNetworkReconstructionError2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1803.02427},
  primaryClass = {physics},
  title = {Network Reconstruction and Error Estimation with Noisy Network Data},
  url = {http://arxiv.org/abs/1803.02427},
  abstract = {Most empirical studies of networks assume that the network data we are given represent a complete and accurate picture of the nodes and edges in the system of interest, but in real-world situations this is rarely the case. More often the data only specify the network structure imperfectly -- like data in essentially every other area of empirical science, network data are prone to measurement error and noise. At the same time, the data may be richer than simple network measurements, incorporating multiple measurements, weights, lengths or strengths of edges, node or edge labels, or annotations of various kinds. Here we develop a general method for making estimates of network structure and properties from any form of network data, simple or complex, when the data are unreliable, and give example applications to a selection of social and biological networks.},
  urldate = {2018-05-19},
  date = {2018-03-06},
  keywords = {Physics - Physics and Society,Computer Science - Social and Information Networks},
  author = {Newman, M. E. J.},
  file = {/home/dimitri/Nextcloud/Zotero/storage/HPS28S3C/Newman - 2018 - Network reconstruction and error estimation with n.pdf;/home/dimitri/Nextcloud/Zotero/storage/NL7F2LGR/1803.html}
}

@article{kusanoKernelMethodPersistence2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1706.03472},
  primaryClass = {physics, stat},
  title = {Kernel Method for Persistence Diagrams via Kernel Embedding and Weight Factor},
  url = {http://arxiv.org/abs/1706.03472},
  abstract = {Topological data analysis is an emerging mathematical concept for characterizing shapes in multi-scale data. In this field, persistence diagrams are widely used as a descriptor of the input data, and can distinguish robust and noisy topological properties. Nowadays, it is highly desired to develop a statistical framework on persistence diagrams to deal with practical data. This paper proposes a kernel method on persistence diagrams. A theoretical contribution of our method is that the proposed kernel allows one to control the effect of persistence, and, if necessary, noisy topological properties can be discounted in data analysis. Furthermore, the method provides a fast approximation technique. The method is applied into several problems including practical data in physics, and the results show the advantage compared to the existing kernel method on persistence diagrams.},
  urldate = {2018-06-12},
  date = {2017-06-12},
  keywords = {Statistics - Machine Learning,Mathematics - Algebraic Topology,Physics - Data Analysis; Statistics and Probability},
  author = {Kusano, Genki and Fukumizu, Kenji and Hiraoka, Yasuaki},
  file = {/home/dimitri/Nextcloud/Zotero/storage/ISLEIEE9/Kusano et al. - 2017 - Kernel method for persistence diagrams via kernel .pdf;/home/dimitri/Nextcloud/Zotero/storage/6FIXWZVF/1706.html}
}

@article{adamsPersistenceImagesStable2017,
  title = {Persistence {{Images}}: {{A Stable Vector Representation}} of {{Persistent Homology}}},
  volume = {18},
  url = {http://jmlr.org/papers/v18/16-337.html},
  shorttitle = {Persistence {{Images}}},
  abstract = {Many data sets can be viewed as a noisy sampling of an
underlying space, and tools from topological data analysis can
characterize this structure for the purpose of knowledge
discovery. One such tool is persistent homology, which provides
a multiscale description of the homological features within a
data set. A useful representation of this homological
information is a persistence diagram (PD). Efforts have
been made to map PDs into spaces with additional structure
valuable to machine learning tasks. We convert a PD to a finite-
dimensional vector representation which we call a
persistence image (PI), and prove the stability of this
transformation with respect to small perturbations in the
inputs. The discriminatory power of PIs is compared against
existing methods, showing significant performance gains. We
explore the use of PIs with vector-based machine learning tools,
such as linear sparse support vector machines, which identify
features containing discriminating topological information.
Finally, high accuracy inference of parameter values from the
dynamic output of a discrete dynamical system (the linked
twist map) and a partial differential equation (the
anisotropic Kuramoto-Sivashinsky equation) provide a
novel application of the discriminatory power of PIs.},
  number = {8},
  journaltitle = {Journal of Machine Learning Research},
  urldate = {2018-06-12},
  date = {2017},
  pages = {1-35},
  author = {Adams, Henry and Emerson, Tegan and Kirby, Michael and Neville, Rachel and Peterson, Chris and Shipman, Patrick and Chepushtanova, Sofya and Hanson, Eric and Motta, Francis and Ziegelmeier, Lori},
  file = {/home/dimitri/Nextcloud/Zotero/storage/EUWNMLQF/Adams et al. - 2017 - Persistence Images A Stable Vector Representation.pdf}
}

@article{kalisnikTropicalCoordinatesSpace2018,
  langid = {english},
  title = {Tropical {{Coordinates}} on the {{Space}} of {{Persistence Barcodes}}},
  issn = {1615-3375, 1615-3383},
  url = {https://link.springer.com/article/10.1007/s10208-018-9379-y},
  doi = {10.1007/s10208-018-9379-y},
  abstract = {The aim of applied topology is to use and develop topological methods for applied mathematics, science and engineering. One of the main tools is persistent homology, an adaptation of classical homology, which assigns a barcode, i.e., a collection of intervals, to a finite metric space. Because of the nature of the invariant, barcodes are not well adapted for use by practitioners in machine learning tasks. We can circumvent this problem by assigning numerical quantities to barcodes, and these outputs can then be used as input to standard algorithms. It is the purpose of this paper to identify tropical coordinates on the space of barcodes and prove that they are stable with respect to the bottleneck distance and Wasserstein distances.},
  journaltitle = {Foundations of Computational Mathematics},
  shortjournal = {Found Comput Math},
  urldate = {2018-06-13},
  date = {2018-01-30},
  pages = {1-29},
  author = {Kališnik, Sara},
  file = {/home/dimitri/Nextcloud/Zotero/storage/VIP5PCKK/Kališnik - 2018 - Tropical Coordinates on the Space of Persistence B.pdf;/home/dimitri/Nextcloud/Zotero/storage/U2VKTXMW/10.html}
}

@article{gauvinRandomizedReferenceModels2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1806.04032},
  primaryClass = {physics, q-bio},
  title = {Randomized Reference Models for Temporal Networks},
  url = {http://arxiv.org/abs/1806.04032},
  abstract = {Many real-world dynamical systems can successfully be analyzed using the temporal network formalism. Empirical temporal networks and dynamic processes that take place in these situations show heterogeneous, non-Markovian, and intrinsically correlated dynamics, making their analysis particularly challenging. Randomized reference models (RRMs) for temporal networks constitute a versatile toolbox for studying such systems. Defined as ensembles of random networks with given features constrained to match those of an input (empirical) network, they may be used to identify statistically significant motifs in empirical temporal networks (i.e. overrepresented w.r.t. the null random networks) and to infer the effects of such motifs on dynamical processes unfolding in the network. However, the effects of most randomization procedures on temporal network characteristics remain poorly understood, rendering their use non-trivial and susceptible to misinterpretation. Here we propose a unified framework for classifying and understanding microcanonical RRMs (MRRMs). We use this framework to propose a canonical naming convention for existing randomization procedures, classify them, and deduce their effects on a range of important temporal network features. We furthermore show that certain classes of compatible MRRMs may be applied in sequential composition to generate more than a hundred new MRRMs from existing ones surveyed in this article. We provide a tutorial for the use of MRRMs to analyze an empirical temporal network and we review applications of MRRMs found in literature. The taxonomy of MRRMs we have developed provides a reference to ease the use of MRRMs, and the theoretical foundations laid here may further serve as a base for the development of a principled and systematic way to generate and apply randomized reference null models for the study of temporal networks.},
  urldate = {2018-06-14},
  date = {2018-06-11},
  keywords = {Physics - Physics and Society,Physics - Data Analysis; Statistics and Probability,Quantitative Biology - Quantitative Methods,Computer Science - Discrete Mathematics},
  author = {Gauvin, Laetitia and Génois, Mathieu and Karsai, Márton and Kivelä, Mikko and Takaguchi, Taro and Valdano, Eugenio and Vestergaard, Christian L.},
  file = {/home/dimitri/Nextcloud/Zotero/storage/GVBEMC2A/Gauvin et al. - 2018 - Randomized reference models for temporal networks.pdf;/home/dimitri/Nextcloud/Zotero/storage/8WF5HVDE/1806.html}
}

@article{liuESESSoftwareEulerian2017,
  langid = {english},
  title = {{{ESES}}: {{Software}} for {{Eulerian}} Solvent Excluded Surface},
  volume = {38},
  issn = {1096-987X},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/jcc.24682},
  doi = {10.1002/jcc.24682},
  shorttitle = {{{ESES}}},
  number = {7},
  journaltitle = {Journal of Computational Chemistry},
  urldate = {2018-06-18},
  date = {2017-01-04},
  pages = {446-466},
  author = {Liu, Beibei and Wang, Bao and Zhao, Rundong and Tong, Yiying and Wei, Guo-Wei},
  file = {/home/dimitri/Nextcloud/Zotero/storage/M3TJKX6T/Liu et al. - 2017 - ESES Software for Eulerian solvent excluded surfa.pdf;/home/dimitri/Nextcloud/Zotero/storage/ULYNDKZZ/jcc.html}
}

@article{petriSimplicialActivityDriven2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1805.06740},
  primaryClass = {physics},
  title = {Simplicial {{Activity Driven Model}}},
  url = {http://arxiv.org/abs/1805.06740},
  abstract = {Many complex systems find a convenient representation in terms of networks: structures made by pairwise interactions of elements. Their evolution is often described by temporal networks, in which links between two nodes are replaced by sequences of events describing how interactions change over time. In particular, the Activity-Driven (AD) model has been widely considered, as the simplicity of its definition allows for analytical insights and various refinements. For many biological and social systems however, elementary interactions involve however more than two elements, and structures such as simplicial complexes are more adequate to describe such phenomena. Here, we propose a Simplicial Activity Driven (SAD) model in which the building block is a simplex of nodes representing a multi-agent interaction, instead of a set of binary interactions. We compare the resulting system with AD models with the same numbers of events. We highlight the resulting structural differences and show analytically and numerically that the simplicial structure leads to crucial differences in the outcome of paradigmatic processes modelling disease propagation or social contagion.},
  urldate = {2018-06-18},
  date = {2018-05-17},
  keywords = {Physics - Physics and Society},
  author = {Petri, Giovanni and Barrat, Alain},
  file = {/home/dimitri/Nextcloud/Zotero/storage/XJANUF3F/Petri and Barrat - 2018 - Simplicial Activity Driven Model.pdf;/home/dimitri/Nextcloud/Zotero/storage/FQ3TYRYA/1805.html}
}

@article{lePersistenceFisherKernel2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1802.03569},
  primaryClass = {cs, math, stat},
  title = {Persistence {{Fisher Kernel}}: {{A Riemannian Manifold Kernel}} for {{Persistence Diagrams}}},
  url = {http://arxiv.org/abs/1802.03569},
  shorttitle = {Persistence {{Fisher Kernel}}},
  abstract = {Algebraic topology methods have recently played an important role for statistical analysis with complicated geometric structured data such as shapes, linked twist maps, and material data. Among them, \textbackslash{}textit\{persistent homology\} is a well-known tool to extract robust topological features, and outputs as \textbackslash{}textit\{persistence diagrams\} (PDs). However, PDs are point multi-sets which can not be used in machine learning algorithms for vector data. To deal with it, an emerged approach is to use kernel methods, and an appropriate geometry for PDs is an important factor to measure the similarity of PDs. A popular geometry for PDs is the \textbackslash{}textit\{Wasserstein metric\}. However, Wasserstein distance is not \textbackslash{}textit\{negative definite\}. Thus, it is limited to build positive definite kernels upon the Wasserstein distance \textbackslash{}textit\{without approximation\}. In this work, we rely upon the alternative \textbackslash{}textit\{Fisher information geometry\} to propose a positive definite kernel for PDs \textbackslash{}textit\{without approximation\}, namely the Persistence Fisher (PF) kernel. Then, we analyze eigensystem of the integral operator induced by the proposed kernel for kernel machines. Based on that, we derive generalization error bounds via covering numbers and Rademacher averages for kernel machines with the PF kernel. Additionally, we show some nice properties such as stability and infinite divisibility for the proposed kernel. Furthermore, we also propose a linear time complexity over the number of points in PDs for an approximation of our proposed kernel with a bounded error. Throughout experiments with many different tasks on various benchmark datasets, we illustrate that the PF kernel compares favorably with other baseline kernels for PDs.},
  urldate = {2018-06-18},
  date = {2018-02-10},
  keywords = {Computer Science - Learning,Statistics - Machine Learning,Mathematics - Algebraic Topology},
  author = {Le, Tam and Yamada, Makoto},
  file = {/home/dimitri/Nextcloud/Zotero/storage/RYBJTQ9F/Le and Yamada - 2018 - Persistence Fisher Kernel A Riemannian Manifold K.pdf;/home/dimitri/Nextcloud/Zotero/storage/4P6MP6RX/1802.html}
}

@inproceedings{carriereSlicedWassersteinKernel2017,
  langid = {english},
  title = {Sliced {{Wasserstein Kernel}} for {{Persistence Diagrams}}},
  url = {http://proceedings.mlr.press/v70/carriere17a.html},
  abstract = {Persistence diagrams (PDs) play a key role in topological data analysis (TDA), in which they are routinely used to describe succinctly complex topological properties of complicated shapes. PDs enjo...},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  booktitle = {International {{Conference}} on {{Machine Learning}}},
  urldate = {2018-06-20},
  date = {2017-07-17},
  pages = {664-673},
  author = {Carrière, Mathieu and Cuturi, Marco and Oudot, Steve},
  file = {/home/dimitri/Nextcloud/Zotero/storage/7FZZJDKP/Carrière et al. - 2017 - Sliced Wasserstein Kernel for Persistence Diagrams.pdf;/home/dimitri/Nextcloud/Zotero/storage/NWMEA95P/Carrière et al. - 2017 - Sliced Wasserstein Kernel for Persistence Diagrams.pdf;/home/dimitri/Nextcloud/Zotero/storage/VDXI2J8D/carriere17a.html}
}

@article{bassettNetworkNeuroscience2017,
  langid = {english},
  title = {Network Neuroscience},
  volume = {20},
  issn = {1546-1726},
  url = {https://www.nature.com/articles/nn.4502},
  doi = {10.1038/nn.4502},
  abstract = {Despite substantial recent progress, our understanding of the principles and mechanisms underlying complex brain function and cognition remains incomplete. Network neuroscience proposes to tackle these enduring challenges. Approaching brain structure and function from an explicitly integrative perspective, network neuroscience pursues new ways to map, record, analyze and model the elements and interactions of neurobiological systems. Two parallel trends drive the approach: the availability of new empirical tools to create comprehensive maps and record dynamic patterns among molecules, neurons, brain areas and social systems; and the theoretical framework and computational tools of modern network science. The convergence of empirical and computational advances opens new frontiers of scientific inquiry, including network dynamics, manipulation and control of brain networks, and integration of network processes across spatiotemporal domains. We review emerging trends in network neuroscience and attempt to chart a path toward a better understanding of the brain as a multiscale networked system.},
  number = {3},
  journaltitle = {Nature Neuroscience},
  urldate = {2018-07-10},
  date = {2017-03},
  pages = {353-364},
  author = {Bassett, Danielle S. and Sporns, Olaf},
  file = {/home/dimitri/Nextcloud/Zotero/storage/8H5EDRXQ/Bassett and Sporns - 2017 - Network neuroscience.pdf;/home/dimitri/Nextcloud/Zotero/storage/E92MLVZA/nn.html}
}

@article{eagleRealityMiningSensing2006,
  langid = {english},
  title = {Reality Mining: Sensing Complex Social Systems},
  volume = {10},
  issn = {1617-4909, 1617-4917},
  url = {https://link.springer.com/article/10.1007/s00779-005-0046-3},
  doi = {10.1007/s00779-005-0046-3},
  shorttitle = {Reality Mining},
  abstract = {We introduce a system for sensing complex social systems with data collected from 100 mobile phones over the course of 9 months. We demonstrate the ability to use standard Bluetooth-enabled mobile telephones to measure information access and use in different contexts, recognize social patterns in daily user activity, infer relationships, identify socially significant locations, and model organizational rhythms.},
  number = {4},
  journaltitle = {Personal and Ubiquitous Computing},
  shortjournal = {Pers Ubiquit Comput},
  urldate = {2018-07-23},
  date = {2006-05-01},
  pages = {255-268},
  author = {Eagle, Nathan and Pentland, Alex (Sandy)},
  file = {/home/dimitri/Nextcloud/Zotero/storage/H9DUQJ6T/Eagle and Pentland - 2006 - Reality mining sensing complex social systems.pdf;/home/dimitri/Nextcloud/Zotero/storage/8DH79ULJ/10.html}
}

@book{adlerPersistentHomologyRandom2010,
  langid = {english},
  title = {Persistent Homology for Random Fields and Complexes},
  isbn = {978-0-940600-79-9},
  url = {https://projecteuclid.org/euclid.imsc/1288099016},
  abstract = {Project Euclid - mathematics and statistics online},
  publisher = {{Institute of Mathematical Statistics}},
  urldate = {2018-07-30},
  date = {2010},
  author = {Adler, Robert J. and Bobrowski, Omer and Borman, Matthew S. and Subag, Eliran and Weinberger, Shmuel},
  file = {/home/dimitri/Nextcloud/Zotero/storage/N8UHEK9G/Adler et al. - 2010 - Persistent homology for random fields and complexe.pdf;/home/dimitri/Nextcloud/Zotero/storage/29PXN97Q/1288099016.html},
  doi = {10.1214/10-IMSCOLL609}
}

@article{holmeTemporalNetworks2012,
  title = {Temporal Networks},
  volume = {519},
  issn = {0370-1573},
  url = {http://www.sciencedirect.com/science/article/pii/S0370157312000841},
  doi = {10.1016/j.physrep.2012.03.001},
  abstract = {A great variety of systems in nature, society and technology–from the web of sexual contacts to the Internet, from the nervous system to power grids–can be modeled as graphs of vertices coupled by edges. The network structure, describing how the graph is wired, helps us understand, predict and optimize the behavior of dynamical systems. In many cases, however, the edges are not continuously active. As an example, in networks of communication via e-mail, text messages, or phone calls, edges represent sequences of instantaneous or practically instantaneous contacts. In some cases, edges are active for non-negligible periods of time: e.g.,~the proximity patterns of inpatients at hospitals can be represented by a graph where an edge between two individuals is on throughout the time they are at the same ward. Like network topology, the temporal structure of edge activations can affect dynamics of systems interacting through the network, from disease contagion on the network of patients to information diffusion over an e-mail network. In this review, we present the emergent field of temporal networks, and discuss methods for analyzing topological and temporal structure and models for elucidating their relation to the behavior of dynamical systems. In the light of traditional network theory, one can see this framework as moving the information of when things happen from the dynamical system on the network, to the network itself. Since fundamental properties, such as the transitivity of edges, do not necessarily hold in temporal networks, many of these methods need to be quite different from those for static networks. The study of temporal networks is very interdisciplinary in nature. Reflecting this, even the object of study has many names—temporal graphs, evolving graphs, time-varying graphs, time-aggregated graphs, time-stamped graphs, dynamic networks, dynamic graphs, dynamical graphs, and so on. This review covers different fields where temporal graphs are considered, but does not attempt to unify related terminology—rather, we want to make papers readable across disciplines.},
  number = {3},
  journaltitle = {Physics Reports},
  shortjournal = {Physics Reports},
  series = {Temporal {{Networks}}},
  urldate = {2018-07-31},
  date = {2012-10-01},
  pages = {97-125},
  author = {Holme, Petter and Saramäki, Jari},
  file = {/home/dimitri/Nextcloud/Zotero/storage/KUU88J97/S0370157312000841.html}
}

@article{holmeModernTemporalNetwork2015,
  langid = {english},
  title = {Modern Temporal Network Theory: A Colloquium},
  volume = {88},
  issn = {1434-6028, 1434-6036},
  url = {https://link.springer.com/article/10.1140/epjb/e2015-60657-4},
  doi = {10.1140/epjb/e2015-60657-4},
  shorttitle = {Modern Temporal Network Theory},
  abstract = {The power of any kind of network approach lies in the ability to simplify a complex system so that one can better understand its function as a whole. Sometimes it is beneficial, however, to include more information than in a simple graph of only nodes and links. Adding information about times of interactions can make predictions and mechanistic understanding more accurate. The drawback, however, is that there are not so many methods available, partly because temporal networks is a relatively young field, partly because it is more difficult to develop such methods compared to for static networks. In this colloquium, we review the methods to analyze and model temporal networks and processes taking place on them, focusing mainly on the last three years. This includes the spreading of infectious disease, opinions, rumors, in social networks; information packets in computer networks; various types of signaling in biology, and more. We also discuss future directions.},
  number = {9},
  journaltitle = {The European Physical Journal B},
  shortjournal = {Eur. Phys. J. B},
  urldate = {2018-07-31},
  date = {2015-09-01},
  pages = {234},
  author = {Holme, Petter},
  file = {/home/dimitri/Nextcloud/Zotero/storage/CYSLT5MA/10.html}
}

@article{cohen-steinerStabilityPersistenceDiagrams2007,
  langid = {english},
  title = {Stability of {{Persistence Diagrams}}},
  volume = {37},
  issn = {0179-5376, 1432-0444},
  url = {https://link.springer.com/article/10.1007/s00454-006-1276-5},
  doi = {10.1007/s00454-006-1276-5},
  abstract = {The persistence diagram of a real-valued function on a topological space is a multiset of points in the extended plane. We prove that under mild assumptions on the function, the persistence diagram is stable: small changes in the function imply only small changes in the diagram. We apply this result to estimating the homology of sets in a metric space and to comparing and classifying geometric shapes.},
  number = {1},
  journaltitle = {Discrete \& Computational Geometry},
  shortjournal = {Discrete Comput Geom},
  urldate = {2018-07-31},
  date = {2007-01-01},
  pages = {103-120},
  author = {Cohen-Steiner, David and Edelsbrunner, Herbert and Harer, John},
  file = {/home/dimitri/Nextcloud/Zotero/storage/4WEUZ4B5/Cohen-Steiner et al. - 2007 - Stability of Persistence Diagrams.pdf;/home/dimitri/Nextcloud/Zotero/storage/5P323WWZ/s00454-006-1276-5.html}
}

@article{chazalPersistenceStabilityGeometric2014,
  langid = {english},
  title = {Persistence Stability for Geometric Complexes},
  volume = {173},
  issn = {0046-5755, 1572-9168},
  url = {https://link.springer.com/article/10.1007/s10711-013-9937-z},
  doi = {10.1007/s10711-013-9937-z},
  abstract = {In this paper we study the properties of the homology of different geometric filtered complexes (such as Vietoris–Rips, Čech and witness complexes) built on top of totally bounded metric spaces. Using recent developments in the theory of topological persistence, we provide simple and natural proofs of the stability of the persistent homology of such complexes with respect to the Gromov–Hausdorff distance. We also exhibit a few noteworthy properties of the homology of the Rips and Čech complexes built on top of compact spaces.},
  number = {1},
  journaltitle = {Geometriae Dedicata},
  shortjournal = {Geom Dedicata},
  urldate = {2018-07-31},
  date = {2014-12-01},
  pages = {193-214},
  author = {Chazal, Frédéric and family=Silva, given=Vin, prefix=de, useprefix=false and Oudot, Steve},
  file = {/home/dimitri/Nextcloud/Zotero/storage/7EESRFL3/s10711-013-9937-z.html}
}

@inproceedings{zomorodianTidySetMinimal2010,
  location = {{New York, NY, USA}},
  title = {The {{Tidy Set}}: {{A Minimal Simplicial Set}} for {{Computing Homology}} of {{Clique Complexes}}},
  isbn = {978-1-4503-0016-2},
  url = {http://doi.acm.org/10.1145/1810959.1811004},
  doi = {10.1145/1810959.1811004},
  shorttitle = {The {{Tidy Set}}},
  abstract = {We introduce the tidy set, a minimal simplicial set that captures the topology of a simplicial complex. The tidy set is particularly effective for computing the homology of clique complexes. This family of complexes include the Vietoris-Rips complex and the weak witness complex, methods that are popular in topological data analysis. The key feature of our approach is that it skips constructing the clique complex. We give algorithms for constructing tidy sets, implement them, and present experiments. Our preliminary results show that tidy sets are orders of magnitude smaller than clique complexes, giving us a homology engine with small memory requirements.},
  booktitle = {Proceedings of the {{Twenty}}-Sixth {{Annual Symposium}} on {{Computational Geometry}}},
  series = {{{SoCG}} '10},
  publisher = {{ACM}},
  urldate = {2018-07-31},
  date = {2010},
  pages = {257--266},
  keywords = {computational topology,simplicial set,vietoris-rips complex,witness complex},
  author = {Zomorodian, Afra}
}

@article{tomitaWorstcaseTimeComplexity2006,
  title = {The Worst-Case Time Complexity for Generating All Maximal Cliques and Computational Experiments},
  volume = {363},
  issn = {0304-3975},
  url = {http://www.sciencedirect.com/science/article/pii/S0304397506003586},
  doi = {10.1016/j.tcs.2006.06.015},
  abstract = {We present a depth-first search algorithm for generating all maximal cliques of an undirected graph, in which pruning methods are employed as in the Bron–Kerbosch algorithm. All the maximal cliques generated are output in a tree-like form. Subsequently, we prove that its worst-case time complexity is O(3n/3) for an n-vertex graph. This is optimal as a function of n, since there exist up to 3n/3 maximal cliques in an n-vertex graph. The algorithm is also demonstrated to run very fast in practice by computational experiments.},
  number = {1},
  journaltitle = {Theoretical Computer Science},
  shortjournal = {Theoretical Computer Science},
  series = {Computing and {{Combinatorics}}},
  urldate = {2018-07-31},
  date = {2006-10-25},
  pages = {28-42},
  keywords = {Computational experiments,Enumeration,Maximal cliques,Worst-case time complexity},
  author = {Tomita, Etsuji and Tanaka, Akira and Takahashi, Haruhisa},
  file = {/home/dimitri/Nextcloud/Zotero/storage/QDLTAXHX/Tomita et al. - 2006 - The worst-case time complexity for generating all .pdf;/home/dimitri/Nextcloud/Zotero/storage/TCJ8J7MV/S0304397506003586.html}
}

@article{isellaWhatCrowdAnalysis2011,
  title = {What's in a Crowd? {{Analysis}} of Face-to-Face Behavioral Networks},
  volume = {271},
  issn = {0022-5193},
  url = {http://www.sciencedirect.com/science/article/pii/S0022519310006284},
  doi = {10.1016/j.jtbi.2010.11.033},
  shorttitle = {What's in a Crowd?},
  abstract = {The availability of new data sources on human mobility is opening new avenues for investigating the interplay of social networks, human mobility and dynamical processes such as epidemic spreading. Here we analyze data on the time-resolved face-to-face proximity of individuals in large-scale real-world scenarios. We compare two settings with very different properties, a scientific conference and a long-running museum exhibition. We track the behavioral networks of face-to-face proximity, and characterize them from both a static and a dynamic point of view, exposing differences and similarities. We use our data to investigate the dynamics of a susceptible–infected model for epidemic spreading that unfolds on the dynamical networks of human proximity. The spreading patterns are markedly different for the conference and the museum case, and they are strongly impacted by the causal structure of the network data. A deeper study of the spreading paths shows that the mere knowledge of static aggregated networks would lead to erroneous conclusions about the transmission paths on the dynamical networks.},
  number = {1},
  journaltitle = {Journal of Theoretical Biology},
  shortjournal = {Journal of Theoretical Biology},
  urldate = {2018-08-08},
  date = {2011-02-21},
  pages = {166-180},
  keywords = {Complex networks,Behavioral social networks,Dynamic networks,Face-to-face proximity,Information spreading},
  author = {Isella, Lorenzo and Stehlé, Juliette and Barrat, Alain and Cattuto, Ciro and Pinton, Jean-François and Van den Broeck, Wouter},
  file = {/home/dimitri/Nextcloud/Zotero/storage/56DMKRM7/Isella et al. - 2011 - What's in a crowd Analysis of face-to-face behavi.pdf;/home/dimitri/Nextcloud/Zotero/storage/J4DJF3P8/S0022519310006284.html}
}

@inproceedings{zeppelzauerTopologicalDescriptors3D2016,
  langid = {english},
  title = {Topological {{Descriptors}} for {{3D Surface Analysis}}},
  isbn = {978-3-319-39440-4 978-3-319-39441-1},
  url = {https://link.springer.com/chapter/10.1007/978-3-319-39441-1_8},
  doi = {10.1007/978-3-319-39441-1_8},
  abstract = {We investigate topological descriptors for 3D surface analysis, i.e. the classification of surfaces according to their geometric fine structure. On a dataset of high-resolution 3D surface reconstructions we compute persistence diagrams for a 2D cubical filtration. In the next step we investigate different topological descriptors and measure their ability to discriminate structurally different 3D surface patches. We evaluate their sensitivity to different parameters and compare the performance of the resulting topological descriptors to alternative (non-topological) descriptors. We present a comprehensive evaluation that shows that topological descriptors are (i) robust, (ii) yield state-of-the-art performance for the task of 3D surface analysis and (iii) improve classification performance when combined with non-topological descriptors.},
  eventtitle = {International {{Workshop}} on {{Computational Topology}} in {{Image Context}}},
  booktitle = {Computational {{Topology}} in {{Image Context}}},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  publisher = {{Springer, Cham}},
  urldate = {2018-08-16},
  date = {2016-06-15},
  pages = {77-87},
  author = {Zeppelzauer, Matthias and Zieliński, Bartosz and Juda, Mateusz and Seidl, Markus},
  file = {/home/dimitri/Nextcloud/Zotero/storage/JH8QTE5R/978-3-319-39441-1_8.html}
}

@inproceedings{suloMeaningfulSelectionTemporal2010,
  location = {{New York, NY, USA}},
  title = {Meaningful {{Selection}} of {{Temporal Resolution}} for {{Dynamic Networks}}},
  isbn = {978-1-4503-0214-2},
  url = {http://doi.acm.org/10.1145/1830252.1830269},
  doi = {10.1145/1830252.1830269},
  abstract = {The understanding of dynamics of data streams is greatly affected by the choice of temporal resolution at which the data are discretized, aggregated, and analyzed. Our paper focuses explicitly on data streams represented as dynamic networks. We propose a framework for identifying meaningful resolution levels that best reveal critical changes in the network structure, by balancing the reduction of noise with the loss of information. We demonstrate the applicability of our approach by analyzing various network statistics of both synthetic and real dynamic networks and using those to detect important events and changes in dynamic network structure.},
  booktitle = {Proceedings of the {{Eighth Workshop}} on {{Mining}} and {{Learning}} with {{Graphs}}},
  series = {{{MLG}} '10},
  publisher = {{ACM}},
  urldate = {2018-08-22},
  date = {2010},
  pages = {127--136},
  author = {Sulo, Rajmonda and Berger-Wolf, Tanya and Grossman, Robert}
}

@article{kringsEffectsTimeWindow2012,
  langid = {english},
  title = {Effects of Time Window Size and Placement on the Structure of an Aggregated Communication Network},
  volume = {1},
  issn = {2193-1127},
  url = {https://epjdatascience.springeropen.com/articles/10.1140/epjds4},
  doi = {10.1140/epjds4},
  abstract = {Complex networks are often constructed by aggregating empirical data over time, such that a link represents the existence of interactions between the endpoint nodes and the link weight represents the intensity of such interactions within the aggregation time window. The resulting networks are then often considered static. More often than not, the aggregation time window is dictated by the availability of data, and the effects of its length on the resulting networks are rarely considered. Here, we address this question by studying the structural features of networks emerging from aggregating empirical data over different time intervals, focussing on networks derived from time-stamped, anonymized mobile telephone call records. Our results show that short aggregation intervals yield networks where strong links associated with dense clusters dominate; the seeds of such clusters or communities become already visible for intervals of around one week. The degree and weight distributions are seen to become stationary around a few days and a few weeks, respectively. An aggregation interval of around 30 days results in the stablest similar networks when consecutive windows are compared. For longer intervals, the effects of weak or random links become increasingly stronger, and the average degree of the network keeps growing even for intervals up to 180 days. The placement of the time window is also seen to affect the outcome: for short windows, different behavioural patterns play a role during weekends and weekdays, and for longer windows it is seen that networks aggregated during holiday periods are significantly different.},
  number = {1},
  journaltitle = {EPJ Data Science},
  urldate = {2018-08-22},
  date = {2012-12},
  pages = {4},
  author = {Krings, Gautier and Karsai, Márton and Bernhardsson, Sebastian and Blondel, Vincent D. and Saramäki, Jari},
  file = {/home/dimitri/Nextcloud/Zotero/storage/3Y8AHZXA/Krings et al. - 2012 - Effects of time window size and placement on the s.pdf;/home/dimitri/Nextcloud/Zotero/storage/AQLGJGDL/epjds4.html}
}

@article{ribeiroQuantifyingEffectTemporal2013,
  langid = {english},
  title = {Quantifying the Effect of Temporal Resolution on Time-Varying Networks},
  volume = {3},
  issn = {2045-2322},
  url = {https://www.nature.com/articles/srep03006},
  doi = {10.1038/srep03006},
  abstract = {Time-varying networks describe a wide array of systems whose constituents and interactions evolve over time. They are defined by an ordered stream of interactions between nodes, yet they are often represented in terms of a sequence of static networks, each aggregating all edges and nodes present in a time interval of size Δt. In this work we quantify the impact of an arbitrary Δt on the description of a dynamical process taking place upon a time-varying network. We focus on the elementary random walk, and put forth a simple mathematical framework that well describes the behavior observed on real datasets. The analytical description of the bias introduced by time integrating techniques represents a step forward in the correct characterization of dynamical processes on time-varying graphs.},
  journaltitle = {Scientific Reports},
  urldate = {2018-08-22},
  date = {2013-10-21},
  pages = {3006},
  author = {Ribeiro, Bruno and Perra, Nicola and Baronchelli, Andrea},
  file = {/home/dimitri/Nextcloud/Zotero/storage/9WPT9TVJ/Ribeiro et al. - 2013 - Quantifying the effect of temporal resolution on t.pdf;/home/dimitri/Nextcloud/Zotero/storage/5IKE4WIN/srep03006.html}
}

@article{muandetKernelMeanEmbedding2017,
  langid = {english},
  title = {Kernel {{Mean Embedding}} of {{Distributions}}: {{A Review}} and {{Beyond}}},
  volume = {10},
  issn = {1935-8237, 1935-8245},
  url = {https://www.nowpublishers.com/article/Details/MAL-060},
  doi = {10.1561/2200000060},
  shorttitle = {Kernel {{Mean Embedding}} of {{Distributions}}},
  abstract = {Kernel Mean Embedding of Distributions: A Review and Beyond},
  number = {1-2},
  journaltitle = {Foundations and Trends® in Machine Learning},
  shortjournal = {MAL},
  urldate = {2018-08-30},
  date = {2017-06-28},
  pages = {1-141},
  author = {Muandet, Krikamol and Fukumizu, Kenji and Sriperumbudur, Bharath and Schölkopf, Bernhard},
  file = {/home/dimitri/Nextcloud/Zotero/storage/3A6PW3II/Muandet et al. - 2017 - Kernel Mean Embedding of Distributions A Review a.pdf;/home/dimitri/Nextcloud/Zotero/storage/7DS27M8D/MAL-060.html}
}

@book{berlinetReproducingKernelHilbert2011,
  langid = {english},
  title = {Reproducing {{Kernel Hilbert Spaces}} in {{Probability}} and {{Statistics}}},
  isbn = {978-1-4419-9096-9},
  abstract = {The reproducing kernel Hilbert space construction is a bijection or transform theory which associates a positive definite kernel (gaussian processes) with a Hilbert space offunctions. Like all transform theories (think Fourier), problems in one space may become transparent in the other, and optimal solutions in one space are often usefully optimal in the other. The theory was born in complex function theory, abstracted and then accidently injected into Statistics; Manny Parzen as a graduate student at Berkeley was given a strip of paper containing his qualifying exam problem- It read "reproducing kernel Hilbert space"- In the 1950's this was a truly obscure topic. Parzen tracked it down and internalized the subject. Soon after, he applied it to problems with the following fla vor: consider estimating the mean functions of a gaussian process. The mean functions which cannot be distinguished with probability one are precisely the functions in the Hilbert space associated to the covariance kernel of the processes. Parzen's own lively account of his work on re producing kernels is charmingly told in his interview with H. Joseph Newton in Statistical Science, 17, 2002, p. 364-366. Parzen moved to Stanford and his infectious enthusiasm caught Jerry Sacks, Don Ylvisaker and Grace Wahba among others. Sacks and Ylvis aker applied the ideas to design problems such as the following. Sup pose (XdO},
  pagetotal = {369},
  publisher = {{Springer Science \& Business Media}},
  date = {2011-06-28},
  keywords = {Business & Economics / Economics / General,Business & Economics / Economics / Theory,Business & Economics / General,Business & Economics / Statistics,Mathematics / Probability & Statistics / General},
  author = {Berlinet, Alain and Thomas-Agnan, Christine},
  file = {/home/dimitri/Nextcloud/Zotero/storage/Q9ZC6RQ9/Berlinet and Thomas-Agnan - 2011 - Reproducing Kernel Hilbert Spaces in Probability a.pdf},
  eprinttype = {googlebooks}
}

@article{pedregosaScikitlearnMachineLearning2011,
  title = {Scikit-Learn: {{Machine Learning}} in {{Python}}},
  volume = {12},
  issn = {1533-7928},
  url = {http://jmlr.csail.mit.edu/papers/v12/pedregosa11a.html},
  shorttitle = {Scikit-Learn},
  abstract = {Scikit-learn is a Python module integrating a wide range of state-of-the-art machine learning algorithms for medium-scale supervised and unsupervised problems. This package focuses on bringing machine learning to non-specialists using a general-purpose high-level language.  Emphasis is put on ease of use, performance, documentation, and API consistency.  It has minimal dependencies and is distributed under the simplified BSD license, encouraging its use in both academic and commercial settings.  Source code, binaries, and documentation can be downloaded from http://scikit-learn.sourceforge.net.},
  journaltitle = {Journal of Machine Learning Research},
  urldate = {2018-09-02},
  date = {2011-10},
  pages = {2825−2830},
  author = {Pedregosa, Fabian and Varoquaux, Gaël and Gramfort, Alexandre and Michel, Vincent and Thirion, Bertrand and Grisel, Olivier and Blondel, Mathieu and Prettenhofer, Peter and Weiss, Ron and Dubourg, Vincent and Vanderplas, Jake and Passos, Alexandre and Cournapeau, David and Brucher, Matthieu and Perrot, Matthieu and Duchesnay, Édouard},
  file = {/home/dimitri/Nextcloud/Zotero/storage/6SAE9PPD/Pedregosa et al. - 2011 - Scikit-learn Machine Learning in Python.pdf}
}

@thesis{price-wrightTopologicalApproachTemporal2015,
  title = {A {{Topological Approach}} to {{Temporal Networks}}},
  abstract = {“Temporal networks” are a mathematical tool to represent systems that change
over time. Research on temporal networks is very active, and limited theoreti-
cal work has been done to study them. One approach to is to construct a series
of static subgraphs called snapshots. Existing techniques attempt to find the
temporal structure of a network to inform its partitioning into snapshots. An
important goal of such methods is to uncover meaningful temporal structure
that corresponds to actual features of the underlying system.
We investigate existing methods used to partition temporal networks based
on di↵erent ways of identifying temporal structure. Such methods have never
previously been compared directly to each other, so we examine and evaluate
their performance side-by-side on a suite of random-graph ensembles. We
show that without prior knowledge about a network’s temporal structure,
these existing methods have limitations producing meaningful partitions.
To tackle the problem of finding temporal structure in a network, we ap-
ply methods from computational topology. Such methods have begun to be
employed in the study of static networks and provide a summary of global
features in data sets. We use them here to track the topology of a network
over time and distinguish important temporal features from trivial ones. We
define two types of topological spaces derived from temporal networks and use
persistent homology to generate a temporal profile for a network. We then
present di↵erent ways to use this to understand a network’s temporal struc-
ture with limited prior knowledge. We show that the methods we apply from
computational topology can distinguish temporal distributions and provide a
high-level summary of temporal structure. These combined can be used to
inform a meaningful network partitioning and a deeper understanding of a
temporal network itself.},
  institution = {{University of Oxford}},
  type = {MSc dissertation in Mathematics and Foundations of Computer Science},
  date = {2015},
  author = {Price-Wright, Erin},
  file = {/home/dimitri/Nextcloud/Zotero/storage/6YI5RC6K/Price-Wrigt - 2015 - A Topological Approach to Temporal Networks.pdf}
}

@misc{sejdinovicAdvancedTopicsStatistical2018,
  title = {Advanced {{Topics}} in {{Statistical Machine Learning}}},
  url = {http://www.stats.ox.ac.uk/%7Esejdinov/atsml/},
  date = {2018-02-10},
  author = {Sejdinovic, Dino},
  file = {/home/dimitri/Nextcloud/Zotero/storage/DPUD6H8Z/Sejdinovic - 2018 - Advanced Topics in Statistical Machine Learning.pdf}
}

@inproceedings{edelsbrunnerTopologicalPersistenceSimplification2000,
  title = {Topological Persistence and Simplification},
  doi = {10.1109/SFCS.2000.892133},
  abstract = {We formalize a notion of topological simplification within the framework of a filtration, which is the history of a growing complex. We classify a topological change that happens during growth as either a feature or noise, depending on its life-time or persistence within the filtration. We give fast algorithms for completing persistence and experimental evidence for their speed and utility.},
  eventtitle = {Proceedings 41st {{Annual Symposium}} on {{Foundations}} of {{Computer Science}}},
  booktitle = {Proceedings 41st {{Annual Symposium}} on {{Foundations}} of {{Computer Science}}},
  date = {2000-11},
  pages = {454-463},
  keywords = {Topology,History,computational topology,algorithm theory,alpha shapes,computational geometry,Computational geometry,Computer graphics,Computer science,Density functional theory,fast algorithms,filtration,Filtration,growing complex,homology groups,Mathematics,Noise shaping,Shape,topological change,topological persistence,topological simplification,topology},
  author = {Edelsbrunner, H. and Letscher, D. and Zomorodian, A.},
  file = {/home/dimitri/Nextcloud/Zotero/storage/5LPIWG5Z/892133.html}
}

@article{morozovPersistenceAlgorithmTakes2005,
  title = {Persistence Algorithm Takes Cubic Time in Worst Case},
  abstract = {Given a sequence of N simplices, we consider the sequence of sets Ki consisting of the first i simplices, for 1 ≤ i ≤ N. We call the sequence of Ki a filtration if all the Ki are simplicial complexes. In this note, we describe a filtration of a simplicial complex of N simplices on which the algorithm Pair-Simplices of Edelsbrunner, Letscher and Zomorodian [1] performs Ω(N 3) operations. The existence of this filtration should be contrasted to the experimentally observed only slightly super-linear running time for filtrations that arise from applications. We describe the space as well as the ordering on the simplices. Let n = ⌊(N + 29)/7⌋, v = ⌊(n − 1)/2⌋, and note that both n and v are in Ω(N). In our filtration, all vertices appear before all edges in the filtration, and all edges appear before all triangles. The indices that we assign to the simplices will be within their respective classes (e.g., edge labeled n will appear before the triangle labeled 1). Some edges will receive a negative index, which is done for simplicity to indicate that they appear before the edges with positive labels (see Figure 2). Figure 1 illustrates the construction of our space as well as the assignment of indices to the simplices. Starting with triangle ABC, we add v vertices inside the triangle in the following manner: we place the first vertex V1 near the middle of edge AB, the second vertex V2 near the middle of},
  journaltitle = {BioGeometry News, Dept. Comput. Sci., Duke Univ},
  date = {2005},
  author = {Morozov, Dmitriy},
  file = {/home/dimitri/Nextcloud/Zotero/storage/I6HKHZQ5/Morozov - 2005 - Persistence algorithm takes cubic time in worst ca.pdf;/home/dimitri/Nextcloud/Zotero/storage/WN8ZVZH9/summary.html}
}

@article{desilvaPersistentCohomologyCircular2011,
  langid = {english},
  title = {Persistent {{Cohomology}} and {{Circular Coordinates}}},
  volume = {45},
  issn = {1432-0444},
  url = {https://doi.org/10.1007/s00454-011-9344-x},
  doi = {10.1007/s00454-011-9344-x},
  abstract = {Nonlinear dimensionality reduction (NLDR) algorithms such as Isomap, LLE, and Laplacian Eigenmaps address the problem of representing high-dimensional nonlinear data in terms of low-dimensional coordinates which represent the intrinsic structure of the data. This paradigm incorporates the assumption that real-valued coordinates provide a rich enough class of functions to represent the data faithfully and efficiently. On the other hand, there are simple structures which challenge this assumption: the circle, for example, is one-dimensional, but its faithful representation requires two real coordinates. In this work, we present a strategy for constructing circle-valued functions on a statistical data set. We develop a machinery of persistent cohomology to identify candidates for significant circle-structures in the data, and we use harmonic smoothing and integration to obtain the circle-valued coordinate functions themselves. We suggest that this enriched class of coordinate functions permits a precise NLDR analysis of a broader range of realistic data sets.},
  number = {4},
  journaltitle = {Discrete \& Computational Geometry},
  shortjournal = {Discrete Comput Geom},
  urldate = {2018-09-05},
  date = {2011-06-01},
  pages = {737-759},
  keywords = {Persistent homology,Computational topology,Dimensionality reduction,Persistent cohomology},
  author = {family=Silva, given=Vin, prefix=de, useprefix=true and Morozov, Dmitriy and Vejdemo-Johansson, Mikael},
  file = {/home/dimitri/Nextcloud/Zotero/storage/EX9L3F7F/de Silva et al. - 2011 - Persistent Cohomology and Circular Coordinates.pdf}
}

@article{desilvaDualitiesPersistentCo2011,
  title = {Dualities in Persistent (Co)Homology},
  volume = {27},
  issn = {0266-5611, 1361-6420},
  url = {http://stacks.iop.org/0266-5611/27/i=12/a=124003?key=crossref.1f4b24ef80c9b1fc789ecdc6221097de},
  doi = {10.1088/0266-5611/27/12/124003},
  number = {12},
  journaltitle = {Inverse Problems},
  urldate = {2018-09-05},
  date = {2011-12-01},
  pages = {124003},
  author = {family=Silva, given=Vin, prefix=de, useprefix=true and Morozov, Dmitriy and Vejdemo-Johansson, Mikael}
}

@incollection{bauerDistributedComputationPersistent2014,
  langid = {english},
  location = {{Philadelphia, PA}},
  title = {Distributed {{Computation}} of {{Persistent Homology}}},
  isbn = {978-1-61197-319-8},
  url = {http://epubs.siam.org/doi/abs/10.1137/1.9781611973198.4},
  booktitle = {2014 {{Proceedings}} of the {{Sixteenth Workshop}} on {{Algorithm Engineering}} and {{Experiments}} ({{ALENEX}})},
  publisher = {{Society for Industrial and Applied Mathematics}},
  urldate = {2018-09-05},
  date = {2014-05},
  pages = {31-38},
  author = {Bauer, Ulrich and Kerber, Michael and Reininghaus, Jan},
  editor = {McGeoch, Catherine C. and Meyer, Ulrich},
  doi = {10.1137/1.9781611973198.4}
}

@misc{golseMAT321AnalyseReelle2015,
  title = {{{MAT321 Analyse}} Réelle},
  publisher = {{École polytechnique}},
  date = {2015},
  author = {Golse, François and Laszlo, Yves and Pacard, Frank and Viterbo, Claude}
}

@book{foussAlgorithmsModelsNetwork2016,
  langid = {english},
  title = {Algorithms and {{Models}} for {{Network Data}} and {{Link Analysis}}},
  isbn = {978-1-107-12577-3},
  abstract = {Network data are produced automatically by everyday interactions - social networks, power grids, and links between data sets are a few examples. Such data capture social and economic behavior in a form that can be analyzed using powerful computational tools. This book is a guide to both basic and advanced techniques and algorithms for extracting useful information from network data. The content is organized around 'tasks', grouping the algorithms needed to gather specific types of information and thus answer specific types of questions. Examples include similarity between nodes in a network, prestige or centrality of individual nodes, and dense regions or communities in a network. Algorithms are derived in detail and summarized in pseudo-code. The book is intended primarily for computer scientists, engineers, statisticians and physicists, but it is also accessible to network scientists based in the social sciences. MATLAB®/Octave code illustrating some of the algorithms will be available at: http://www.cambridge.org/9781107125773.},
  pagetotal = {549},
  publisher = {{Cambridge University Press}},
  date = {2016-07-12},
  keywords = {Computers / Computer Science,Computers / Databases / Data Mining,Computers / Databases / General},
  author = {Fouss, François and Saerens, Marco and Shimbo, Masashi},
  file = {/home/dimitri/Nextcloud/Zotero/storage/DULGH6PQ/Fouss et al. - 2016 - Algorithms and Models for Network Data and Link An.pdf},
  eprinttype = {googlebooks}
}

@article{cattutoDynamicsPersontoPersonInteractions2010,
  langid = {english},
  title = {Dynamics of {{Person}}-to-{{Person Interactions}} from {{Distributed RFID Sensor Networks}}},
  volume = {5},
  issn = {1932-6203},
  url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0011596},
  doi = {10.1371/journal.pone.0011596},
  abstract = {Background Digital networks, mobile devices, and the possibility of mining the ever-increasing amount of digital traces that we leave behind in our daily activities are changing the way we can approach the study of human and social interactions. Large-scale datasets, however, are mostly available for collective and statistical behaviors, at coarse granularities, while high-resolution data on person-to-person interactions are generally limited to relatively small groups of individuals. Here we present a scalable experimental framework for gathering real-time data resolving face-to-face social interactions with tunable spatial and temporal granularities. Methods and Findings We use active Radio Frequency Identification (RFID) devices that assess mutual proximity in a distributed fashion by exchanging low-power radio packets. We analyze the dynamics of person-to-person interaction networks obtained in three high-resolution experiments carried out at different orders of magnitude in community size. The data sets exhibit common statistical properties and lack of a characteristic time scale from 20 seconds to several hours. The association between the number of connections and their duration shows an interesting super-linear behavior, which indicates the possibility of defining super-connectors both in the number and intensity of connections. Conclusions Taking advantage of scalability and resolution, this experimental framework allows the monitoring of social interactions, uncovering similarities in the way individuals interact in different contexts, and identifying patterns of super-connector behavior in the community. These results could impact our understanding of all phenomena driven by face-to-face interactions, such as the spreading of transmissible infectious diseases and information.},
  number = {7},
  journaltitle = {PLOS ONE},
  shortjournal = {PLOS ONE},
  urldate = {2018-09-07},
  date = {2010-07-15},
  pages = {e11596},
  keywords = {Computer networks,Behavior,Behavioral geography,Human mobility,Probability distribution,Radio waves,Statistical data,Statistical distributions},
  author = {Cattuto, Ciro and family=Broeck, given=Wouter Van, prefix=den, useprefix=false and Barrat, Alain and Colizza, Vittoria and Pinton, Jean-François and Vespignani, Alessandro},
  file = {/home/dimitri/Nextcloud/Zotero/storage/GFAHQ6F2/Cattuto et al. - 2010 - Dynamics of Person-to-Person Interactions from Dis.pdf;/home/dimitri/Nextcloud/Zotero/storage/67R2UX2N/article.html}
}

@online{InfectiousSocioPatterns2011,
  langid = {american},
  title = {Infectious {{SocioPatterns}}},
  url = {http://www.sociopatterns.org/datasets/infectious-sociopatterns/},
  abstract = {A research project that aims to uncover fundamental patterns in social dynamics and coordinated human activity through a data-driven approach.},
  journaltitle = {SocioPatterns.org},
  date = {2011-03-31},
  file = {/home/dimitri/Nextcloud/Zotero/storage/VNBHGW9K/infectious-sociopatterns.html}
}

@online{InfectiousSocioPatternsDynamic2011,
  langid = {american},
  title = {Infectious {{SocioPatterns}} Dynamic Contact Networks},
  url = {http://www.sociopatterns.org/datasets/infectious-sociopatterns-dynamic-contact-networks/},
  abstract = {A research project that aims to uncover fundamental patterns in social dynamics and coordinated human activity through a data-driven approach.},
  journaltitle = {SocioPatterns.org},
  date = {2011-11-28},
  file = {/home/dimitri/Nextcloud/Zotero/storage/9YMG2VGK/infectious-sociopatterns-dynamic-contact-networks.html}
}

@article{carlssonZigzagPersistence2008,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {0812.0197},
  primaryClass = {cs},
  title = {Zigzag {{Persistence}}},
  url = {http://arxiv.org/abs/0812.0197},
  abstract = {We describe a new methodology for studying persistence of topological features across a family of spaces or point-cloud data sets, called zigzag persistence. Building on classical results about quiver representations, zigzag persistence generalises the highly successful theory of persistent homology and addresses several situations which are not covered by that theory. In this paper we develop theoretical and algorithmic foundations with a view towards applications in topological statistics.},
  urldate = {2018-09-08},
  date = {2008-11-30},
  keywords = {Computer Science - Computational Geometry,I.3.5},
  author = {Carlsson, Gunnar and family=Silva, given=Vin, prefix=de, useprefix=true},
  file = {/home/dimitri/Nextcloud/Zotero/storage/PKSM89FF/Carlsson and de Silva - 2008 - Zigzag Persistence.pdf;/home/dimitri/Nextcloud/Zotero/storage/QF37EI5F/0812.html}
}

@article{mariaComputingZigzagPersistent2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1608.06039},
  primaryClass = {cs},
  title = {Computing {{Zigzag Persistent Cohomology}}},
  url = {http://arxiv.org/abs/1608.06039},
  abstract = {Zigzag persistent homology is a powerful generalisation of persistent homology that allows one not only to compute persistence diagrams with less noise and using less memory, but also to use persistence in new fields of application. However, due to the increase in complexity of the algebraic treatment of the theory, most algorithmic results in the field have remained of theoretical nature. This article describes an efficient algorithm to compute zigzag persistence, emphasising on its practical interest. The algorithm is a zigzag persistent cohomology algorithm, based on the dualisation of reflections and transpositions transformations within the zigzag sequence. We provide an extensive experimental study of the algorithm. We study the algorithm along two directions. First, we compare its performance with zigzag persistent homology algorithm and show the interest of cohomology in zigzag persistence. Second, we illustrate the interest of zigzag persistence in topological data analysis by comparing it to state of the art methods in the field, specifically optimised algorithm for standard persistent homology and sparse filtrations. We compare the memory and time complexities of the different algorithms, as well as the quality of the output persistence diagrams.},
  urldate = {2018-09-08},
  date = {2016-08-21},
  keywords = {Computer Science - Computational Geometry},
  author = {Maria, Clément and Oudot, Steve},
  file = {/home/dimitri/Nextcloud/Zotero/storage/LJBHWTMY/Maria and Oudot - 2016 - Computing Zigzag Persistent Cohomology.pdf;/home/dimitri/Nextcloud/Zotero/storage/TCKJGZET/1608.html}
}

@article{holmeAttackVulnerabilityComplex2002,
  title = {Attack Vulnerability of Complex Networks},
  volume = {65},
  url = {https://link.aps.org/doi/10.1103/PhysRevE.65.056109},
  doi = {10.1103/PhysRevE.65.056109},
  abstract = {We study the response of complex networks subject to attacks on vertices and edges. Several existing complex network models as well as real-world networks of scientific collaborations and Internet traffic are numerically investigated, and the network performance is quantitatively measured by the average inverse geodesic length and the size of the largest connected subgraph. For each case of attacks on vertices and edges, four different attacking strategies are used: removals by the descending order of the degree and the betweenness centrality, calculated for either the initial network or the current network during the removal procedure. It is found that the removals by the recalculated degrees and betweenness centralities are often more harmful than the attack strategies based on the initial network, suggesting that the network structure changes as important vertices or edges are removed. Furthermore, the correlation between the betweenness centrality and the degree in complex networks is studied.},
  number = {5},
  journaltitle = {Physical Review E},
  shortjournal = {Phys. Rev. E},
  urldate = {2018-09-09},
  date = {2002-05-07},
  pages = {056109},
  author = {Holme, Petter and Kim, Beom Jun and Yoon, Chang No and Han, Seung Kee},
  file = {/home/dimitri/Nextcloud/Zotero/storage/PW8XHWT3/PhysRevE.65.html}
}

@article{aledavoodDigitalDailyCycles2015,
  langid = {english},
  title = {Digital Daily Cycles of Individuals},
  volume = {3},
  issn = {2296-424X},
  url = {https://www.frontiersin.org/articles/10.3389/fphy.2015.00073/full},
  doi = {10.3389/fphy.2015.00073},
  abstract = {Humans, like almost all animals, are phase-locked to the diurnal cycle. Most of us sleep at night and are active through the day. Because we have evolved to function with this cycle, the circadian rhythm is deeply ingrained and even detectable at the biochemical level. However, within the broader day-night pattern, there are individual differences: e.g., some of us are intrinsically morning-active, while others prefer evenings. In this article, we look at digital daily cycles: circadian patterns of activity viewed through the lens of auto-recorded data of communication and online activity. We begin at the aggregate level, discuss earlier results, and illustrate differences between population-level daily rhythms in different media. Then we move on to the individual level, and show that there is a strong individual-level variation beyond averages: individuals typically have their distinctive daily pattern that persists in time. We conclude by discussing the driving forces behind these signature daily patterns, from personal traits (morningness/eveningness) to variation in activity level and external constraints, and outline possibilities for future research.},
  journaltitle = {Frontiers in Physics},
  shortjournal = {Front. Phys.},
  urldate = {2018-09-09},
  date = {2015},
  keywords = {circadian rhythms,Digital phenotyping,electronic communication records,individual differences,Mobile phones},
  author = {Aledavood, Talayeh and Lehmann, Sune and Saramäki, Jari},
  file = {/home/dimitri/Nextcloud/Zotero/storage/TZP4KMJ4/Aledavood et al. - 2015 - Digital daily cycles of individuals.pdf}
}

@article{aledavoodDailyRhythmsMobile2015,
  langid = {english},
  title = {Daily {{Rhythms}} in {{Mobile Telephone Communication}}},
  volume = {10},
  issn = {1932-6203},
  url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0138098},
  doi = {10.1371/journal.pone.0138098},
  abstract = {Circadian rhythms are known to be important drivers of human activity and the recent availability of electronic records of human behaviour has provided fine-grained data of temporal patterns of activity on a large scale. Further, questionnaire studies have identified important individual differences in circadian rhythms, with people broadly categorised into morning-like or evening-like individuals. However, little is known about the social aspects of these circadian rhythms, or how they vary across individuals. In this study we use a unique 18-month dataset that combines mobile phone calls and questionnaire data to examine individual differences in the daily rhythms of mobile phone activity. We demonstrate clear individual differences in daily patterns of phone calls, and show that these individual differences are persistent despite a high degree of turnover in the individuals’ social networks. Further, women’s calls were longer than men’s calls, especially during the evening and at night, and these calls were typically focused on a small number of emotionally intense relationships. These results demonstrate that individual differences in circadian rhythms are not just related to broad patterns of morningness and eveningness, but have a strong social component, in directing phone calls to specific individuals at specific times of day.},
  number = {9},
  journaltitle = {PLOS ONE},
  shortjournal = {PLOS ONE},
  urldate = {2018-09-09},
  date = {2015-09-21},
  pages = {e0138098},
  keywords = {Behavior,Cell phones,Circadian rhythms,Emotions,Entropy,Interpersonal relationships,Questionnaires,Social networks},
  author = {Aledavood, Talayeh and López, Eduardo and Roberts, Sam G. B. and Reed-Tsochas, Felix and Moro, Esteban and Dunbar, Robin I. M. and Saramäki, Jari},
  file = {/home/dimitri/Nextcloud/Zotero/storage/FFG9S8PK/Aledavood et al. - 2015 - Daily Rhythms in Mobile Telephone Communication.pdf;/home/dimitri/Nextcloud/Zotero/storage/MI9VN585/article.html}
}

@article{holmeNetworkDynamicsOngoing2003,
  langid = {english},
  title = {Network Dynamics of Ongoing Social Relationships},
  volume = {64},
  issn = {0295-5075},
  url = {http://iopscience.iop.org/article/10.1209/epl/i2003-00505-4/meta},
  doi = {10.1209/epl/i2003-00505-4},
  number = {3},
  journaltitle = {EPL (Europhysics Letters)},
  shortjournal = {EPL},
  urldate = {2018-09-09},
  date = {2003-11},
  pages = {427},
  author = {Holme, P.},
  file = {/home/dimitri/Nextcloud/Zotero/storage/5IXF7A2B/meta.html}
}

@article{joCircadianPatternBurstiness2012,
  title = {Circadian Pattern and Burstiness in Mobile Phone Communication},
  volume = {14},
  issn = {1367-2630},
  url = {http://stacks.iop.org/1367-2630/14/i=1/a=013055?key=crossref.49fc43f1e121d47657c8da6f05484442},
  doi = {10.1088/1367-2630/14/1/013055},
  number = {1},
  journaltitle = {New Journal of Physics},
  urldate = {2018-09-09},
  date = {2012-01-25},
  pages = {013055},
  author = {Jo, Hang-Hyun and Karsai, Márton and Kertész, János and Kaski, Kimmo}
}

@article{hermanGraphVisualizationNavigation2000,
  title = {Graph Visualization and Navigation in Information Visualization: {{A}} Survey},
  volume = {6},
  issn = {1077-2626},
  doi = {10.1109/2945.841119},
  shorttitle = {Graph Visualization and Navigation in Information Visualization},
  abstract = {This is a survey on graph visualization and navigation techniques, as used in information visualization. Graphs appear in numerous applications such as Web browsing, state-transition diagrams, and data structures. The ability to visualize and to navigate in these potentially large, abstract graphs is often a crucial part of an application. Information visualization has specific requirements, which means that this survey approaches the results of traditional graph drawing from a different perspective.},
  number = {1},
  journaltitle = {IEEE Transactions on Visualization and Computer Graphics},
  date = {2000-01},
  pages = {24-43},
  keywords = {abstract graphs,Application software,Books,Computer Society,data structures,Data structures,data visualisation,Data visualization,graph drawing,graph navigation,graph visualization,graphs,information visualization,Navigation,Project management,state-transition diagrams,Taxonomy,Tree graphs,Usability,Web browsing},
  author = {Herman, I. and Melancon, G. and Marshall, M. S.},
  file = {/home/dimitri/Nextcloud/Zotero/storage/R8IZUEKX/Herman et al. - 2000 - Graph visualization and navigation in information .pdf;/home/dimitri/Nextcloud/Zotero/storage/7IH4D3K6/841119.html}
}

@collection{kaufmannDrawingGraphsMethods2001,
  location = {{Berlin ; New York}},
  title = {Drawing Graphs: Methods and Models},
  isbn = {978-3-540-42062-0},
  shorttitle = {Drawing Graphs},
  pagetotal = {312},
  number = {2025},
  series = {Lecture Notes in Computer Science},
  publisher = {{Springer}},
  date = {2001},
  keywords = {Graph theory,Computer graphics},
  editor = {Kaufmann, Michael and Wagner, Dorothea},
  file = {/home/dimitri/Nextcloud/Zotero/storage/FTSL4DRC/Kaufmann and Wagner - 2001 - Drawing graphs methods and models.pdf}
}

@collection{dibattistaGraphDrawingAlgorithms1999,
  location = {{Upper Saddle River, N.J}},
  title = {Graph Drawing: Algorithms for the Visualization of Graphs},
  isbn = {978-0-13-301615-4},
  shorttitle = {Graph Drawing},
  pagetotal = {397},
  publisher = {{Prentice Hall}},
  date = {1999},
  keywords = {Graph theory,Computer graphics},
  editor = {Di Battista, Giuseppe},
  file = {/home/dimitri/Nextcloud/Zotero/storage/ZF46TB6I/Di Battista - 1999 - Graph drawing algorithms for the visualization of.pdf}
}

@collection{tamassiaHandbookGraphDrawing2014,
  location = {{Boca Raton}},
  title = {Handbook of Graph Drawing and Visualization},
  isbn = {978-1-58488-412-5},
  pagetotal = {851},
  series = {Discrete Mathematics and Its Applications},
  publisher = {{CRC Press, Taylor \& Francis Group}},
  date = {2014},
  keywords = {Graph theory,Graphic methods,Handbooks; manuals; etc,Information visualization},
  editor = {Tamassia, Roberto},
  file = {/home/dimitri/Nextcloud/Zotero/storage/26B7GY4Z/Tamassia - 2014 - Handbook of graph drawing and visualization.pdf}
}

@article{boyerCuttingEdgeSimplified2004,
  langid = {english},
  title = {On the {{Cutting Edge}}: {{Simplified O}}(n) {{Planarity}} by {{Edge Addition}}},
  volume = {8},
  issn = {1526-1719},
  url = {http://jgaa.info/getPaper?id=91},
  doi = {10.7155/jgaa.00091},
  shorttitle = {On the {{Cutting Edge}}},
  number = {3},
  journaltitle = {Journal of Graph Algorithms and Applications},
  urldate = {2018-09-25},
  date = {2004},
  pages = {241-273},
  author = {Boyer, John M. and Myrvold, Wendy J.},
  file = {/home/dimitri/Nextcloud/Zotero/storage/MVNUFRCN/Boyer and Myrvold - 2004 - On the Cutting Edge Simplified O(n) Planarity by .pdf}
}

@article{chowYouCouldHave2006,
  title = {You Could Have Invented Spectral Sequences},
  volume = {53},
  journaltitle = {Notices of the AMS},
  date = {2006},
  pages = {15-19},
  author = {Chow, Timothy Y},
  file = {/home/dimitri/Nextcloud/Zotero/storage/NYLF3JEI/Chow - 2006 - You could have invented spectral sequences.pdf}
}

@article{fruchtermanGraphDrawingForcedirected1991,
  langid = {english},
  title = {Graph Drawing by Force-Directed Placement},
  volume = {21},
  issn = {00380644, 1097024X},
  url = {http://doi.wiley.com/10.1002/spe.4380211102},
  doi = {10.1002/spe.4380211102},
  number = {11},
  journaltitle = {Software: Practice and Experience},
  urldate = {2018-09-29},
  date = {1991-11},
  pages = {1129-1164},
  author = {Fruchterman, Thomas M. J. and Reingold, Edward M.},
  file = {/home/dimitri/Nextcloud/Zotero/storage/3MNVF9NS/Fruchterman and Reingold - 1991 - Graph drawing by force-directed placement.pdf}
}

@article{vaswaniAttentionAllYou2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1706.03762},
  primaryClass = {cs},
  title = {Attention {{Is All You Need}}},
  url = {http://arxiv.org/abs/1706.03762},
  abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
  urldate = {2018-11-01},
  date = {2017-06-12},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  file = {/home/dimitri/Nextcloud/Zotero/storage/S5MY56ZH/Vaswani et al. - 2017 - Attention Is All You Need.pdf;/home/dimitri/Nextcloud/Zotero/storage/78H5JLFW/1706.html}
}

@article{elbayadPervasiveAttention2D2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1808.03867},
  primaryClass = {cs},
  title = {Pervasive {{Attention}}: {{2D Convolutional Neural Networks}} for {{Sequence}}-to-{{Sequence Prediction}}},
  url = {http://arxiv.org/abs/1808.03867},
  shorttitle = {Pervasive {{Attention}}},
  abstract = {Current state-of-the-art machine translation systems are based on encoder-decoder architectures, that first encode the input sequence, and then generate an output sequence based on the input encoding. Both are interfaced with an attention mechanism that recombines a fixed encoding of the source tokens based on the decoder state. We propose an alternative approach which instead relies on a single 2D convolutional neural network across both sequences. Each layer of our network re-codes source tokens on the basis of the output sequence produced so far. Attention-like properties are therefore pervasive throughout the network. Our model yields excellent results, outperforming state-of-the-art encoder-decoder systems, while being conceptually simpler and having fewer parameters.},
  urldate = {2018-11-01},
  date = {2018-08-11},
  keywords = {Computer Science - Computation and Language},
  author = {Elbayad, Maha and Besacier, Laurent and Verbeek, Jakob},
  file = {/home/dimitri/Nextcloud/Zotero/storage/PPPBD5FQ/Elbayad et al. - 2018 - Pervasive Attention 2D Convolutional Neural Netwo.pdf;/home/dimitri/Nextcloud/Zotero/storage/32I4JFXK/1808.html}
}

@article{wuTaleThreeProbabilistic2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1810.04261},
  primaryClass = {cs, stat},
  title = {A {{Tale}} of {{Three Probabilistic Families}}: {{Discriminative}}, {{Descriptive}} and {{Generative Models}}},
  url = {http://arxiv.org/abs/1810.04261},
  shorttitle = {A {{Tale}} of {{Three Probabilistic Families}}},
  abstract = {The pattern theory of Grenander is a mathematical framework where the patterns are represented by probability models on random variables of algebraic structures. In this paper, we review three families of probability models, namely, the discriminative models, the descriptive models, and the generative models. A discriminative model is in the form of a classifier. It specifies the conditional probability of the class label given the input signal. The descriptive model specifies the probability distribution of the signal, based on an energy function defined on the signal. A generative model assumes that the signal is generated by some latent variables via a transformation. We shall review these models within a common framework and explore their connections. We shall also review the recent developments that take advantage of the high approximation capacities of deep neural networks.},
  urldate = {2018-11-01},
  date = {2018-10-09},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Statistics - Machine Learning,Computer Science - Machine Learning},
  author = {Wu, Ying Nian and Gao, Ruiqi and Han, Tian and Zhu, Song-Chun},
  file = {/home/dimitri/Nextcloud/Zotero/storage/GR22VWEA/Wu et al. - 2018 - A Tale of Three Probabilistic Families Discrimina.pdf;/home/dimitri/Nextcloud/Zotero/storage/TYD8LULW/1810.html}
}

@online{ImprovingLanguageUnderstanding2018,
  title = {Improving {{Language Understanding}} with {{Unsupervised Learning}}},
  url = {https://blog.openai.com/language-unsupervised/},
  abstract = {We've obtained state-of-the-art results on a suite of diverse language tasks with a scalable, task-agnostic system, which we're also releasing. Our approach is a combination of two existing ideas: transformers and unsupervised pre-training. These results provide a convincing example that pairing supervised learning methods with unsupervised pre-training works very well;},
  journaltitle = {OpenAI Blog},
  urldate = {2018-11-01},
  date = {2018-06-11T18:11:50.000Z},
  file = {/home/dimitri/Nextcloud/Zotero/storage/5Q7P832V/2018 - Improving Language Understanding with Unsupervised.pdf;/home/dimitri/Nextcloud/Zotero/storage/LSPHJAFJ/language-unsupervised.html}
}

@article{howardUniversalLanguageModel2018,
  langid = {english},
  title = {Universal {{Language Model Fine}}-Tuning for {{Text Classification}}},
  url = {https://arxiv.org/abs/1801.06146},
  urldate = {2018-11-01},
  date = {2018-01-18},
  author = {Howard, Jeremy and Ruder, Sebastian},
  file = {/home/dimitri/Nextcloud/Zotero/storage/EZ637SJV/Howard and Ruder - 2018 - Universal Language Model Fine-tuning for Text Clas.pdf;/home/dimitri/Nextcloud/Zotero/storage/NFZLNNBA/1801.html}
}

@article{petersDeepContextualizedWord2018,
  langid = {english},
  title = {Deep Contextualized Word Representations},
  url = {https://arxiv.org/abs/1802.05365},
  urldate = {2018-11-01},
  date = {2018-02-15},
  author = {Peters, Matthew E. and Neumann, Mark and Iyyer, Mohit and Gardner, Matt and Clark, Christopher and Lee, Kenton and Zettlemoyer, Luke},
  file = {/home/dimitri/Nextcloud/Zotero/storage/IJGTZG24/Peters et al. - 2018 - Deep contextualized word representations.pdf;/home/dimitri/Nextcloud/Zotero/storage/D53AXJ8X/1802.html}
}

@article{ghavamzadehBayesianReinforcementLearning2015,
  langid = {english},
  title = {Bayesian {{Reinforcement Learning}}: {{A Survey}}},
  volume = {8},
  issn = {1935-8237, 1935-8245},
  url = {http://www.nowpublishers.com/article/Details/MAL-049},
  doi = {10.1561/2200000049},
  shorttitle = {Convex {{Optimization}}},
  number = {5-6},
  journaltitle = {Foundations and Trends® in Machine Learning},
  urldate = {2018-11-01},
  date = {2015},
  pages = {359-483},
  author = {Ghavamzadeh, Mohammed and Mannor, Shie and Pineau, Joelle and Tamar, Aviv},
  file = {/home/dimitri/Nextcloud/Zotero/storage/7SQZMB83/ghavamzadeh2015.pdf}
}

@incollection{kirosSkipThoughtVectors2015,
  title = {Skip-{{Thought Vectors}}},
  url = {http://papers.nips.cc/paper/5950-skip-thought-vectors.pdf},
  booktitle = {Advances in {{Neural Information Processing Systems}} 28},
  publisher = {{Curran Associates, Inc.}},
  urldate = {2018-11-01},
  date = {2015},
  pages = {3294--3302},
  author = {Kiros, Ryan and Zhu, Yukun and Salakhutdinov, Ruslan R and Zemel, Richard and Urtasun, Raquel and Torralba, Antonio and Fidler, Sanja},
  editor = {Cortes, C. and Lawrence, N. D. and Lee, D. D. and Sugiyama, M. and Garnett, R.},
  file = {/home/dimitri/Nextcloud/Zotero/storage/9G7C5BWF/Kiros et al. - 2015 - Skip-Thought Vectors.pdf;/home/dimitri/Nextcloud/Zotero/storage/H6Q23GPE/5950-skip-thought-vectors.html}
}

@inproceedings{morchidImprovingDialogueClassification2014,
  location = {{Florence, Italy}},
  title = {Improving Dialogue Classification Using a Topic Space Representation and a {{Gaussian}} Classifier Based on the Decision Rule},
  isbn = {978-1-4799-2893-4},
  url = {http://ieeexplore.ieee.org/document/6853571/},
  doi = {10.1109/ICASSP.2014.6853571},
  eventtitle = {{{ICASSP}} 2014 - 2014 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  booktitle = {2014 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  publisher = {{IEEE}},
  urldate = {2018-11-01},
  date = {2014-05},
  pages = {126-130},
  author = {Morchid, Mohamed and Dufour, Richard and Bousquet, Pierre-Michel and Bouallegue, Mohamed and Linares, Georges and De Mori, Renato},
  file = {/home/dimitri/Nextcloud/Zotero/storage/DUC8VMMF/Morchid et al. - 2014 - Improving dialogue classification using a topic sp.pdf}
}

@article{loweAutomaticTuringTest2017,
  langid = {english},
  title = {Towards an {{Automatic Turing Test}}: {{Learning}} to {{Evaluate Dialogue Responses}}},
  url = {https://arxiv.org/abs/1708.07149},
  shorttitle = {Towards an {{Automatic Turing Test}}},
  urldate = {2018-11-01},
  date = {2017-08-23},
  author = {Lowe, Ryan and Noseworthy, Michael and Serban, Iulian V. and Angelard-Gontier, Nicolas and Bengio, Yoshua and Pineau, Joelle},
  file = {/home/dimitri/Nextcloud/Zotero/storage/F7XIVP7E/Lowe et al. - 2017 - Towards an Automatic Turing Test Learning to Eval.pdf;/home/dimitri/Nextcloud/Zotero/storage/BKIQMR9L/1708.html}
}

@article{kulisMetricLearningSurvey2013,
  langid = {english},
  title = {Metric {{Learning}}: {{A Survey}}},
  volume = {5},
  issn = {1935-8237, 1935-8245},
  url = {https://www.nowpublishers.com/article/Details/MAL-019},
  doi = {10.1561/2200000019},
  shorttitle = {Metric {{Learning}}},
  abstract = {Metric Learning: A Survey},
  number = {4},
  journaltitle = {Foundations and Trends® in Machine Learning},
  shortjournal = {MAL},
  urldate = {2018-11-01},
  date = {2013-07-31},
  pages = {287-364},
  author = {Kulis, Brian},
  file = {/home/dimitri/Nextcloud/Zotero/storage/9VTCMMGJ/Kulis - 2013 - Metric Learning A Survey.pdf;/home/dimitri/Nextcloud/Zotero/storage/EN8ZKWRV/MAL-019.html}
}

@inproceedings{davisStructuredMetricLearning2008,
  location = {{New York, NY, USA}},
  title = {Structured {{Metric Learning}} for {{High Dimensional Problems}}},
  isbn = {978-1-60558-193-4},
  url = {http://doi.acm.org/10.1145/1401890.1401918},
  doi = {10.1145/1401890.1401918},
  abstract = {The success of popular algorithms such as k-means clustering or nearest neighbor searches depend on the assumption that the underlying distance functions reflect domain-specific notions of similarity for the problem at hand. The distance metric learning problem seeks to optimize a distance function subject to constraints that arise from fully-supervised or semisupervised information. Several recent algorithms have been proposed to learn such distance functions in low dimensional settings. One major shortcoming of these methods is their failure to scale to high dimensional problems that are becoming increasingly ubiquitous in modern data mining applications. In this paper, we present metric learning algorithms that scale linearly with dimensionality, permitting efficient optimization, storage, and evaluation of the learned metric. This is achieved through our main technical contribution which provides a framework based on the log-determinant matrix divergence which enables efficient optimization of structured, low-parameter Mahalanobis distances. Experimentally, we evaluate our methods across a variety of high dimensional domains, including text, statistical software analysis, and collaborative filtering, showing that our methods scale to data sets with tens of thousands or more features. We show that our learned metric can achieve excellent quality with respect to various criteria. For example, in the context of metric learning for nearest neighbor classification, we show that our methods achieve 24\% higher accuracy over the baseline distance. Additionally, our methods yield very good precision while providing recall measures up to 20\% higher than other baseline methods such as latent semantic analysis.},
  booktitle = {Proceedings of the 14th {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} and {{Data Mining}}},
  series = {{{KDD}} '08},
  publisher = {{ACM}},
  urldate = {2018-11-01},
  date = {2008},
  pages = {195--203},
  keywords = {algorithms,experimentation},
  author = {Davis, Jason V. and Dhillon, Inderjit S.},
  file = {/home/dimitri/Nextcloud/Zotero/storage/356T8BXL/Davis and Dhillon - 2008 - Structured Metric Learning for High Dimensional Pr.pdf}
}

@incollection{chorowskiAttentionBasedModelsSpeech2015,
  title = {Attention-{{Based Models}} for {{Speech Recognition}}},
  url = {http://papers.nips.cc/paper/5847-attention-based-models-for-speech-recognition.pdf},
  booktitle = {Advances in {{Neural Information Processing Systems}} 28},
  publisher = {{Curran Associates, Inc.}},
  urldate = {2018-11-03},
  date = {2015},
  pages = {577--585},
  author = {Chorowski, Jan K and Bahdanau, Dzmitry and Serdyuk, Dmitriy and Cho, Kyunghyun and Bengio, Yoshua},
  editor = {Cortes, C. and Lawrence, N. D. and Lee, D. D. and Sugiyama, M. and Garnett, R.},
  file = {/home/dimitri/Nextcloud/Zotero/storage/LLDYXQ9S/Chorowski et al. - 2015 - Attention-Based Models for Speech Recognition.pdf;/home/dimitri/Nextcloud/Zotero/storage/VJ4HG4T8/5847-attention-based-models-for-speech-recognition.html}
}

@article{bahdanauNeuralMachineTranslation2014,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1409.0473},
  primaryClass = {cs, stat},
  title = {Neural {{Machine Translation}} by {{Jointly Learning}} to {{Align}} and {{Translate}}},
  url = {http://arxiv.org/abs/1409.0473},
  abstract = {Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.},
  urldate = {2018-11-03},
  date = {2014-09-01},
  keywords = {Statistics - Machine Learning,Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  author = {Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
  file = {/home/dimitri/Nextcloud/Zotero/storage/EDT3ACT8/Bahdanau et al. - 2014 - Neural Machine Translation by Jointly Learning to .pdf;/home/dimitri/Nextcloud/Zotero/storage/APNE596P/1409.html}
}

@inproceedings{yangHierarchicalAttentionNetworks2016,
  langid = {english},
  location = {{San Diego, California}},
  title = {Hierarchical {{Attention Networks}} for {{Document Classification}}},
  url = {http://aclweb.org/anthology/N16-1174},
  doi = {10.18653/v1/N16-1174},
  eventtitle = {Proceedings of the 2016 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}},
  booktitle = {Proceedings of the 2016 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}},
  publisher = {{Association for Computational Linguistics}},
  urldate = {2018-11-03},
  date = {2016},
  pages = {1480-1489},
  author = {Yang, Zichao and Yang, Diyi and Dyer, Chris and He, Xiaodong and Smola, Alex and Hovy, Eduard},
  file = {/home/dimitri/Nextcloud/Zotero/storage/A3Q6ZJ6X/Yang et al. - 2016 - Hierarchical Attention Networks for Document Class.pdf}
}

@online{EmbedEncodeAttend,
  langid = {english},
  title = {Embed, Encode, Attend, Predict: {{The}} New Deep Learning Formula for State-of-the-Art {{NLP}} Models · {{Blog}} · {{Explosion AI}}},
  url = {https://explosion.ai/blog/deep-learning-formula-nlp},
  shorttitle = {Embed, Encode, Attend, Predict},
  abstract = {Over the last six months, a powerful new neural network playbook has come together for Natural Language Processing. The new approach can be summarised as a simple four-step formula: embed, encode, attend, predict. This post explains the components of this new approach, and shows how they're put together in two recent systems.},
  journaltitle = {Explosion AI},
  urldate = {2018-11-06},
  file = {/home/dimitri/Nextcloud/Zotero/storage/ZVWZQ88C/deep-learning-formula-nlp.html}
}

@article{parikhDecomposableAttentionModel2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1606.01933},
  primaryClass = {cs},
  title = {A {{Decomposable Attention Model}} for {{Natural Language Inference}}},
  url = {http://arxiv.org/abs/1606.01933},
  abstract = {We propose a simple neural architecture for natural language inference. Our approach uses attention to decompose the problem into subproblems that can be solved separately, thus making it trivially parallelizable. On the Stanford Natural Language Inference (SNLI) dataset, we obtain state-of-the-art results with almost an order of magnitude fewer parameters than previous work and without relying on any word-order information. Adding intra-sentence attention that takes a minimum amount of order into account yields further improvements.},
  urldate = {2018-11-06},
  date = {2016-06-06},
  keywords = {Computer Science - Computation and Language},
  author = {Parikh, Ankur P. and Täckström, Oscar and Das, Dipanjan and Uszkoreit, Jakob},
  file = {/home/dimitri/Nextcloud/Zotero/storage/6WB3I77D/Parikh et al. - 2016 - A Decomposable Attention Model for Natural Languag.pdf;/home/dimitri/Nextcloud/Zotero/storage/VVBB2ER4/1606.html}
}

@book{suttonReinforcementLearningIntroduction2018,
  location = {{Cambridge, MA}},
  title = {Reinforcement Learning: An Introduction},
  edition = {Second edition},
  isbn = {978-0-262-03924-6},
  shorttitle = {Reinforcement Learning},
  abstract = {"Reinforcement learning, one of the most active research areas in artificial intelligence, is a computational approach to learning whereby an agent tries to maximize the total amount of reward it receives while interacting with a complex, uncertain environment. In Reinforcement Learning, Richard Sutton and Andrew Barto provide a clear and simple account of the field's key ideas and algorithms."--},
  series = {Adaptive Computation and Machine Learning Series},
  publisher = {{The MIT Press}},
  date = {2018},
  keywords = {Reinforcement learning},
  author = {Sutton, Richard S. and Barto, Andrew G.},
  file = {/home/dimitri/Nextcloud/Zotero/storage/YAV98CV3/Sutton and Barto - 2018 - Reinforcement learning an introduction.pdf}
}

@article{vandemeentIntroductionProbabilisticProgramming2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1809.10756},
  primaryClass = {cs, stat},
  title = {An {{Introduction}} to {{Probabilistic Programming}}},
  url = {http://arxiv.org/abs/1809.10756},
  abstract = {This document is designed to be a first-year graduate-level introduction to probabilistic programming. It not only provides a thorough background for anyone wishing to use a probabilistic programming system, but also introduces the techniques needed to design and build these systems. It is aimed at people who have an undergraduate-level understanding of either or, ideally, both probabilistic machine learning and programming languages. We start with a discussion of model-based reasoning and explain why conditioning as a foundational computation is central to the fields of probabilistic machine learning and artificial intelligence. We then introduce a simple first-order probabilistic programming language (PPL) whose programs define static-computation-graph, finite-variable-cardinality models. In the context of this restricted PPL we introduce fundamental inference algorithms and describe how they can be implemented in the context of models denoted by probabilistic programs. In the second part of this document, we introduce a higher-order probabilistic programming language, with a functionality analogous to that of established programming languages. This affords the opportunity to define models with dynamic computation graphs, at the cost of requiring inference methods that generate samples by repeatedly executing the program. Foundational inference algorithms for this kind of probabilistic programming language are explained in the context of an interface between program executions and an inference controller. This document closes with a chapter on advanced topics which we believe to be, at the time of writing, interesting directions for probabilistic programming research; directions that point towards a tight integration with deep neural network research and the development of systems for next-generation artificial intelligence applications.},
  urldate = {2019-01-05},
  date = {2018-09-27},
  keywords = {Statistics - Machine Learning,Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Programming Languages},
  author = {family=Meent, given=Jan-Willem, prefix=van de, useprefix=true and Paige, Brooks and Yang, Hongseok and Wood, Frank},
  file = {/home/dimitri/Nextcloud/Zotero/storage/P4QUYCRF/van de Meent et al. - 2018 - An Introduction to Probabilistic Programming.pdf;/home/dimitri/Nextcloud/Zotero/storage/J4HQPMDM/1809.html}
}

@article{fongBackpropFunctorCompositional2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1711.10455},
  primaryClass = {cs, math},
  title = {Backprop as {{Functor}}: {{A}} Compositional Perspective on Supervised Learning},
  url = {http://arxiv.org/abs/1711.10455},
  shorttitle = {Backprop as {{Functor}}},
  abstract = {A supervised learning algorithm searches over a set of functions \$A \textbackslash{}to B\$ parametrised by a space \$P\$ to find the best approximation to some ideal function \$f\textbackslash{}colon A \textbackslash{}to B\$. It does this by taking examples \$(a,f(a)) \textbackslash{}in A\textbackslash{}times B\$, and updating the parameter according to some rule. We define a category where these update rules may be composed, and show that gradient descent---with respect to a fixed step size and an error function satisfying a certain property---defines a monoidal functor from a category of parametrised functions to this category of update rules. This provides a structural perspective on backpropagation, as well as a broad generalisation of neural networks.},
  urldate = {2019-01-05},
  date = {2017-11-28},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Mathematics - Category Theory},
  author = {Fong, Brendan and Spivak, David I. and Tuyéras, Rémy},
  file = {/home/dimitri/Nextcloud/Zotero/storage/2WBW6WCN/Fong et al. - 2017 - Backprop as Functor A compositional perspective o.pdf;/home/dimitri/Nextcloud/Zotero/storage/SSIRE6JS/1711.html}
}

@article{chenNeuralOrdinaryDifferential2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1806.07366},
  primaryClass = {cs, stat},
  title = {Neural {{Ordinary Differential Equations}}},
  url = {http://arxiv.org/abs/1806.07366},
  abstract = {We introduce a new family of deep neural network models. Instead of specifying a discrete sequence of hidden layers, we parameterize the derivative of the hidden state using a neural network. The output of the network is computed using a black-box differential equation solver. These continuous-depth models have constant memory cost, adapt their evaluation strategy to each input, and can explicitly trade numerical precision for speed. We demonstrate these properties in continuous-depth residual networks and continuous-time latent variable models. We also construct continuous normalizing flows, a generative model that can train by maximum likelihood, without partitioning or ordering the data dimensions. For training, we show how to scalably backpropagate through any ODE solver, without access to its internal operations. This allows end-to-end training of ODEs within larger models.},
  urldate = {2019-01-05},
  date = {2018-06-19},
  keywords = {Statistics - Machine Learning,Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  author = {Chen, Ricky T. Q. and Rubanova, Yulia and Bettencourt, Jesse and Duvenaud, David},
  file = {/home/dimitri/Nextcloud/Zotero/storage/26D4Y3GG/Chen et al. - 2018 - Neural Ordinary Differential Equations.pdf;/home/dimitri/Nextcloud/Zotero/storage/RNXT4EQV/1806.html}
}

@article{suarezTutorialDistanceMetric2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1812.05944},
  primaryClass = {cs, stat},
  title = {A {{Tutorial}} on {{Distance Metric Learning}}: {{Mathematical Foundations}}, {{Algorithms}} and {{Software}}},
  url = {http://arxiv.org/abs/1812.05944},
  shorttitle = {A {{Tutorial}} on {{Distance Metric Learning}}},
  abstract = {This paper describes the discipline of distance metric learning, a branch of machine learning that aims to learn distances from the data. Distance metric learning can be useful to improve similarity learning algorithms, and also has applications in dimensionality reduction. We describe the distance metric learning problem and analyze its main mathematical foundations. We discuss some of the most popular distance metric learning techniques used in classification, showing their goals and the required information to understand and use them. Furthermore, we present a Python package that collects a set of 17 distance metric learning techniques explained in this paper, with some experiments to evaluate the performance of the different algorithms. Finally, we discuss several possibilities of future work in this topic.},
  urldate = {2019-01-05},
  date = {2018-12-14},
  keywords = {Statistics - Machine Learning,Computer Science - Machine Learning},
  author = {Suárez, Juan Luis and García, Salvador and Herrera, Francisco},
  file = {/home/dimitri/Nextcloud/Zotero/storage/K5KJ9LWH/Suárez et al. - 2018 - A Tutorial on Distance Metric Learning Mathematic.pdf;/home/dimitri/Nextcloud/Zotero/storage/TK9QEXRI/1812.html}
}

@article{zeghidourFullyConvolutionalSpeech2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1812.06864},
  primaryClass = {cs},
  title = {Fully {{Convolutional Speech Recognition}}},
  url = {http://arxiv.org/abs/1812.06864},
  abstract = {Current state-of-the-art speech recognition systems build on recurrent neural networks for acoustic and/or language modeling, and rely on feature extraction pipelines to extract mel-filterbanks or cepstral coefficients. In this paper we present an alternative approach based solely on convolutional neural networks, leveraging recent advances in acoustic models from the raw waveform and language modeling. This fully convolutional approach is trained end-to-end to predict characters from the raw waveform, removing the feature extraction step altogether. An external convolutional language model is used to decode words. On Wall Street Journal, our model matches the current state-of-the-art. On Librispeech, we report state-of-the-art performance among end-to-end models, including Deep Speech 2 trained with 12 times more acoustic data and significantly more linguistic data.},
  urldate = {2019-01-05},
  date = {2018-12-17},
  keywords = {Computer Science - Computation and Language},
  author = {Zeghidour, Neil and Xu, Qiantong and Liptchinsky, Vitaliy and Usunier, Nicolas and Synnaeve, Gabriel and Collobert, Ronan},
  file = {/home/dimitri/Nextcloud/Zotero/storage/WED6N77V/Zeghidour et al. - 2018 - Fully Convolutional Speech Recognition.pdf;/home/dimitri/Nextcloud/Zotero/storage/YI3SZPAW/1812.html}
}

@article{rezendeVariationalInferenceNormalizing2015,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1505.05770},
  primaryClass = {cs, stat},
  title = {Variational {{Inference}} with {{Normalizing Flows}}},
  url = {http://arxiv.org/abs/1505.05770},
  abstract = {The choice of approximate posterior distribution is one of the core problems in variational inference. Most applications of variational inference employ simple families of posterior approximations in order to allow for efficient inference, focusing on mean-field or other simple structured approximations. This restriction has a significant impact on the quality of inferences made using variational methods. We introduce a new approach for specifying flexible, arbitrarily complex and scalable approximate posterior distributions. Our approximations are distributions constructed through a normalizing flow, whereby a simple initial density is transformed into a more complex one by applying a sequence of invertible transformations until a desired level of complexity is attained. We use this view of normalizing flows to develop categories of finite and infinitesimal flows and provide a unified view of approaches for constructing rich posterior approximations. We demonstrate that the theoretical advantages of having posteriors that better match the true posterior, combined with the scalability of amortized variational approaches, provides a clear improvement in performance and applicability of variational inference.},
  urldate = {2019-01-05},
  date = {2015-05-21},
  keywords = {Statistics - Machine Learning,Computer Science - Artificial Intelligence,Statistics - Computation,Statistics - Methodology,Computer Science - Machine Learning},
  author = {Rezende, Danilo Jimenez and Mohamed, Shakir},
  file = {/home/dimitri/Nextcloud/Zotero/storage/EAISFZAB/Rezende and Mohamed - 2015 - Variational Inference with Normalizing Flows.pdf;/home/dimitri/Nextcloud/Zotero/storage/N7GPL9BI/1505.html}
}

@incollection{xuDistilledWassersteinLearning2018,
  title = {Distilled {{Wasserstein Learning}} for {{Word Embedding}} and {{Topic Modeling}}},
  url = {http://papers.nips.cc/paper/7443-distilled-wasserstein-learning-for-word-embedding-and-topic-modeling.pdf},
  booktitle = {Advances in {{Neural Information Processing Systems}} 31},
  publisher = {{Curran Associates, Inc.}},
  urldate = {2019-01-05},
  date = {2018},
  pages = {1723--1732},
  author = {Xu, Hongteng and Wang, Wenlin and Liu, Wei and Carin, Lawrence},
  editor = {Bengio, S. and Wallach, H. and Larochelle, H. and Grauman, K. and Cesa-Bianchi, N. and Garnett, R.},
  file = {/home/dimitri/Nextcloud/Zotero/storage/BBNAVAHB/Xu et al. - 2018 - Distilled Wasserstein Learning for Word Embedding .pdf;/home/dimitri/Nextcloud/Zotero/storage/EMHZUYN6/7443-distilled-wasserstein-learning-for-word-embedding-and-topic-modeling.html}
}

@incollection{chenDimensionalityReductionStationary2018,
  title = {Dimensionality {{Reduction}} for {{Stationary Time Series}} via {{Stochastic Nonconvex Optimization}}},
  url = {http://papers.nips.cc/paper/7609-dimensionality-reduction-for-stationary-time-series-via-stochastic-nonconvex-optimization.pdf},
  booktitle = {Advances in {{Neural Information Processing Systems}} 31},
  publisher = {{Curran Associates, Inc.}},
  urldate = {2019-01-05},
  date = {2018},
  pages = {3500--3510},
  author = {Chen, Minshuo and Yang, Lin and Wang, Mengdi and Zhao, Tuo},
  editor = {Bengio, S. and Wallach, H. and Larochelle, H. and Grauman, K. and Cesa-Bianchi, N. and Garnett, R.},
  file = {/home/dimitri/Nextcloud/Zotero/storage/RRSPFFHD/Chen et al. - 2018 - Dimensionality Reduction for Stationary Time Serie.pdf;/home/dimitri/Nextcloud/Zotero/storage/RFXM8S47/7609-dimensionality-reduction-for-stationary-time-series-via-stochastic-nonconvex-optimization.html}
}

@article{zhangDeepLearningGraphs2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1812.04202},
  primaryClass = {cs, stat},
  title = {Deep {{Learning}} on {{Graphs}}: {{A Survey}}},
  url = {http://arxiv.org/abs/1812.04202},
  shorttitle = {Deep {{Learning}} on {{Graphs}}},
  abstract = {Deep learning has been shown successful in a number of domains, ranging from acoustics, images to natural language processing. However, applying deep learning to the ubiquitous graph data is non-trivial because of the unique characteristics of graphs. Recently, a significant amount of research efforts have been devoted to this area, greatly advancing graph analyzing techniques. In this survey, we comprehensively review different kinds of deep learning methods applied to graphs. We divide existing methods into three main categories: semi-supervised methods including Graph Neural Networks and Graph Convolutional Networks, unsupervised methods including Graph Autoencoders, and recent advancements including Graph Recurrent Neural Networks and Graph Reinforcement Learning. We then provide a comprehensive overview of these methods in a systematic manner following their history of developments. We also analyze the differences of these methods and how to composite different architectures. Finally, we briefly outline their applications and discuss potential future directions.},
  urldate = {2019-01-05},
  date = {2018-12-10},
  keywords = {Statistics - Machine Learning,Computer Science - Social and Information Networks,Computer Science - Machine Learning},
  author = {Zhang, Ziwei and Cui, Peng and Zhu, Wenwu},
  file = {/home/dimitri/Nextcloud/Zotero/storage/BSARA6B9/Zhang et al. - 2018 - Deep Learning on Graphs A Survey.pdf;/home/dimitri/Nextcloud/Zotero/storage/4GDZXSVW/1812.html}
}

@article{francois-lavetIntroductionDeepReinforcement2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1811.12560},
  title = {An {{Introduction}} to {{Deep Reinforcement Learning}}},
  volume = {11},
  issn = {1935-8237, 1935-8245},
  url = {http://arxiv.org/abs/1811.12560},
  doi = {10.1561/2200000071},
  abstract = {Deep reinforcement learning is the combination of reinforcement learning (RL) and deep learning. This field of research has been able to solve a wide range of complex decision-making tasks that were previously out of reach for a machine. Thus, deep RL opens up many new applications in domains such as healthcare, robotics, smart grids, finance, and many more. This manuscript provides an introduction to deep reinforcement learning models, algorithms and techniques. Particular focus is on the aspects related to generalization and how deep RL can be used for practical applications. We assume the reader is familiar with basic machine learning concepts.},
  number = {3-4},
  journaltitle = {Foundations and Trends® in Machine Learning},
  urldate = {2019-01-05},
  date = {2018},
  pages = {219-354},
  keywords = {Statistics - Machine Learning,Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  author = {Francois-Lavet, Vincent and Henderson, Peter and Islam, Riashat and Bellemare, Marc G. and Pineau, Joelle},
  file = {/home/dimitri/Nextcloud/Zotero/storage/YLA8FGDN/Francois-Lavet et al. - 2018 - An Introduction to Deep Reinforcement Learning.pdf;/home/dimitri/Nextcloud/Zotero/storage/E7XPSSQZ/1811.html}
}

@article{hofferDeepMetricLearning2014,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1412.6622},
  primaryClass = {cs, stat},
  title = {Deep Metric Learning Using {{Triplet}} Network},
  url = {http://arxiv.org/abs/1412.6622},
  abstract = {Deep learning has proven itself as a successful set of models for learning useful semantic representations of data. These, however, are mostly implicitly learned as part of a classification task. In this paper we propose the triplet network model, which aims to learn useful representations by distance comparisons. A similar model was defined by Wang et al. (2014), tailor made for learning a ranking for image information retrieval. Here we demonstrate using various datasets that our model learns a better representation than that of its immediate competitor, the Siamese network. We also discuss future possible usage as a framework for unsupervised learning.},
  urldate = {2019-01-05},
  date = {2014-12-20},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Statistics - Machine Learning,Computer Science - Machine Learning},
  author = {Hoffer, Elad and Ailon, Nir},
  file = {/home/dimitri/Nextcloud/Zotero/storage/Z66BDQLS/Hoffer and Ailon - 2014 - Deep metric learning using Triplet network.pdf;/home/dimitri/Nextcloud/Zotero/storage/NRZ8EIN8/1412.html}
}

@article{coifmanDiffusionMaps2006,
  title = {Diffusion Maps},
  volume = {21},
  issn = {1063-5203},
  url = {http://www.sciencedirect.com/science/article/pii/S1063520306000546},
  doi = {10.1016/j.acha.2006.04.006},
  abstract = {In this paper, we provide a framework based upon diffusion processes for finding meaningful geometric descriptions of data sets. We show that eigenfunctions of Markov matrices can be used to construct coordinates called diffusion maps that generate efficient representations of complex geometric structures. The associated family of diffusion distances, obtained by iterating the Markov matrix, defines multiscale geometries that prove to be useful in the context of data parametrization and dimensionality reduction. The proposed framework relates the spectral properties of Markov processes to their geometric counterparts and it unifies ideas arising in a variety of contexts such as machine learning, spectral graph theory and eigenmap methods.},
  number = {1},
  journaltitle = {Applied and Computational Harmonic Analysis},
  shortjournal = {Applied and Computational Harmonic Analysis},
  series = {Special {{Issue}}: {{Diffusion Maps}} and {{Wavelets}}},
  urldate = {2019-01-05},
  date = {2006-07-01},
  pages = {5-30},
  keywords = {Graph Laplacian,Dimensionality reduction,Diffusion metric,Diffusion processes,Eigenmaps,Manifold learning},
  author = {Coifman, Ronald R. and Lafon, Stéphane},
  file = {/home/dimitri/Nextcloud/Zotero/storage/N4H9GEL3/Coifman and Lafon - 2006 - Diffusion maps.pdf;/home/dimitri/Nextcloud/Zotero/storage/GHMIPP5F/S1063520306000546.html}
}

@online{FlowbasedDeepGenerative,
  title = {Flow-Based {{Deep Generative Models}}},
  url = {https://lilianweng.github.io/lil-log/2018/10/13/flow-based-deep-generative-models.html},
  urldate = {2019-01-05},
  file = {/home/dimitri/Nextcloud/Zotero/storage/32JQSMQ5/flow-based-deep-generative-models.html}
}

@article{ibarzRewardLearningHuman2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1811.06521},
  primaryClass = {cs, stat},
  title = {Reward Learning from Human Preferences and Demonstrations in {{Atari}}},
  url = {http://arxiv.org/abs/1811.06521},
  abstract = {To solve complex real-world problems with reinforcement learning, we cannot rely on manually specified reward functions. Instead, we can have humans communicate an objective to the agent directly. In this work, we combine two approaches to learning from human feedback: expert demonstrations and trajectory preferences. We train a deep neural network to model the reward function and use its predicted reward to train an DQN-based deep reinforcement learning agent on 9 Atari games. Our approach beats the imitation learning baseline in 7 games and achieves strictly superhuman performance on 2 games without using game rewards. Additionally, we investigate the goodness of fit of the reward model, present some reward hacking problems, and study the effects of noise in the human labels.},
  urldate = {2019-01-15},
  date = {2018-11-15},
  keywords = {Statistics - Machine Learning,Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  author = {Ibarz, Borja and Leike, Jan and Pohlen, Tobias and Irving, Geoffrey and Legg, Shane and Amodei, Dario},
  file = {/home/dimitri/Nextcloud/Zotero/storage/FDNA6ENP/Ibarz et al. - 2018 - Reward learning from human preferences and demonst.pdf;/home/dimitri/Nextcloud/Zotero/storage/KLGWEAMC/1811.html}
}

@article{burdaExplorationRandomNetwork2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1810.12894},
  primaryClass = {cs, stat},
  title = {Exploration by {{Random Network Distillation}}},
  url = {http://arxiv.org/abs/1810.12894},
  abstract = {We introduce an exploration bonus for deep reinforcement learning methods that is easy to implement and adds minimal overhead to the computation performed. The bonus is the error of a neural network predicting features of the observations given by a fixed randomly initialized neural network. We also introduce a method to flexibly combine intrinsic and extrinsic rewards. We find that the random network distillation (RND) bonus combined with this increased flexibility enables significant progress on several hard exploration Atari games. In particular we establish state of the art performance on Montezuma's Revenge, a game famously difficult for deep reinforcement learning methods. To the best of our knowledge, this is the first method that achieves better than average human performance on this game without using demonstrations or having access to the underlying state of the game, and occasionally completes the first level.},
  urldate = {2019-01-15},
  date = {2018-10-30},
  keywords = {Statistics - Machine Learning,Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  author = {Burda, Yuri and Edwards, Harrison and Storkey, Amos and Klimov, Oleg},
  file = {/home/dimitri/Nextcloud/Zotero/storage/IDJN2LWZ/Burda et al. - 2018 - Exploration by Random Network Distillation.pdf;/home/dimitri/Nextcloud/Zotero/storage/JQG98I96/1810.html}
}

@article{zhouGraphNeuralNetworks2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1812.08434},
  primaryClass = {cs, stat},
  title = {Graph {{Neural Networks}}: {{A Review}} of {{Methods}} and {{Applications}}},
  url = {http://arxiv.org/abs/1812.08434},
  shorttitle = {Graph {{Neural Networks}}},
  abstract = {Lots of learning tasks require dealing with graph data which contains rich relation information among elements. Modeling physics system, learning molecular fingerprints, predicting protein interface, and classifying diseases require that a model learns from graph inputs. In other domains such as learning from non-structural data like texts and images, reasoning on extracted structures, like the dependency tree of sentences and the scene graph of images, is an important research topic which also needs graph reasoning models. Graph neural networks (GNNs) are connectionist models that capture the dependence of graphs via message passing between the nodes of graphs. Unlike standard neural networks, graph neural networks retain a state that can represent information from its neighborhood with arbitrary depth. Although the primitive GNNs have been found difficult to train for a fixed point, recent advances in network architectures, optimization techniques, and parallel computation have enabled successful learning with them. In recent years, systems based on graph convolutional network (GCN) and gated graph neural network (GGNN) have demonstrated ground-breaking performance on many tasks mentioned above. In this survey, we provide a detailed review over existing graph neural network models, systematically categorize the applications, and propose four open problems for future research.},
  urldate = {2019-01-15},
  date = {2018-12-20},
  keywords = {Statistics - Machine Learning,Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  author = {Zhou, Jie and Cui, Ganqu and Zhang, Zhengyan and Yang, Cheng and Liu, Zhiyuan and Sun, Maosong},
  file = {/home/dimitri/Nextcloud/Zotero/storage/BGVEJHL9/Zhou et al. - 2018 - Graph Neural Networks A Review of Methods and App.pdf;/home/dimitri/Nextcloud/Zotero/storage/5DUMYMA2/1812.html}
}

@article{wuComprehensiveSurveyGraph2019,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1901.00596},
  primaryClass = {cs, stat},
  title = {A {{Comprehensive Survey}} on {{Graph Neural Networks}}},
  url = {http://arxiv.org/abs/1901.00596},
  abstract = {Deep learning has revolutionized many machine learning tasks in recent years, ranging from image classification and video processing to speech recognition and natural language understanding. The data in these tasks are typically represented in the Euclidean space. However, there is an increasing number of applications where data are generated from non-Euclidean domains and are represented as graphs with complex relationships and interdependency between objects. The complexity of graph data has imposed significant challenges on existing machine learning algorithms. Recently, many studies on extending deep learning approaches for graph data have emerged. In this survey, we provide a comprehensive overview of graph neural networks (GNNs) in data mining and machine learning fields. We propose a new taxonomy to divide the state-of-the-art graph neural networks into different categories. With a focus on graph convolutional networks, we review alternative architectures that have recently been developed; these learning paradigms include graph attention networks, graph autoencoders, graph generative networks, and graph spatial-temporal networks. We further discuss the applications of graph neural networks across various domains and summarize the open source codes and benchmarks of the existing algorithms on different learning tasks. Finally, we propose potential research directions in this fast-growing field.},
  urldate = {2019-01-15},
  date = {2019-01-02},
  keywords = {Statistics - Machine Learning,Computer Science - Machine Learning},
  author = {Wu, Zonghan and Pan, Shirui and Chen, Fengwen and Long, Guodong and Zhang, Chengqi and Yu, Philip S.},
  file = {/home/dimitri/Nextcloud/Zotero/storage/5WNGAHMD/Wu et al. - 2019 - A Comprehensive Survey on Graph Neural Networks.pdf;/home/dimitri/Nextcloud/Zotero/storage/KF58J6QT/1901.html}
}

@article{kimTutorialDeepLatent2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1812.06834},
  primaryClass = {cs, stat},
  title = {A {{Tutorial}} on {{Deep Latent Variable Models}} of {{Natural Language}}},
  url = {http://arxiv.org/abs/1812.06834},
  abstract = {There has been much recent, exciting work on combining the complementary strengths of latent variable models and deep learning. Latent variable modeling makes it easy to explicitly specify model constraints through conditional independence properties, while deep learning makes it possible to parameterize these conditional likelihoods with powerful function approximators. While these "deep latent variable" models provide a rich, flexible framework for modeling many real-world phenomena, difficulties exist: deep parameterizations of conditional likelihoods usually make posterior inference intractable, and latent variable objectives often complicate backpropagation by introducing points of non-differentiability. This tutorial explores these issues in depth through the lens of variational inference.},
  urldate = {2019-01-15},
  date = {2018-12-17},
  keywords = {Statistics - Machine Learning,Computer Science - Computation and Language,Computer Science - Machine Learning},
  author = {Kim, Yoon and Wiseman, Sam and Rush, Alexander M.},
  file = {/home/dimitri/Nextcloud/Zotero/storage/R69WH5C8/Kim et al. - 2018 - A Tutorial on Deep Latent Variable Models of Natur.pdf;/home/dimitri/Nextcloud/Zotero/storage/68ULGC3B/1812.html}
}

@article{bjervaWhatLanguageRepresentations2019,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1901.02646},
  primaryClass = {cs},
  title = {What Do {{Language Representations Really Represent}}?},
  url = {http://arxiv.org/abs/1901.02646},
  abstract = {A neural language model trained on a text corpus can be used to induce distributed representations of words, such that similar words end up with similar representations. If the corpus is multilingual, the same model can be used to learn distributed representations of languages, such that similar languages end up with similar representations. We show that this holds even when the multilingual corpus has been translated into English, by picking up the faint signal left by the source languages. However, just like it is a thorny problem to separate semantic from syntactic similarity in word representations, it is not obvious what type of similarity is captured by language representations. We investigate correlations and causal relationships between language representations learned from translations on one hand, and genetic, geographical, and several levels of structural similarity between languages on the other. Of these, structural similarity is found to correlate most strongly with language representation similarity, while genetic relationships---a convenient benchmark used for evaluation in previous work---appears to be a confounding factor. Apart from implications about translation effects, we see this more generally as a case where NLP and linguistic typology can interact and benefit one another.},
  urldate = {2019-01-16},
  date = {2019-01-09},
  keywords = {Computer Science - Computation and Language},
  author = {Bjerva, Johannes and Östling, Robert and Veiga, Maria Han and Tiedemann, Jörg and Augenstein, Isabelle},
  file = {/home/dimitri/Nextcloud/Zotero/storage/UDXDGTQJ/Bjerva et al. - 2019 - What do Language Representations Really Represent.pdf;/home/dimitri/Nextcloud/Zotero/storage/MIAJ5W9B/1901.html}
}

@article{egerItTimeSwish2019,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1901.02671},
  primaryClass = {cs},
  title = {Is It {{Time}} to {{Swish}}? {{Comparing Deep Learning Activation Functions Across NLP}} Tasks},
  url = {http://arxiv.org/abs/1901.02671},
  shorttitle = {Is It {{Time}} to {{Swish}}?},
  abstract = {Activation functions play a crucial role in neural networks because they are the nonlinearities which have been attributed to the success story of deep learning. One of the currently most popular activation functions is ReLU, but several competitors have recently been proposed or 'discovered', including LReLU functions and swish. While most works compare newly proposed activation functions on few tasks (usually from image classification) and against few competitors (usually ReLU), we perform the first large-scale comparison of 21 activation functions across eight different NLP tasks. We find that a largely unknown activation function performs most stably across all tasks, the so-called penalized tanh function. We also show that it can successfully replace the sigmoid and tanh gates in LSTM cells, leading to a 2 percentage point (pp) improvement over the standard choices on a challenging NLP task.},
  urldate = {2019-01-16},
  date = {2019-01-09},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  author = {Eger, Steffen and Youssef, Paul and Gurevych, Iryna},
  file = {/home/dimitri/Nextcloud/Zotero/storage/H88APBIP/Eger et al. - 2019 - Is it Time to Swish Comparing Deep Learning Activ.pdf;/home/dimitri/Nextcloud/Zotero/storage/2KW8IADU/1901.html}
}

@article{liDeepReinforcementLearning2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1810.06339},
  primaryClass = {cs, stat},
  title = {Deep {{Reinforcement Learning}}},
  url = {http://arxiv.org/abs/1810.06339},
  abstract = {We discuss deep reinforcement learning in an overview style. We draw a big picture, filled with details. We discuss six core elements, six important mechanisms, and twelve applications, focusing on contemporary work, and in historical contexts. We start with background of artificial intelligence, machine learning, deep learning, and reinforcement learning (RL), with resources. Next we discuss RL core elements, including value function, policy, reward, model, exploration vs. exploitation, and representation. Then we discuss important mechanisms for RL, including attention and memory, unsupervised learning, hierarchical RL, multi-agent RL, relational RL, and learning to learn. After that, we discuss RL applications, including games, robotics, natural language processing (NLP), computer vision, finance, business management, healthcare, education, energy, transportation, computer systems, and, science, engineering, and art. Finally we summarize briefly, discuss challenges and opportunities, and close with an epilogue.},
  urldate = {2019-01-16},
  date = {2018-10-15},
  keywords = {Statistics - Machine Learning,Computer Science - Machine Learning},
  author = {Li, Yuxi},
  file = {/home/dimitri/Nextcloud/Zotero/storage/IRDZMEMJ/Li - 2018 - Deep Reinforcement Learning.pdf;/home/dimitri/Nextcloud/Zotero/storage/V8PVQIUQ/1810.html}
}

@article{hancockLearningDialogueDeployment2019,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1901.05415},
  primaryClass = {cs, stat},
  title = {Learning from {{Dialogue}} after {{Deployment}}: {{Feed Yourself}}, {{Chatbot}}!},
  url = {http://arxiv.org/abs/1901.05415},
  shorttitle = {Learning from {{Dialogue}} after {{Deployment}}},
  abstract = {The majority of conversations a dialogue agent sees over its lifetime occur after it has already been trained and deployed, leaving a vast store of potential training signal untapped. In this work, we propose the self-feeding chatbot, a dialogue agent with the ability to extract new training examples from the conversations it participates in. As our agent engages in conversation, it also estimates user satisfaction in its responses. When the conversation appears to be going well, the user's responses become new training examples to imitate. When the agent believes it has made a mistake, it asks for feedback; learning to predict the feedback that will be given improves the chatbot's dialogue abilities further. On the PersonaChat chit-chat dataset with over 131k training examples, we find that learning from dialogue with a self-feeding chatbot significantly improves performance, regardless of the amount of traditional supervision.},
  urldate = {2019-01-18},
  date = {2019-01-16},
  keywords = {Statistics - Machine Learning,Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Human-Computer Interaction},
  author = {Hancock, Braden and Bordes, Antoine and Mazare, Pierre-Emmanuel and Weston, Jason},
  file = {/home/dimitri/Nextcloud/Zotero/storage/7Y8L4A8C/Hancock et al. - 2019 - Learning from Dialogue after Deployment Feed Your.pdf;/home/dimitri/Nextcloud/Zotero/storage/N3VWAGA2/1901.html}
}

@article{yangTheoreticalAnalysisDeep2019,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1901.00137},
  primaryClass = {cs, math, stat},
  title = {A {{Theoretical Analysis}} of {{Deep Q}}-{{Learning}}},
  url = {http://arxiv.org/abs/1901.00137},
  abstract = {Despite the great empirical success of deep reinforcement learning, its theoretical foundation is less well understood. In this work, we make the first attempt to theoretically understand the deep Q-network (DQN) algorithm (Mnih et al., 2015) from both algorithmic and statistical perspectives. In specific, we focus on a slight simplification of DQN that fully captures its key features. Under mild assumptions, we establish the algorithmic and statistical rates of convergence for the action-value functions of the iterative policy sequence obtained by DQN. In particular, the statistical error characterizes the bias and variance that arise from approximating the action-value function using deep neural network, while the algorithmic error converges to zero at a geometric rate. As a byproduct, our analysis provides justifications for the techniques of experience replay and target network, which are crucial to the empirical success of DQN. Furthermore, as a simple extension of DQN, we propose the Minimax-DQN algorithm for zero-sum Markov game with two players. Borrowing the analysis of DQN, we also quantify the difference between the policies obtained by Minimax-DQN and the Nash equilibrium of the Markov game in terms of both the algorithmic and statistical rates of convergence.},
  urldate = {2019-01-18},
  date = {2019-01-01},
  keywords = {Statistics - Machine Learning,Computer Science - Machine Learning,Mathematics - Optimization and Control},
  author = {Yang, Zhuora and Xie, Yuchen and Wang, Zhaoran},
  file = {/home/dimitri/Nextcloud/Zotero/storage/6HWR8AAS/Yang et al. - 2019 - A Theoretical Analysis of Deep Q-Learning.pdf;/home/dimitri/Nextcloud/Zotero/storage/62AJI6KW/1901.html}
}

@article{leikeScalableAgentAlignment2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1811.07871},
  primaryClass = {cs, stat},
  title = {Scalable Agent Alignment via Reward Modeling: A Research Direction},
  url = {http://arxiv.org/abs/1811.07871},
  shorttitle = {Scalable Agent Alignment via Reward Modeling},
  abstract = {One obstacle to applying reinforcement learning algorithms to real-world problems is the lack of suitable reward functions. Designing such reward functions is difficult in part because the user only has an implicit understanding of the task objective. This gives rise to the agent alignment problem: how do we create agents that behave in accordance with the user's intentions? We outline a high-level research direction to solve the agent alignment problem centered around reward modeling: learning a reward function from interaction with the user and optimizing the learned reward function with reinforcement learning. We discuss the key challenges we expect to face when scaling reward modeling to complex and general domains, concrete approaches to mitigate these challenges, and ways to establish trust in the resulting agents.},
  urldate = {2019-01-18},
  date = {2018-11-19},
  keywords = {Statistics - Machine Learning,Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  author = {Leike, Jan and Krueger, David and Everitt, Tom and Martic, Miljan and Maini, Vishal and Legg, Shane},
  file = {/home/dimitri/Nextcloud/Zotero/storage/LA4VMIPH/Leike et al. - 2018 - Scalable agent alignment via reward modeling a re.pdf;/home/dimitri/Nextcloud/Zotero/storage/D5SRETKG/1811.html}
}

@article{garneloNeuralProcesses2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1807.01622},
  primaryClass = {cs, stat},
  title = {Neural {{Processes}}},
  url = {http://arxiv.org/abs/1807.01622},
  abstract = {A neural network (NN) is a parameterised function that can be tuned via gradient descent to approximate a labelled collection of data with high precision. A Gaussian process (GP), on the other hand, is a probabilistic model that defines a distribution over possible functions, and is updated in light of data via the rules of probabilistic inference. GPs are probabilistic, data-efficient and flexible, however they are also computationally intensive and thus limited in their applicability. We introduce a class of neural latent variable models which we call Neural Processes (NPs), combining the best of both worlds. Like GPs, NPs define distributions over functions, are capable of rapid adaptation to new observations, and can estimate the uncertainty in their predictions. Like NNs, NPs are computationally efficient during training and evaluation but also learn to adapt their priors to data. We demonstrate the performance of NPs on a range of learning tasks, including regression and optimisation, and compare and contrast with related models in the literature.},
  urldate = {2019-01-20},
  date = {2018-07-04},
  keywords = {Statistics - Machine Learning,Computer Science - Machine Learning},
  author = {Garnelo, Marta and Schwarz, Jonathan and Rosenbaum, Dan and Viola, Fabio and Rezende, Danilo J. and Eslami, S. M. Ali and Teh, Yee Whye},
  file = {/home/dimitri/Nextcloud/Zotero/storage/HMVDFLLS/Garnelo et al. - 2018 - Neural Processes.pdf;/home/dimitri/Nextcloud/Zotero/storage/AKJQZCJ2/1807.html}
}

@inproceedings{domingosUnifyingLogicalStatistical2016,
  langid = {english},
  location = {{New York, NY, USA}},
  title = {Unifying {{Logical}} and {{Statistical AI}}},
  isbn = {978-1-4503-4391-6},
  url = {http://dl.acm.org/citation.cfm?doid=2933575.2935321},
  doi = {10.1145/2933575.2935321},
  eventtitle = {The 31st {{Annual ACM}}/{{IEEE Symposium}}},
  booktitle = {Proceedings of the 31st {{Annual ACM}}/{{IEEE Symposium}} on {{Logic}} in {{Computer Science}} - {{LICS}} '16},
  publisher = {{ACM Press}},
  urldate = {2019-01-20},
  date = {2016},
  pages = {1-11},
  author = {Domingos, Pedro and Lowd, Daniel and Kok, Stanley and Nath, Aniruddh and Poon, Hoifung and Richardson, Matthew and Singla, Parag},
  file = {/home/dimitri/Nextcloud/Zotero/storage/C48DFC4I/Domingos et al. - 2016 - Unifying Logical and Statistical AI.pdf}
}

@article{bellmanMarkovianDecisionProcess1957,
  title = {A {{Markovian Decision Process}}},
  volume = {6},
  issn = {0095-9057},
  url = {https://www.jstor.org/stable/24900506},
  number = {5},
  journaltitle = {Journal of Mathematics and Mechanics},
  urldate = {2019-01-21},
  date = {1957},
  pages = {679-684},
  author = {Bellman, Richard},
  file = {/home/dimitri/Nextcloud/Zotero/storage/JQRF7ZWX/Bellman - 1957 - A Markovian Decision Process.pdf}
}

@article{suttonLearningPredictMethods1988,
  langid = {english},
  title = {Learning to Predict by the Methods of Temporal Differences},
  volume = {3},
  issn = {1573-0565},
  url = {https://doi.org/10.1007/BF00115009},
  doi = {10.1007/BF00115009},
  abstract = {This article introduces a class of incremental learning procedures specialized for prediction-that is, for using past experience with an incompletely known system to predict its future behavior. Whereas conventional prediction-learning methods assign credit by means of the difference between predicted and actual outcomes, the new methods assign credit by means of the difference between temporally successive predictions. Although such temporal-difference methods have been used in Samuel's checker player, Holland's bucket brigade, and the author's Adaptive Heuristic Critic, they have remained poorly understood. Here we prove their convergence and optimality for special cases and relate them to supervised-learning methods. For most real-world prediction problems, temporal-difference methods require less memory and less peak computation than conventional methods and they produce more accurate predictions. We argue that most problems to which supervised learning is currently applied are really prediction problems of the sort to which temporal-difference methods can be applied to advantage.},
  number = {1},
  journaltitle = {Machine Learning},
  shortjournal = {Mach Learn},
  urldate = {2019-01-21},
  date = {1988-08-01},
  pages = {9-44},
  keywords = {connectionism,credit assignment,evaluation functions,Incremental learning,prediction},
  author = {Sutton, Richard S.},
  file = {/home/dimitri/Nextcloud/Zotero/storage/PH3QB6KI/Sutton - 1988 - Learning to predict by the methods of temporal dif.pdf}
}

@article{watkinsQlearning1992,
  langid = {english},
  title = {Q-Learning},
  volume = {8},
  issn = {1573-0565},
  url = {https://doi.org/10.1007/BF00992698},
  doi = {10.1007/BF00992698},
  abstract = {Q-learning (Watkins, 1989) is a simple way for agents to learn how to act optimally in controlled Markovian domains. It amounts to an incremental method for dynamic programming which imposes limited computational demands. It works by successively improving its evaluations of the quality of particular actions at particular states.This paper presents and proves in detail a convergence theorem forQ-learning based on that outlined in Watkins (1989). We show thatQ-learning converges to the optimum action-values with probability 1 so long as all actions are repeatedly sampled in all states and the action-values are represented discretely. We also sketch extensions to the cases of non-discounted, but absorbing, Markov environments, and where manyQ values can be changed each iteration, rather than just one.},
  number = {3},
  journaltitle = {Machine Learning},
  shortjournal = {Mach Learn},
  urldate = {2019-01-21},
  date = {1992-05-01},
  pages = {279-292},
  keywords = {asynchronous dynamic programming,Q-learning,reinforcement learning,temporal differences},
  author = {Watkins, Christopher J. C. H. and Dayan, Peter},
  file = {/home/dimitri/Nextcloud/Zotero/storage/T2KCK4HZ/Watkins and Dayan - 1992 - Q-learning.pdf}
}

@article{williamsSimpleStatisticalGradientfollowing1992,
  langid = {english},
  title = {Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning},
  volume = {8},
  issn = {1573-0565},
  url = {https://doi.org/10.1007/BF00992696},
  doi = {10.1007/BF00992696},
  abstract = {This article presents a general class of associative reinforcement learning algorithms for connectionist networks containing stochastic units. These algorithms, called REINFORCE algorithms, are shown to make weight adjustments in a direction that lies along the gradient of expected reinforcement in both immediate-reinforcement tasks and certain limited forms of delayed-reinforcement tasks, and they do this without explicitly computing gradient estimates or even storing information from which such estimates could be computed. Specific examples of such algorithms are presented, some of which bear a close relationship to certain existing algorithms while others are novel but potentially interesting in their own right. Also given are results that show how such algorithms can be naturally integrated with backpropagation. We close with a brief discussion of a number of additional issues surrounding the use of such algorithms, including what is known about their limiting behaviors as well as further considerations that might be used to help develop similar but potentially more powerful reinforcement learning algorithms.},
  number = {3},
  journaltitle = {Machine Learning},
  shortjournal = {Mach Learn},
  urldate = {2019-01-21},
  date = {1992-05-01},
  pages = {229-256},
  keywords = {Reinforcement learning,connectionist networks,gradient descent,mathematical analysis},
  author = {Williams, Ronald J.},
  file = {/home/dimitri/Nextcloud/Zotero/storage/GT76XSQD/Williams - 1992 - Simple statistical gradient-following algorithms f.pdf}
}

@article{moorePrioritizedSweepingReinforcement1993,
  langid = {english},
  title = {Prioritized Sweeping: {{Reinforcement}} Learning with Less Data and Less Time},
  volume = {13},
  issn = {1573-0565},
  url = {https://doi.org/10.1007/BF00993104},
  doi = {10.1007/BF00993104},
  shorttitle = {Prioritized Sweeping},
  abstract = {We present a new algorithm,prioritized sweeping, for efficient prediction and control of stochastic Markov systems. Incremental learning methods such as temporal differencing and Q-learning have real-time performance. Classical methods are slower, but more accurate, because they make full use of the observations. Prioritized sweeping aims for the best of both worlds. It uses all previous experiences both to prioritize important dynamic programming sweeps and to guide the exploration of state-space. We compare prioritized sweeping with other reinforcement learning schemes for a number of different stochastic optimal control problems. It successfully solves large state-space real-time problems with which other methods have difficulty.},
  number = {1},
  journaltitle = {Machine Learning},
  shortjournal = {Mach Learn},
  urldate = {2019-01-21},
  date = {1993-10-01},
  pages = {103-130},
  keywords = {asynchronous dynamic programming,reinforcement learning,heuristic search,learning control,Memory-based learning,prioritized sweeping,temporal differencing},
  author = {Moore, Andrew W. and Atkeson, Christopher G.},
  file = {/home/dimitri/Nextcloud/Zotero/storage/T6UITRMF/Moore and Atkeson - 1993 - Prioritized sweeping Reinforcement learning with .pdf}
}

@incollection{littmanMarkovGamesFramework1994,
  langid = {english},
  title = {Markov Games as a Framework for Multi-Agent Reinforcement Learning},
  isbn = {978-1-55860-335-6},
  url = {https://linkinghub.elsevier.com/retrieve/pii/B9781558603356500271},
  booktitle = {Machine {{Learning Proceedings}} 1994},
  publisher = {{Elsevier}},
  urldate = {2019-01-21},
  date = {1994},
  pages = {157-163},
  author = {Littman, Michael L.},
  file = {/home/dimitri/Nextcloud/Zotero/storage/PMKIBFF2/Littman - 1994 - Markov games as a framework for multi-agent reinfo.pdf},
  doi = {10.1016/B978-1-55860-335-6.50027-1}
}

@incollection{boyanGeneralizationReinforcementLearning1995,
  title = {Generalization in {{Reinforcement Learning}}: {{Safely Approximating}} the {{Value Function}}},
  url = {http://papers.nips.cc/paper/1018-generalization-in-reinforcement-learning-safely-approximating-the-value-function.pdf},
  shorttitle = {Generalization in {{Reinforcement Learning}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 7},
  publisher = {{MIT Press}},
  urldate = {2019-01-21},
  date = {1995},
  pages = {369--376},
  author = {Boyan, Justin A. and Moore, Andrew W.},
  editor = {Tesauro, G. and Touretzky, D. S. and Leen, T. K.},
  file = {/home/dimitri/Nextcloud/Zotero/storage/DX2RVAK7/Boyan and Moore - 1995 - Generalization in Reinforcement Learning Safely A.pdf;/home/dimitri/Nextcloud/Zotero/storage/96D26GVQ/1018-generalization-in-reinforcement-learning-safely-approximating-the-value-function.html}
}

@article{littmanComplexitySolvingMarkov2013,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1302.4971},
  primaryClass = {cs},
  title = {On the {{Complexity}} of {{Solving Markov Decision Problems}}},
  url = {http://arxiv.org/abs/1302.4971},
  abstract = {Markov decision problems (MDPs) provide the foundations for a number of problems of interest to AI researchers studying automated planning and reinforcement learning. In this paper, we summarize results regarding the complexity of solving MDPs and the running time of MDP solution algorithms. We argue that, although MDPs can be solved efficiently in theory, more study is needed to reveal practical algorithms for solving large problems quickly. To encourage future research, we sketch some alternative methods of analysis that rely on the structure of MDPs.},
  urldate = {2019-01-21},
  date = {2013-02-20},
  keywords = {Computer Science - Artificial Intelligence},
  author = {Littman, Michael L. and Dean, Thomas L. and Kaelbling, Leslie Pack},
  file = {/home/dimitri/Nextcloud/Zotero/storage/R6IPHFQW/Littman et al. - 2013 - On the Complexity of Solving Markov Decision Probl.pdf;/home/dimitri/Nextcloud/Zotero/storage/7NDC6AA9/1302.html}
}

@article{kaelblingPlanningActingPartially1998,
  langid = {english},
  title = {Planning and Acting in Partially Observable Stochastic Domains},
  volume = {101},
  issn = {00043702},
  url = {http://linkinghub.elsevier.com/retrieve/pii/S000437029800023X},
  doi = {10.1016/S0004-3702(98)00023-X},
  number = {1-2},
  journaltitle = {Artificial Intelligence},
  urldate = {2019-01-21},
  date = {1998-05},
  pages = {99-134},
  author = {Kaelbling, Leslie Pack and Littman, Michael L. and Cassandra, Anthony R.},
  file = {/home/dimitri/Nextcloud/Zotero/storage/GEHKNS3V/Kaelbling et al. - 1998 - Planning and acting in partially observable stocha.pdf}
}

@incollection{suttonPolicyGradientMethods2000,
  title = {Policy {{Gradient Methods}} for {{Reinforcement Learning}} with {{Function Approximation}}},
  url = {http://papers.nips.cc/paper/1713-policy-gradient-methods-for-reinforcement-learning-with-function-approximation.pdf},
  booktitle = {Advances in {{Neural Information Processing Systems}} 12},
  publisher = {{MIT Press}},
  urldate = {2019-01-21},
  date = {2000},
  pages = {1057--1063},
  author = {Sutton, Richard S and McAllester, David A. and Singh, Satinder P. and Mansour, Yishay},
  editor = {Solla, S. A. and Leen, T. K. and Müller, K.},
  file = {/home/dimitri/Nextcloud/Zotero/storage/A8I263XZ/Sutton et al. - 2000 - Policy Gradient Methods for Reinforcement Learning.pdf;/home/dimitri/Nextcloud/Zotero/storage/DNSWFUB7/1713-policy-gradient-methods-for-reinforcement-learning-with-function-approximation.html}
}

@article{dietterichHierarchicalReinforcementLearning2000,
  langid = {english},
  title = {Hierarchical {{Reinforcement Learning}} with the {{MAXQ Value Function Decomposition}}},
  volume = {13},
  issn = {1076-9757},
  url = {https://www.jair.org/index.php/jair/article/view/10266},
  doi = {10.1613/jair.639},
  journaltitle = {Journal of Artificial Intelligence Research},
  shortjournal = {1},
  urldate = {2019-01-21},
  date = {2000-11-01},
  pages = {227-303},
  author = {Dietterich, T. G.},
  file = {/home/dimitri/Nextcloud/Zotero/storage/7S9KX8PT/Dietterich - 2000 - Hierarchical Reinforcement Learning with the MAXQ .pdf;/home/dimitri/Nextcloud/Zotero/storage/QC75TU4I/10266.html}
}

@article{kearnsNearOptimalReinforcementLearning2002,
  langid = {english},
  title = {Near-{{Optimal Reinforcement Learning}} in {{Polynomial Time}}},
  volume = {49},
  issn = {1573-0565},
  url = {https://doi.org/10.1023/A:1017984413808},
  doi = {10.1023/A:1017984413808},
  abstract = {We present new algorithms for reinforcement learning and prove that they have polynomial bounds on the resources required to achieve near-optimal return in general Markov decision processes. After observing that the number of actions required to approach the optimal return is lower bounded by the mixing time T of the optimal policy (in the undiscounted case) or by the horizon time T (in the discounted case), we then give algorithms requiring a number of actions and total computation time that are only polynomial in T and the number of states and actions, for both the undiscounted and discounted cases. An interesting aspect of our algorithms is their explicit handling of the Exploration-Exploitation trade-off.},
  number = {2},
  journaltitle = {Machine Learning},
  shortjournal = {Machine Learning},
  urldate = {2019-01-21},
  date = {2002-11-01},
  pages = {209-232},
  keywords = {reinforcement learning,exploration versus exploitation,Markov decision processes},
  author = {Kearns, Michael and Singh, Satinder},
  file = {/home/dimitri/Nextcloud/Zotero/storage/QHHJB8YE/Kearns and Singh - 2002 - Near-Optimal Reinforcement Learning in Polynomial .pdf}
}

@article{brafmanRMAXGeneralPolynomial2002,
  title = {R-{{MAX}} - {{A General Polynomial Time Algorithm}} for {{Near}}-{{Optimal Reinforcement Learning}}},
  volume = {3},
  issn = {ISSN 1533-7928},
  url = {http://www.jmlr.org/papers/v3/brafman02a.html},
  issue = {Oct},
  journaltitle = {Journal of Machine Learning Research},
  urldate = {2019-01-21},
  date = {2002},
  pages = {213-231},
  author = {Brafman, Ronen I. and Tennenholtz, Moshe},
  file = {/home/dimitri/Nextcloud/Zotero/storage/JL529I5C/Brafman and Tennenholtz - 2002 - R-MAX - A General Polynomial Time Algorithm for Ne.pdf;/home/dimitri/Nextcloud/Zotero/storage/FVS2PYIW/brafman02a.html}
}

@inproceedings{abbeelApprenticeshipLearningInverse2004,
  langid = {english},
  location = {{Banff, Alberta, Canada}},
  title = {Apprenticeship Learning via Inverse Reinforcement Learning},
  url = {http://portal.acm.org/citation.cfm?doid=1015330.1015430},
  doi = {10.1145/1015330.1015430},
  eventtitle = {Twenty-First International Conference},
  booktitle = {Twenty-First International Conference on {{Machine}} Learning  - {{ICML}} '04},
  publisher = {{ACM Press}},
  urldate = {2019-01-21},
  date = {2004},
  pages = {1},
  author = {Abbeel, Pieter and Ng, Andrew Y.},
  file = {/home/dimitri/Nextcloud/Zotero/storage/5IWVWVEX/Abbeel and Ng - 2004 - Apprenticeship learning via inverse reinforcement .pdf}
}

@article{suttonMDPsSemiMDPsFramework1999,
  title = {Between {{MDPs}} and Semi-{{MDPs}}: {{A}} Framework for Temporal Abstraction in Reinforcement Learning},
  volume = {112},
  issn = {0004-3702},
  url = {http://www.sciencedirect.com/science/article/pii/S0004370299000521},
  doi = {10.1016/S0004-3702(99)00052-1},
  shorttitle = {Between {{MDPs}} and Semi-{{MDPs}}},
  abstract = {Learning, planning, and representing knowledge at multiple levels of temporal abstraction are key, longstanding challenges for AI. In this paper we consider how these challenges can be addressed within the mathematical framework of reinforcement learning and Markov decision processes (MDPs). We extend the usual notion of action in this framework to include options—closed-loop policies for taking action over a period of time. Examples of options include picking up an object, going to lunch, and traveling to a distant city, as well as primitive actions such as muscle twitches and joint torques. Overall, we show that options enable temporally abstract knowledge and action to be included in the reinforcement learning framework in a natural and general way. In particular, we show that options may be used interchangeably with primitive actions in planning methods such as dynamic programming and in learning methods such as Q-learning. Formally, a set of options defined over an MDP constitutes a semi-Markov decision process (SMDP), and the theory of SMDPs provides the foundation for the theory of options. However, the most interesting issues concern the interplay between the underlying MDP and the SMDP and are thus beyond SMDP theory. We present results for three such cases: (1) we show that the results of planning with options can be used during execution to interrupt options and thereby perform even better than planned, (2) we introduce new intra-option methods that are able to learn about an option from fragments of its execution, and (3) we propose a notion of subgoal that can be used to improve the options themselves. All of these results have precursors in the existing literature; the contribution of this paper is to establish them in a simpler and more general setting with fewer changes to the existing reinforcement learning framework. In particular, we show that these results can be obtained without committing to (or ruling out) any particular approach to state abstraction, hierarchy, function approximation, or the macro-utility problem.},
  number = {1},
  journaltitle = {Artificial Intelligence},
  shortjournal = {Artificial Intelligence},
  urldate = {2019-01-21},
  date = {1999-08-01},
  pages = {181-211},
  keywords = {Reinforcement learning,Markov decision processes,Hierarchical planning,Intra-option learning,Macroactions,Macros,Options,Semi-Markov decision processes,Subgoals,Temporal abstraction},
  author = {Sutton, Richard S. and Precup, Doina and Singh, Satinder},
  file = {/home/dimitri/Nextcloud/Zotero/storage/IHV6BCZC/Sutton et al. - 1999 - Between MDPs and semi-MDPs A framework for tempor.pdf;/home/dimitri/Nextcloud/Zotero/storage/NSVZFQGZ/S0004370299000521.html}
}

@inproceedings{ngPolicyInvarianceReward1999,
  location = {{San Francisco, CA, USA}},
  title = {Policy {{Invariance Under Reward Transformations}}: {{Theory}} and {{Application}} to {{Reward Shaping}}},
  isbn = {978-1-55860-612-8},
  url = {http://dl.acm.org/citation.cfm?id=645528.657613},
  shorttitle = {Policy {{Invariance Under Reward Transformations}}},
  booktitle = {Proceedings of the {{Sixteenth International Conference}} on {{Machine Learning}}},
  series = {{{ICML}} '99},
  publisher = {{Morgan Kaufmann Publishers Inc.}},
  urldate = {2019-01-21},
  date = {1999},
  pages = {278--287},
  author = {Ng, Andrew Y. and Harada, Daishi and Russell, Stuart J.},
  file = {/home/dimitri/Nextcloud/Zotero/storage/EFCC73Q2/Ng et al. - 1999 - Policy Invariance Under Reward Transformations Th.pdf}
}

@article{francois-lavetCombinedReinforcementLearning2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1809.04506},
  primaryClass = {cs, stat},
  title = {Combined {{Reinforcement Learning}} via {{Abstract Representations}}},
  url = {http://arxiv.org/abs/1809.04506},
  abstract = {In the quest for efficient and robust reinforcement learning methods, both model-free and model-based approaches offer advantages. In this paper we propose a new way of explicitly bridging both approaches via a shared low-dimensional learned encoding of the environment, meant to capture summarizing abstractions. We show that the modularity brought by this approach leads to good generalization while being computationally efficient, with planning happening in a smaller latent state space. In addition, this approach recovers a sufficient low-dimensional representation of the environment, which opens up new strategies for interpretable AI, exploration and transfer learning.},
  urldate = {2019-01-24},
  date = {2018-09-12},
  keywords = {Statistics - Machine Learning,Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  author = {François-Lavet, Vincent and Bengio, Yoshua and Precup, Doina and Pineau, Joelle},
  file = {/home/dimitri/Nextcloud/Zotero/storage/CI9TT4HP/François-Lavet et al. - 2018 - Combined Reinforcement Learning via Abstract Repre.pdf;/home/dimitri/Nextcloud/Zotero/storage/9HFUG4CY/1809.html}
}

@article{leiGeometricUnderstandingDeep2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1805.10451},
  primaryClass = {cs, stat},
  title = {Geometric {{Understanding}} of {{Deep Learning}}},
  url = {http://arxiv.org/abs/1805.10451},
  abstract = {Deep learning is the mainstream technique for many machine learning tasks, including image recognition, machine translation, speech recognition, and so on. It has outperformed conventional methods in various fields and achieved great successes. Unfortunately, the understanding on how it works remains unclear. It has the central importance to lay down the theoretic foundation for deep learning. In this work, we give a geometric view to understand deep learning: we show that the fundamental principle attributing to the success is the manifold structure in data, namely natural high dimensional data concentrates close to a low-dimensional manifold, deep learning learns the manifold and the probability distribution on it. We further introduce the concepts of rectified linear complexity for deep neural network measuring its learning capability, rectified linear complexity of an embedding manifold describing the difficulty to be learned. Then we show for any deep neural network with fixed architecture, there exists a manifold that cannot be learned by the network. Finally, we propose to apply optimal mass transportation theory to control the probability distribution in the latent space.},
  urldate = {2019-01-24},
  date = {2018-05-26},
  keywords = {Statistics - Machine Learning,Computer Science - Machine Learning},
  author = {Lei, Na and Luo, Zhongxuan and Yau, Shing-Tung and Gu, David Xianfeng},
  file = {/home/dimitri/Nextcloud/Zotero/storage/JFRTDIWJ/Lei et al. - 2018 - Geometric Understanding of Deep Learning.pdf;/home/dimitri/Nextcloud/Zotero/storage/WLAUH4C4/1805.html}
}

@article{bloem-reddyProbabilisticSymmetryInvariant2019,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1901.06082},
  primaryClass = {cs, stat},
  title = {Probabilistic Symmetry and Invariant Neural Networks},
  url = {http://arxiv.org/abs/1901.06082},
  abstract = {In an effort to improve the performance of deep neural networks in data-scarce, non-i.i.d., or unsupervised settings, much recent research has been devoted to encoding invariance under symmetry transformations into neural network architectures. We treat the neural network input and output as random variables, and consider group invariance from the perspective of probabilistic symmetry. Drawing on tools from probability and statistics, we establish a link between functional and probabilistic symmetry, and obtain generative functional representations of joint and conditional probability distributions that are invariant or equivariant under the action of a compact group. Those representations completely characterize the structure of neural networks that can be used to model such distributions and yield a general program for constructing invariant stochastic or deterministic neural networks. We develop the details of the general program for exchangeable sequences and arrays, recovering a number of recent examples as special cases.},
  urldate = {2019-01-25},
  date = {2019-01-17},
  keywords = {Statistics - Machine Learning,Computer Science - Machine Learning},
  author = {Bloem-Reddy, Benjamin and Teh, Yee Whye},
  file = {/home/dimitri/Nextcloud/Zotero/storage/86G4UXEK/Bloem-Reddy and Teh - 2019 - Probabilistic symmetry and invariant neural networ.pdf;/home/dimitri/Nextcloud/Zotero/storage/GV3VFG7K/1901.html}
}

@article{mathieuHierarchicalRepresentationsPoincare2019,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1901.06033},
  primaryClass = {cs, stat},
  title = {Hierarchical {{Representations}} with {{Poincaré Variational Auto}}-{{Encoders}}},
  url = {http://arxiv.org/abs/1901.06033},
  abstract = {The Variational Auto-Encoder (VAE) model has become widely popular as a way to learn at once a generative model and embeddings for observations living in a high-dimensional space. In the real world, many such observations may be assumed to be hierarchically structured, such as living organisms data which are related through the evolutionary tree. Also, it has been theoretically and empirically shown that data with hierarchical structure can efficiently be embedded in hyperbolic spaces. We therefore endow the VAE with a hyperbolic geometry and empirically show that it can better generalise to unseen data than its Euclidean counterpart, and can qualitatively recover the hierarchical structure.},
  urldate = {2019-01-25},
  date = {2019-01-17},
  keywords = {Statistics - Machine Learning,Computer Science - Machine Learning},
  author = {Mathieu, Emile and Lan, Charline Le and Maddison, Chris J. and Tomioka, Ryota and Teh, Yee Whye},
  file = {/home/dimitri/Nextcloud/Zotero/storage/28V97CG2/Mathieu et al. - 2019 - Hierarchical Representations with Poincar'e Varia.pdf;/home/dimitri/Nextcloud/Zotero/storage/5ZALERKH/1901.html}
}

@article{rezendeStochasticBackpropagationApproximate2014,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1401.4082},
  primaryClass = {cs, stat},
  title = {Stochastic {{Backpropagation}} and {{Approximate Inference}} in {{Deep Generative Models}}},
  url = {http://arxiv.org/abs/1401.4082},
  abstract = {We marry ideas from deep neural networks and approximate Bayesian inference to derive a generalised class of deep, directed generative models, endowed with a new algorithm for scalable inference and learning. Our algorithm introduces a recognition model to represent approximate posterior distributions, and that acts as a stochastic encoder of the data. We develop stochastic back-propagation -- rules for back-propagation through stochastic variables -- and use this to develop an algorithm that allows for joint optimisation of the parameters of both the generative and recognition model. We demonstrate on several real-world data sets that the model generates realistic samples, provides accurate imputations of missing data and is a useful tool for high-dimensional data visualisation.},
  urldate = {2019-01-25},
  date = {2014-01-16},
  keywords = {Statistics - Machine Learning,Computer Science - Artificial Intelligence,Statistics - Computation,Statistics - Methodology,Computer Science - Machine Learning},
  author = {Rezende, Danilo Jimenez and Mohamed, Shakir and Wierstra, Daan},
  file = {/home/dimitri/Nextcloud/Zotero/storage/KQQEGKHT/Rezende et al. - 2014 - Stochastic Backpropagation and Approximate Inferen.pdf;/home/dimitri/Nextcloud/Zotero/storage/DHSBCQPC/1401.html}
}

@article{levineReinforcementLearningControl2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1805.00909},
  primaryClass = {cs, stat},
  title = {Reinforcement {{Learning}} and {{Control}} as {{Probabilistic Inference}}: {{Tutorial}} and {{Review}}},
  url = {http://arxiv.org/abs/1805.00909},
  shorttitle = {Reinforcement {{Learning}} and {{Control}} as {{Probabilistic Inference}}},
  abstract = {The framework of reinforcement learning or optimal control provides a mathematical formalization of intelligent decision making that is powerful and broadly applicable. While the general form of the reinforcement learning problem enables effective reasoning about uncertainty, the connection between reinforcement learning and inference in probabilistic models is not immediately obvious. However, such a connection has considerable value when it comes to algorithm design: formalizing a problem as probabilistic inference in principle allows us to bring to bear a wide array of approximate inference tools, extend the model in flexible and powerful ways, and reason about compositionality and partial observability. In this article, we will discuss how a generalization of the reinforcement learning or optimal control problem, which is sometimes termed maximum entropy reinforcement learning, is equivalent to exact probabilistic inference in the case of deterministic dynamics, and variational inference in the case of stochastic dynamics. We will present a detailed derivation of this framework, overview prior work that has drawn on this and related ideas to propose new reinforcement learning and control algorithms, and describe perspectives on future research.},
  urldate = {2019-01-25},
  date = {2018-05-02},
  keywords = {Statistics - Machine Learning,Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics},
  author = {Levine, Sergey},
  file = {/home/dimitri/Nextcloud/Zotero/storage/X9L5E6TF/Levine - 2018 - Reinforcement Learning and Control as Probabilisti.pdf;/home/dimitri/Nextcloud/Zotero/storage/L8VYC66K/1805.html}
}

@article{couzinCollectiveMemorySpatial2002,
  title = {Collective {{Memory}} and {{Spatial Sorting}} in {{Animal Groups}}},
  volume = {218},
  issn = {0022-5193},
  url = {http://www.sciencedirect.com/science/article/pii/S0022519302930651},
  doi = {10.1006/jtbi.2002.3065},
  abstract = {We present a self-organizing model of group formation in three-dimensional space, and use it to investigate the spatial dynamics of animal groups such as fish schools and bird flocks. We reveal the existence of major group-level behavioural transitions related to minor changes in individual-level interactions. Further, we present the first evidence for collective memory in such animal groups (where the previous history of group structure influences the collective behaviour exhibited as individual interactions change) during the transition of a group from one type of collective behaviour to another. The model is then used to show how differences among individuals influence group structure, and how individuals employing simple, local rules of thumb, can accurately change their spatial position within a group (e.g. to move to the centre, the front, or the periphery) in the absence of information on their current position within the group as a whole. These results are considered in the context of the evolution and ecological importance of animal groups.},
  number = {1},
  journaltitle = {Journal of Theoretical Biology},
  shortjournal = {Journal of Theoretical Biology},
  urldate = {2019-01-26},
  date = {2002-09-07},
  pages = {1-11},
  author = {Couzin, IAIN D. and Krause, JENS and James, RICHARD and Ruxton, GRAEME D. and Franks, NIGEL R.},
  file = {/home/dimitri/Nextcloud/Zotero/storage/UNZ35LQH/Couzin et al. - 2002 - Collective Memory and Spatial Sorting in Animal Gr.pdf;/home/dimitri/Nextcloud/Zotero/storage/VYUHH8L6/S0022519302930651.html}
}

@article{khodayarDeepGenerativeModel2019,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1901.09674},
  primaryClass = {cs, stat},
  title = {A {{Deep Generative Model}} for {{Graphs}}: {{Supervised Subset Selection}} to {{Create Diverse Realistic Graphs}} with {{Applications}} to {{Power Networks Synthesis}}},
  url = {http://arxiv.org/abs/1901.09674},
  shorttitle = {A {{Deep Generative Model}} for {{Graphs}}},
  abstract = {Creating and modeling real-world graphs is a crucial problem in various applications of engineering, biology, and social sciences; however, learning the distributions of nodes/edges and sampling from them to generate realistic graphs is still challenging. Moreover, generating a diverse set of synthetic graphs that all imitate a real network is not addressed. In this paper, the novel problem of creating diverse synthetic graphs is solved. First, we devise the deep supervised subset selection (DeepS3) algorithm; Given a ground-truth set of data points, DeepS3 selects a diverse subset of all items (i.e. data points) that best represent the items in the ground-truth set. Furthermore, we propose the deep graph representation recurrent network (GRRN) as a novel generative model that learns a probabilistic representation of a real weighted graph. Training the GRRN, we generate a large set of synthetic graphs that are likely to follow the same features and adjacency patterns as the original one. Incorporating GRRN with DeepS3, we select a diverse subset of generated graphs that best represent the behaviors of the real graph (i.e. our ground-truth). We apply our model to the novel problem of power grid synthesis, where a synthetic power network is created with the same physical/geometric properties as a real power system without revealing the real locations of the substations (nodes) and the lines (edges), since such data is confidential. Experiments on the Synthetic Power Grid Data Set show accurate synthetic networks that follow similar structural and spatial properties as the real power grid.},
  urldate = {2019-01-30},
  date = {2019-01-17},
  keywords = {Statistics - Machine Learning,Computer Science - Social and Information Networks,Computer Science - Machine Learning},
  author = {Khodayar, Mahdi and Wang, Jianhui and Wang, Zhaoyu},
  file = {/home/dimitri/Nextcloud/Zotero/storage/P9U5UN3K/Khodayar et al. - 2019 - A Deep Generative Model for Graphs Supervised Sub.pdf;/home/dimitri/Nextcloud/Zotero/storage/6P37TCZ6/1901.html}
}

@article{allenAnalogiesExplainedUnderstanding2019,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1901.09813},
  primaryClass = {cs, stat},
  title = {Analogies {{Explained}}: {{Towards Understanding Word Embeddings}}},
  url = {http://arxiv.org/abs/1901.09813},
  shorttitle = {Analogies {{Explained}}},
  abstract = {Word embeddings generated by neural network methods such as word2vec (W2V) are well known to exhibit seemingly linear behaviour, e.g. the embeddings of analogy "woman is to queen as man is to king" approximately describe a parallelogram. This property is particularly intriguing since the embeddings are not trained to achieve it. Several explanations have been proposed, but each introduces assumptions that do not hold in practice. We derive a probabilistically grounded definition of paraphrasing and show it can be re-interpreted as word transformation, a mathematical description of "\$w\_x\$ is to \$w\_y\$". From these concepts we prove existence of the linear relationship between W2V-type embeddings that underlies the analogical phenomenon, and identify explicit error terms in the relationship.},
  urldate = {2019-01-30},
  date = {2019-01-28},
  keywords = {Statistics - Machine Learning,Computer Science - Computation and Language,Computer Science - Machine Learning},
  author = {Allen, Carl and Hospedales, Timothy},
  file = {/home/dimitri/Nextcloud/Zotero/storage/ABPGTHMK/Allen and Hospedales - 2019 - Analogies Explained Towards Understanding Word Em.pdf;/home/dimitri/Nextcloud/Zotero/storage/ZWMNAD4J/1901.html}
}

@article{whittingtonTheoriesErrorBackPropagation2019,
  langid = {english},
  title = {Theories of {{Error Back}}-{{Propagation}} in the {{Brain}}},
  volume = {0},
  issn = {1364-6613, 1879-307X},
  url = {https://www.cell.com/trends/cognitive-sciences/abstract/S1364-6613(19)30012-9},
  doi = {10.1016/j.tics.2018.12.005},
  number = {0},
  journaltitle = {Trends in Cognitive Sciences},
  shortjournal = {Trends in Cognitive Sciences},
  urldate = {2019-01-30},
  date = {2019-01-28},
  keywords = {deep learning,neural networks,predictive coding,synaptic plasticity},
  author = {Whittington, James C. R. and Bogacz, Rafal},
  file = {/home/dimitri/Nextcloud/Zotero/storage/ARBKG2GF/Whittington and Bogacz - 2019 - Theories of Error Back-Propagation in the Brain.pdf;/home/dimitri/Nextcloud/Zotero/storage/QPKN64NE/S1364-6613(19)30012-9.html}
}

@article{aroraSimpleToughtoBeatBaseline2016,
  title = {A {{Simple}} but {{Tough}}-to-{{Beat Baseline}} for {{Sentence Embeddings}}},
  url = {https://openreview.net/forum?id=SyK00v5xx},
  abstract = {The success of neural network methods for computing word embeddings has motivated methods for generating semantic embeddings of longer pieces of text, such as sentences and paragraphs....},
  urldate = {2019-01-30},
  date = {2016-11-04},
  author = {Arora, Sanjeev and Liang, Yingyu and Ma, Tengyu},
  file = {/home/dimitri/Nextcloud/Zotero/storage/SQDBBR98/Arora et al. - 2016 - A Simple but Tough-to-Beat Baseline for Sentence E.pdf;/home/dimitri/Nextcloud/Zotero/storage/B8V8H9DR/forum.html}
}

@article{hebert-dufresneMultiscaleStructureTopological2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1510.08542},
  title = {Multi-Scale Structure and Topological Anomaly Detection via a New Network Statistic: {{The}} Onion Decomposition},
  volume = {6},
  issn = {2045-2322},
  url = {http://arxiv.org/abs/1510.08542},
  doi = {10.1038/srep31708},
  shorttitle = {Multi-Scale Structure and Topological Anomaly Detection via a New Network Statistic},
  abstract = {We introduce a new network statistic that measures diverse structural properties at the micro-, meso-, and macroscopic scales, while still being easy to compute and easy to interpret at a glance. Our statistic, the onion spectrum, is based on the onion decomposition, which refines the k-core decomposition, a standard network fingerprinting method. The onion spectrum is exactly as easy to compute as the k-cores: It is based on the stages at which each vertex gets removed from a graph in the standard algorithm for computing the k-cores. But the onion spectrum reveals much more information about a network, and at multiple scales; for example, it can be used to quantify node heterogeneity, degree correlations, centrality, and tree- or lattice-likeness of the whole network as well as of each k-core. Furthermore, unlike the k-core decomposition, the combined degree-onion spectrum immediately gives a clear local picture of the network around each node which allows the detection of interesting subgraphs whose topological structure differs from the global network organization. This local description can also be leveraged to easily generate samples from the ensemble of networks with a given joint degree-onion distribution. We demonstrate the utility of the onion spectrum for understanding both static and dynamic properties on several standard graph models and on many real-world networks.},
  number = {1},
  journaltitle = {Scientific Reports},
  urldate = {2019-01-30},
  date = {2016-10},
  keywords = {Physics - Physics and Society,Computer Science - Social and Information Networks,Condensed Matter - Disordered Systems and Neural Networks,Computer Science - Discrete Mathematics,Mathematics - Combinatorics},
  author = {Hébert-Dufresne, Laurent and Grochow, Joshua A. and Allard, Antoine},
  file = {/home/dimitri/Nextcloud/Zotero/storage/V6WP4EGA/Hébert-Dufresne et al. - 2016 - Multi-scale structure and topological anomaly dete.pdf;/home/dimitri/Nextcloud/Zotero/storage/UWCVXDE4/1510.html}
}

@article{hendrycksBaselineDetectingMisclassified2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1610.02136},
  primaryClass = {cs},
  title = {A {{Baseline}} for {{Detecting Misclassified}} and {{Out}}-of-{{Distribution Examples}} in {{Neural Networks}}},
  url = {http://arxiv.org/abs/1610.02136},
  abstract = {We consider the two related problems of detecting if an example is misclassified or out-of-distribution. We present a simple baseline that utilizes probabilities from softmax distributions. Correctly classified examples tend to have greater maximum softmax probabilities than erroneously classified and out-of-distribution examples, allowing for their detection. We assess performance by defining several tasks in computer vision, natural language processing, and automatic speech recognition, showing the effectiveness of this baseline across all. We then show the baseline can sometimes be surpassed, demonstrating the room for future research on these underexplored detection tasks.},
  urldate = {2019-01-30},
  date = {2016-10-07},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  author = {Hendrycks, Dan and Gimpel, Kevin},
  file = {/home/dimitri/Nextcloud/Zotero/storage/KDTHENRP/Hendrycks and Gimpel - 2016 - A Baseline for Detecting Misclassified and Out-of-.pdf;/home/dimitri/Nextcloud/Zotero/storage/JRETA5IS/1610.html}
}

@article{nalisnickDeepGenerativeModels2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1810.09136},
  primaryClass = {cs, stat},
  title = {Do {{Deep Generative Models Know What They Don}}'t {{Know}}?},
  url = {http://arxiv.org/abs/1810.09136},
  abstract = {A neural network deployed in the wild may be asked to make predictions for inputs that were drawn from a different distribution than that of the training data. A plethora of work has demonstrated that it is easy to find or synthesize inputs for which a neural network is highly confident yet wrong. Generative models are widely viewed to be robust to such mistaken confidence as modeling the density of the input features can be used to detect novel, out-of-distribution inputs. In this paper we challenge this assumption. We find that the density learned by flow-based models, VAEs, and PixelCNNs cannot distinguish images of common objects such as dogs, trucks, and horses (i.e. CIFAR-10) from those of house numbers (i.e. SVHN), assigning a higher likelihood to the latter when the model is trained on the former. Moreover, we find evidence of this phenomenon when pairing several popular image data sets: FashionMNIST vs MNIST, CelebA vs SVHN, ImageNet vs CIFAR-10 / CIFAR-100 / SVHN. To investigate this curious behavior, we focus analysis on flow-based generative models in particular since they are trained and evaluated via the exact marginal likelihood. We find such behavior persists even when we restrict the flow models to constant-volume transformations. These transformations admit some theoretical analysis, and we show that the difference in likelihoods can be explained by the location and variances of the data and the model curvature. Our results caution against using the density estimates from deep generative models to identify inputs similar to the training distribution until their behavior for out-of-distribution inputs is better understood.},
  urldate = {2019-01-30},
  date = {2018-10-22},
  keywords = {Statistics - Machine Learning,Computer Science - Machine Learning},
  author = {Nalisnick, Eric and Matsukawa, Akihiro and Teh, Yee Whye and Gorur, Dilan and Lakshminarayanan, Balaji},
  file = {/home/dimitri/Nextcloud/Zotero/storage/KTJ6QI4W/Nalisnick et al. - 2018 - Do Deep Generative Models Know What They Don't Kno.pdf;/home/dimitri/Nextcloud/Zotero/storage/K95R8HC9/1810.html}
}

@article{daiTransformerXLAttentiveLanguage2019,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1901.02860},
  primaryClass = {cs, stat},
  title = {Transformer-{{XL}}: {{Attentive Language Models Beyond}} a {{Fixed}}-{{Length Context}}},
  url = {http://arxiv.org/abs/1901.02860},
  shorttitle = {Transformer-{{XL}}},
  abstract = {Transformer networks have a potential of learning longer-term dependency, but are limited by a fixed-length context in the setting of language modeling. As a solution, we propose a novel neural architecture, Transformer-XL, that enables Transformer to learn dependency beyond a fixed length without disrupting temporal coherence. Concretely, it consists of a segment-level recurrence mechanism and a novel positional encoding scheme. Our method not only enables capturing longer-term dependency, but also resolves the problem of context fragmentation. As a result, Transformer-XL learns dependency that is about 80\% longer than RNNs and 450\% longer than vanilla Transformers, achieves better performance on both short and long sequences, and is up to 1,800+ times faster than vanilla Transformer during evaluation. Additionally, we improve the state-of-the-art (SoTA) results of bpc/perplexity from 1.06 to 0.99 on enwiki8, from 1.13 to 1.08 on text8, from 20.5 to 18.3 on WikiText-103, from 23.7 to 21.8 on One Billion Word, and from 55.3 to 54.5 on Penn Treebank (without finetuning). Our code, pretrained models, and hyperparameters are available in both Tensorflow and PyTorch.},
  urldate = {2019-01-30},
  date = {2019-01-09},
  keywords = {Statistics - Machine Learning,Computer Science - Computation and Language,Computer Science - Machine Learning},
  author = {Dai, Zihang and Yang, Zhilin and Yang, Yiming and Carbonell, Jaime and Le, Quoc V. and Salakhutdinov, Ruslan},
  file = {/home/dimitri/Nextcloud/Zotero/storage/SR686JDP/Dai et al. - 2019 - Transformer-XL Attentive Language Models Beyond a.pdf;/home/dimitri/Nextcloud/Zotero/storage/CHWMM3TL/1901.html}
}

@article{artetxeMassivelyMultilingualSentence2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1812.10464},
  primaryClass = {cs},
  title = {Massively {{Multilingual Sentence Embeddings}} for {{Zero}}-{{Shot Cross}}-{{Lingual Transfer}} and {{Beyond}}},
  url = {http://arxiv.org/abs/1812.10464},
  abstract = {We introduce an architecture to learn joint multilingual sentence representations for 93 languages, belonging to more than 30 different language families and written in 28 different scripts. Our system uses a single BiLSTM encoder with a shared BPE vocabulary for all languages, which is coupled with an auxiliary decoder and trained on publicly available parallel corpora. This enables us to learn a classifier on top of the resulting sentence embeddings using English annotated data only, and transfer it to any of the 93 languages without any modification. Our approach sets a new state-of-the-art on zero-shot cross-lingual natural language inference for all the 14 languages in the XNLI dataset but one. We also achieve very competitive results in cross-lingual document classification (MLDoc dataset). Our sentence embeddings are also strong at parallel corpus mining, establishing a new state-of-the-art in the BUCC shared task for 3 of its 4 language pairs. Finally, we introduce a new test set of aligned sentences in 122 languages based on the Tatoeba corpus, and show that our sentence embeddings obtain strong results in multilingual similarity search even for low-resource languages. Our PyTorch implementation, pre-trained encoder and the multilingual test set will be freely available.},
  urldate = {2019-01-30},
  date = {2018-12-26},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  author = {Artetxe, Mikel and Schwenk, Holger},
  file = {/home/dimitri/Nextcloud/Zotero/storage/WNCIYWRP/Artetxe and Schwenk - 2018 - Massively Multilingual Sentence Embeddings for Zer.pdf;/home/dimitri/Nextcloud/Zotero/storage/CKGMT9NN/1812.html}
}

@article{gottliebNeuroscienceActiveSampling2018,
  langid = {english},
  title = {Towards a Neuroscience of Active Sampling and Curiosity},
  volume = {19},
  issn = {1471-003X, 1471-0048},
  url = {http://www.nature.com/articles/s41583-018-0078-0},
  doi = {10.1038/s41583-018-0078-0},
  number = {12},
  journaltitle = {Nature Reviews Neuroscience},
  urldate = {2019-01-30},
  date = {2018-12},
  pages = {758-770},
  author = {Gottlieb, Jacqueline and Oudeyer, Pierre-Yves},
  file = {/home/dimitri/Nextcloud/Zotero/storage/Q2IWBVRV/Gottlieb and Oudeyer - 2018 - Towards a neuroscience of active sampling and curi.pdf}
}

@article{kunchevaSpectralMultiscaleCommunity2019,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1901.10521},
  primaryClass = {physics, stat},
  title = {Spectral {{Multi}}-Scale {{Community Detection}} in {{Temporal Networks}} with an {{Application}}},
  url = {http://arxiv.org/abs/1901.10521},
  abstract = {The analysis of temporal networks has a wide area of applications in a world of technological advances. An important aspect of temporal network analysis is the discovery of community structures. Real data networks are often very large and the communities are observed to have a hierarchical structure referred to as multi-scale communities. Changes in the community structure over time might take place either at one scale or across all scales of the community structure. The multilayer formulation of the modularity maximization (MM) method introduced captures the changing multi-scale community structure of temporal networks. This method introduces a coupling between communities in neighboring time layers by allowing inter-layer connections, while different values of the resolution parameter enable the detection of multi-scale communities. However, the range of this parameter's values must be manually selected. When dealing with real life data, communities at one or more scales can go undiscovered if appropriate parameter ranges are not selected. A novel Temporal Multi-scale Community Detection (TMSCD) method overcomes the obstacles mentioned above. This is achieved by using the spectral properties of the temporal network represented as a multilayer network. In this framework we select automatically the range of relevant scales within which multi-scale community partitions are sought.},
  urldate = {2019-01-31},
  date = {2019-01-29},
  keywords = {Statistics - Machine Learning,Physics - Physics and Society,Computer Science - Social and Information Networks,Computer Science - Machine Learning},
  author = {Kuncheva, Zhana and Montana, Giovanni},
  file = {/home/dimitri/Nextcloud/Zotero/storage/2ZF7B936/Kuncheva and Montana - 2019 - Spectral Multi-scale Community Detection in Tempor.pdf;/home/dimitri/Nextcloud/Zotero/storage/424VQEEI/1901.html}
}

@article{williamsHybridCodeNetworks2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1702.03274},
  primaryClass = {cs},
  title = {Hybrid {{Code Networks}}: Practical and Efficient End-to-End Dialog Control with Supervised and Reinforcement Learning},
  url = {http://arxiv.org/abs/1702.03274},
  shorttitle = {Hybrid {{Code Networks}}},
  abstract = {End-to-end learning of recurrent neural networks (RNNs) is an attractive solution for dialog systems; however, current techniques are data-intensive and require thousands of dialogs to learn simple behaviors. We introduce Hybrid Code Networks (HCNs), which combine an RNN with domain-specific knowledge encoded as software and system action templates. Compared to existing end-to-end approaches, HCNs considerably reduce the amount of training data required, while retaining the key benefit of inferring a latent representation of dialog state. In addition, HCNs can be optimized with supervised learning, reinforcement learning, or a mixture of both. HCNs attain state-of-the-art performance on the bAbI dialog dataset, and outperform two commercially deployed customer-facing dialog systems.},
  urldate = {2019-02-01},
  date = {2017-02-10},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  author = {Williams, Jason D. and Asadi, Kavosh and Zweig, Geoffrey},
  file = {/home/dimitri/Nextcloud/Zotero/storage/D2SFF2BX/Williams et al. - 2017 - Hybrid Code Networks practical and efficient end-.pdf;/home/dimitri/Nextcloud/Zotero/storage/M4UT656D/1702.html}
}

@article{wuStarSpaceEmbedAll2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1709.03856},
  primaryClass = {cs},
  title = {{{StarSpace}}: {{Embed All The Things}}!},
  url = {http://arxiv.org/abs/1709.03856},
  shorttitle = {{{StarSpace}}},
  abstract = {We present StarSpace, a general-purpose neural embedding model that can solve a wide variety of problems: labeling tasks such as text classification, ranking tasks such as information retrieval/web search, collaborative filtering-based or content-based recommendation, embedding of multi-relational graphs, and learning word, sentence or document level embeddings. In each case the model works by embedding those entities comprised of discrete features and comparing them against each other -- learning similarities dependent on the task. Empirical results on a number of tasks show that StarSpace is highly competitive with existing methods, whilst also being generally applicable to new cases where those methods are not.},
  urldate = {2019-02-01},
  date = {2017-09-12},
  keywords = {Computer Science - Computation and Language},
  author = {Wu, Ledell and Fisch, Adam and Chopra, Sumit and Adams, Keith and Bordes, Antoine and Weston, Jason},
  file = {/home/dimitri/Nextcloud/Zotero/storage/BNUMM7ZZ/Wu et al. - 2017 - StarSpace Embed All The Things!.pdf;/home/dimitri/Nextcloud/Zotero/storage/VYH8J4GE/1709.html}
}

@book{santambrogioOptimalTransportApplied2015,
  location = {{Cham}},
  title = {Optimal {{Transport}} for {{Applied Mathematicians}}},
  volume = {87},
  isbn = {978-3-319-20827-5 978-3-319-20828-2},
  url = {http://link.springer.com/10.1007/978-3-319-20828-2},
  series = {Progress in {{Nonlinear Differential Equations}} and {{Their Applications}}},
  publisher = {{Springer International Publishing}},
  urldate = {2019-02-01},
  date = {2015},
  author = {Santambrogio, Filippo},
  file = {/home/dimitri/Nextcloud/Zotero/storage/8NHLGF5U/Santambrogio - 2015 - Optimal Transport for Applied Mathematicians.pdf},
  doi = {10.1007/978-3-319-20828-2}
}

@book{villaniOptimalTransportOld2009,
  location = {{Berlin}},
  title = {Optimal Transport: Old and New},
  isbn = {978-3-540-71049-3},
  shorttitle = {Optimal Transport},
  pagetotal = {973},
  number = {338},
  series = {Grundlehren Der Mathematischen {{Wissenschaften}}},
  publisher = {{Springer}},
  date = {2009},
  keywords = {Probabilities,Dynamics,Dynamique,Géométrie différentielle,Geometry; Differential,Mathematical optimization,Optimisation mathématique,Probabilités,Problèmes de transport (Programmation),Transportation problems (Programming)},
  author = {Villani, Cédric},
  file = {/home/dimitri/Nextcloud/Zotero/storage/XMWCC335/Villani - 2009 - Optimal transport old and new.pdf},
  note = {OCLC: ocn244421231}
}

@article{yogatamaLearningEvaluatingGeneral2019,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1901.11373},
  primaryClass = {cs, stat},
  title = {Learning and {{Evaluating General Linguistic Intelligence}}},
  url = {http://arxiv.org/abs/1901.11373},
  abstract = {We define general linguistic intelligence as the ability to reuse previously acquired knowledge about a language's lexicon, syntax, semantics, and pragmatic conventions to adapt to new tasks quickly. Using this definition, we analyze state-of-the-art natural language understanding models and conduct an extensive empirical investigation to evaluate them against these criteria through a series of experiments that assess the task-independence of the knowledge being acquired by the learning process. In addition to task performance, we propose a new evaluation metric based on an online encoding of the test data that quantifies how quickly an existing agent (model) learns a new task. Our results show that while the field has made impressive progress in terms of model architectures that generalize to many tasks, these models still require a lot of in-domain training examples (e.g., for fine tuning, training task-specific modules), and are prone to catastrophic forgetting. Moreover, we find that far from solving general tasks (e.g., document question answering), our models are overfitting to the quirks of particular datasets (e.g., SQuAD). We discuss missing components and conjecture on how to make progress toward general linguistic intelligence.},
  urldate = {2019-02-04},
  date = {2019-01-31},
  keywords = {Statistics - Machine Learning,Computer Science - Computation and Language,Computer Science - Machine Learning},
  author = {Yogatama, Dani and family=Autume, given=Cyprien de Masson, prefix=d', useprefix=true and Connor, Jerome and Kocisky, Tomas and Chrzanowski, Mike and Kong, Lingpeng and Lazaridou, Angeliki and Ling, Wang and Yu, Lei and Dyer, Chris and Blunsom, Phil},
  file = {/home/dimitri/Nextcloud/Zotero/storage/FP7K77IR/Yogatama et al. - 2019 - Learning and Evaluating General Linguistic Intelli.pdf;/home/dimitri/Nextcloud/Zotero/storage/8V7JCZPB/1901.html}
}

@article{nickelPoincareEmbeddingsLearning2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1705.08039},
  primaryClass = {cs, stat},
  title = {Poincaré {{Embeddings}} for {{Learning Hierarchical Representations}}},
  url = {http://arxiv.org/abs/1705.08039},
  abstract = {Representation learning has become an invaluable approach for learning from symbolic data such as text and graphs. However, while complex symbolic datasets often exhibit a latent hierarchical structure, state-of-the-art methods typically learn embeddings in Euclidean vector spaces, which do not account for this property. For this purpose, we introduce a new approach for learning hierarchical representations of symbolic data by embedding them into hyperbolic space -- or more precisely into an n-dimensional Poincar\textbackslash{}'e ball. Due to the underlying hyperbolic geometry, this allows us to learn parsimonious representations of symbolic data by simultaneously capturing hierarchy and similarity. We introduce an efficient algorithm to learn the embeddings based on Riemannian optimization and show experimentally that Poincar\textbackslash{}'e embeddings outperform Euclidean embeddings significantly on data with latent hierarchies, both in terms of representation capacity and in terms of generalization ability.},
  urldate = {2019-02-04},
  date = {2017-05-22},
  keywords = {Statistics - Machine Learning,Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  author = {Nickel, Maximilian and Kiela, Douwe},
  file = {/home/dimitri/Nextcloud/Zotero/storage/7VWAWCXU/Nickel and Kiela - 2017 - Poincar'e Embeddings for Learning Hierarchical Re.pdf;/home/dimitri/Nextcloud/Zotero/storage/P2HS3IIC/1705.html}
}

@article{leTreeSlicedApproximationWasserstein2019,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1902.00342},
  primaryClass = {cs, stat},
  title = {Tree-{{Sliced Approximation}} of {{Wasserstein Distances}}},
  url = {http://arxiv.org/abs/1902.00342},
  abstract = {Optimal transport (\$\textbackslash{}OT\$) theory provides a useful set of tools to compare probability distributions. As a consequence, the field of \$\textbackslash{}OT\$ is gaining traction and interest within the machine learning community. A few deficiencies usually associated with \$\textbackslash{}OT\$ include its high computational complexity when comparing discrete measures, which is quadratic when approximating it through entropic regularization; or supercubic when solving it exactly. For some applications, the fact that \$\textbackslash{}OT\$ distances are not usually negative definite also means that they cannot be used with usual Hilbertian tools. In this work, we consider a particular family of ground metrics, namely tree metrics, which yield negative definite \$\textbackslash{}OT\$ metrics that can be computed in linear time. By averaging over randomly sampled tree metrics, we obtain a tree-sliced-Wasserstein distance. We illustrate that the proposed tree-sliced-Wasserstein distances compare favorably with other baselines on various benchmark datasets.},
  urldate = {2019-02-04},
  date = {2019-02-01},
  keywords = {Statistics - Machine Learning,Computer Science - Machine Learning},
  author = {Le, Tam and Yamada, Makoto and Fukumizu, Kenji and Cuturi, Marco},
  file = {/home/dimitri/Nextcloud/Zotero/storage/CEFL2EQA/Le et al. - 2019 - Tree-Sliced Approximation of Wasserstein Distances.pdf;/home/dimitri/Nextcloud/Zotero/storage/MJ4SP48G/1902.html}
}

@inproceedings{kolouriSlicedWassersteinKernels2016,
  location = {{Las Vegas, NV, USA}},
  title = {Sliced {{Wasserstein Kernels}} for {{Probability Distributions}}},
  isbn = {978-1-4673-8851-1},
  url = {http://ieeexplore.ieee.org/document/7780937/},
  doi = {10.1109/CVPR.2016.568},
  eventtitle = {2016 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  booktitle = {2016 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  publisher = {{IEEE}},
  urldate = {2019-02-04},
  date = {2016-06},
  pages = {5258-5267},
  author = {Kolouri, Soheil and Zou, Yang and Rohde, Gustavo K.},
  file = {/home/dimitri/Nextcloud/Zotero/storage/HKQGHVGZ/Kolouri et al. - 2016 - Sliced Wasserstein Kernels for Probability Distrib.pdf}
}

@article{dadashiValueFunctionPolytope2019,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1901.11524},
  primaryClass = {cs, stat},
  title = {The {{Value Function Polytope}} in {{Reinforcement Learning}}},
  url = {http://arxiv.org/abs/1901.11524},
  abstract = {We establish geometric and topological properties of the space of value functions in finite state-action Markov decision processes. Our main contribution is the characterization of the nature of its shape: a general polytope (Aigner et al., 2010). To demonstrate this result, we exhibit several properties of the structural relationship between policies and value functions including the line theorem, which shows that the value functions of policies constrained on all but one state describe a line segment. Finally, we use this novel perspective to introduce visualizations to enhance the understanding of the dynamics of reinforcement learning algorithms.},
  urldate = {2019-02-05},
  date = {2019-01-31},
  keywords = {Statistics - Machine Learning,Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  author = {Dadashi, Robert and Taïga, Adrien Ali and Roux, Nicolas Le and Schuurmans, Dale and Bellemare, Marc G.},
  file = {/home/dimitri/Nextcloud/Zotero/storage/BMAG529V/Dadashi et al. - 2019 - The Value Function Polytope in Reinforcement Learn.pdf;/home/dimitri/Nextcloud/Zotero/storage/NKH528QV/1901.html}
}

@article{wietingNoTrainingRequired2019,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1901.10444},
  primaryClass = {cs},
  title = {No {{Training Required}}: {{Exploring Random Encoders}} for {{Sentence Classification}}},
  url = {http://arxiv.org/abs/1901.10444},
  shorttitle = {No {{Training Required}}},
  abstract = {We explore various methods for computing sentence representations from pre-trained word embeddings without any training, i.e., using nothing but random parameterizations. Our aim is to put sentence embeddings on more solid footing by 1) looking at how much modern sentence embeddings gain over random methods---as it turns out, surprisingly little; and by 2) providing the field with more appropriate baselines going forward---which are, as it turns out, quite strong. We also make important observations about proper experimental protocol for sentence classification evaluation, together with recommendations for future research.},
  urldate = {2019-02-05},
  date = {2019-01-29},
  keywords = {Computer Science - Computation and Language},
  author = {Wieting, John and Kiela, Douwe},
  file = {/home/dimitri/Nextcloud/Zotero/storage/VKSYHPRW/Wieting and Kiela - 2019 - No Training Required Exploring Random Encoders fo.pdf;/home/dimitri/Nextcloud/Zotero/storage/LGLUFAAQ/1901.html}
}

@article{sigaudPolicySearchContinuous2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1803.04706},
  primaryClass = {cs},
  title = {Policy {{Search}} in {{Continuous Action Domains}}: An {{Overview}}},
  url = {http://arxiv.org/abs/1803.04706},
  shorttitle = {Policy {{Search}} in {{Continuous Action Domains}}},
  abstract = {Continuous action policy search is currently the focus of intensive research, driven both by the recent success of deep reinforcement learning algorithms and the emergence of competitors based on evolutionary algorithms. In this paper, we present a broad survey of policy search methods, providing a unified perspective on very different approaches, including also Bayesian Optimization and directed exploration methods. The main message of this overview is in the relationship between the families of methods, but we also outline some factors underlying sample efficiency properties of the various approaches.},
  urldate = {2019-02-06},
  date = {2018-03-13},
  keywords = {Computer Science - Machine Learning},
  author = {Sigaud, Olivier and Stulp, Freek},
  file = {/home/dimitri/Nextcloud/Zotero/storage/J9AGTZTB/Sigaud and Stulp - 2018 - Policy Search in Continuous Action Domains an Ove.pdf;/home/dimitri/Nextcloud/Zotero/storage/K2UYB2U2/1803.html}
}

@article{chandakLearningActionRepresentations2019,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1902.00183},
  primaryClass = {cs, stat},
  title = {Learning {{Action Representations}} for {{Reinforcement Learning}}},
  url = {http://arxiv.org/abs/1902.00183},
  abstract = {Most model-free reinforcement learning methods leverage state representations (embeddings) for generalization, but either ignore structure in the space of actions or assume the structure is provided a priori. We show how a policy can be decomposed into a component that acts in a low-dimensional space of action representations and a component that transforms these representations into actual actions. These representations improve generalization over large, finite action sets by allowing the agent to infer the outcomes of actions similar to actions already taken. We provide an algorithm to both learn and use action representations and provide conditions for its convergence. The efficacy of the proposed method is demonstrated on large-scale real-world problems.},
  urldate = {2019-02-06},
  date = {2019-01-31},
  keywords = {Statistics - Machine Learning,Computer Science - Machine Learning},
  author = {Chandak, Yash and Theocharous, Georgios and Kostas, James and Jordan, Scott and Thomas, Philip S.},
  file = {/home/dimitri/Nextcloud/Zotero/storage/6XP2H8RG/Chandak et al. - 2019 - Learning Action Representations for Reinforcement .pdf;/home/dimitri/Nextcloud/Zotero/storage/52SZ5PBH/1902.html}
}

@book{villaniTopicsOptimalTransportation2003,
  location = {{Providence, RI}},
  title = {Topics in Optimal Transportation},
  isbn = {978-0-8218-3312-4},
  pagetotal = {370},
  number = {v. 58},
  series = {Graduate Studies in Mathematics},
  publisher = {{American Mathematical Society}},
  date = {2003},
  keywords = {Transportation problems (Programming),Monge-Ampère equations},
  author = {Villani, Cédric}
}

@book{korteCombinatorialOptimizationTheory2018,
  langid = {english},
  location = {{Berlin, Germany}},
  title = {Combinatorial Optimization: Theory and Algorithms},
  edition = {Sixth edition},
  isbn = {978-3-662-56039-6 978-3-662-56038-9},
  shorttitle = {Combinatorial Optimization},
  pagetotal = {698},
  number = {volume 21},
  series = {Algorithms and Combinatorics},
  publisher = {{Springer}},
  date = {2018},
  author = {Korte, Bernhard and Vygen, Jens},
  file = {/home/dimitri/Nextcloud/Zotero/storage/M7U27QRQ/Bernhard Korte, Jens Vygen - Combinatorial Optimization. Theory and Algorithms [6th ed.]-Springer (2018).pdf},
  note = {OCLC: 1011040795}
}

@book{papadimitriouCombinatorialOptimizationAlgorithms1998,
  location = {{Mineola, N.Y}},
  title = {Combinatorial Optimization: Algorithms and Complexity},
  isbn = {978-0-486-40258-1},
  shorttitle = {Combinatorial Optimization},
  pagetotal = {496},
  publisher = {{Dover Publications}},
  date = {1998},
  keywords = {Computational complexity,Mathematical optimization,Combinatorial optimization},
  author = {Papadimitriou, Christos H. and Steiglitz, Kenneth},
  file = {/home/dimitri/Nextcloud/Zotero/storage/IZNAG3IX/Christos H. Papadimitriou, Kenneth Steiglitz - Combinatorial Optimization_ Algorithms and Complexity-Dover Publications (1998).djvu}
}

@book{leeFirstCourseCombinatorial2004,
  location = {{Cambridge, UK ; New York}},
  title = {A First Course in Combinatorial Optimization},
  isbn = {978-0-521-81151-4 978-0-521-01012-2},
  pagetotal = {211},
  series = {Cambridge Texts in Applied Mathematics},
  publisher = {{Cambridge University Press}},
  date = {2004},
  keywords = {Combinatorial optimization},
  author = {Lee, Jon},
  file = {/home/dimitri/Nextcloud/Zotero/storage/MNBHLTLV/Lee - 2004 - A first course in combinatorial optimization.pdf}
}

@book{maclaneCategoriesWorkingMathematician1998,
  langid = {english},
  title = {Categories for the Working Mathematician},
  isbn = {978-1-4757-4721-8},
  date = {1998},
  author = {Mac Lane, Saunders},
  file = {/home/dimitri/Nextcloud/Zotero/storage/CT6QAGR6/Mac Lane - 1998 - Categories for the working mathematician.pdf},
  note = {OCLC: 982170670}
}

@article{hardalovMachineReadingComprehension2019,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1902.04574},
  primaryClass = {cs},
  title = {Machine {{Reading Comprehension}} for {{Answer Re}}-{{Ranking}} in {{Customer Support Chatbots}}},
  url = {http://arxiv.org/abs/1902.04574},
  abstract = {Recent advances in deep neural networks, language modeling and language generation have introduced new ideas to the field of conversational agents. As a result, deep neural models such as sequence-to-sequence, Memory Networks, and the Transformer have become key ingredients of state-of-the-art dialog systems. While those models are able to generate meaningful responses even in unseen situation, they need a lot of training data to build a reliable model. Thus, most real-world systems stuck to traditional approaches based on information retrieval and even hand-crafted rules, due to their robustness and effectiveness, especially for narrow-focused conversations. Here, we present a method that adapts a deep neural architecture from the domain of machine reading comprehension to re-rank the suggested answers from different models using the question as context. We train our model using negative sampling based on question-answer pairs from the Twitter Customer Support Dataset.The experimental results show that our re-ranking framework can improve the performance in terms of word overlap and semantics both for individual models as well as for model combinations.},
  urldate = {2019-02-15},
  date = {2019-02-12},
  keywords = {Computer Science - Computation and Language},
  author = {Hardalov, Momchil and Koychev, Ivan and Nakov, Preslav},
  file = {/home/dimitri/Nextcloud/Zotero/storage/XUWKUSKJ/Hardalov et al. - 2019 - Machine Reading Comprehension for Answer Re-Rankin.pdf;/home/dimitri/Nextcloud/Zotero/storage/CBQUVADF/1902.html}
}

@article{gultchinHumorWordEmbeddings2019,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1902.02783},
  primaryClass = {cs, stat},
  title = {Humor in {{Word Embeddings}}: {{Cockamamie Gobbledegook}} for {{Nincompoops}}},
  url = {http://arxiv.org/abs/1902.02783},
  shorttitle = {Humor in {{Word Embeddings}}},
  abstract = {We study humor in Word Embeddings, a popular AI tool that associates each word with a Euclidean vector. We find that: (a) the word vectors capture multiple aspects of humor discussed in theories of humor; and (b) each individual's sense of humor can be represented by a vector, and that these sense-of-humor vectors accurately predict differences in people's sense of humor on new, unrated, words. The fact that single-word humor seems to be relatively easy for AI has implications for the study of humor in language. Humor ratings are taken from the work of Englethaler and Hills (2017) as well as our own crowdsourcing study of 120,000 words.},
  urldate = {2019-02-15},
  date = {2019-02-08},
  keywords = {Statistics - Machine Learning,Computer Science - Computation and Language,Computer Science - Machine Learning},
  author = {Gultchin, Limor and Patterson, Genevieve and Baym, Nancy and Swinger, Nathaniel and Kalai, Adam Tauman},
  file = {/home/dimitri/Nextcloud/Zotero/storage/83GXJ4WZ/Gultchin et al. - 2019 - Humor in Word Embeddings Cockamamie Gobbledegook .pdf;/home/dimitri/Nextcloud/Zotero/storage/FIRW9ZAY/1902.html}
}

@article{liShortIntroductionLearning2011,
  langid = {english},
  title = {A {{Short Introduction}} to {{Learning}} to {{Rank}}},
  volume = {E94-D},
  issn = {0916-8532, 1745-1361},
  url = {http://joi.jlc.jst.go.jp/JST.JSTAGE/transinf/E94.D.1854?from=CrossRef},
  doi = {10.1587/transinf.E94.D.1854},
  number = {10},
  journaltitle = {IEICE Transactions on Information and Systems},
  urldate = {2019-02-15},
  date = {2011},
  pages = {1854-1862},
  author = {Li, Hang},
  file = {/home/dimitri/Nextcloud/Zotero/storage/I9WGZ6UA/Li - 2011 - A Short Introduction to Learning to Rank.pdf}
}

@article{burgesRankNetLambdaRankLambdaMART2010,
  langid = {american},
  title = {From {{RankNet}} to {{LambdaRank}} to {{LambdaMART}}: {{An Overview}}},
  url = {https://www.microsoft.com/en-us/research/publication/from-ranknet-to-lambdarank-to-lambdamart-an-overview/},
  shorttitle = {From {{RankNet}} to {{LambdaRank}} to {{LambdaMART}}},
  abstract = {LambdaMART is the boosted tree version of LambdaRank, which is based on RankNet. RankNet, LambdaRank, and LambdaMART have proven to be very successful algorithms for solving real world ranking problems: for example an ensemble of LambdaMART rankers won Track 1 of the 2010 Yahoo! Learning To Rank Challenge. The details of these algorithms are spread …},
  urldate = {2019-02-15},
  date = {2010-06-23},
  author = {Burges, Chris J. C.},
  file = {/home/dimitri/Nextcloud/Zotero/storage/ZFV3AZ43/Burges - 2010 - From RankNet to LambdaRank to LambdaMART An Overv.pdf;/home/dimitri/Nextcloud/Zotero/storage/VCCSEGHK/from-ranknet-to-lambdarank-to-lambdamart-an-overview.html}
}

@book{jurafskySpeechLanguageProcessing2009,
  location = {{Upper Saddle River, N.J}},
  title = {Speech and Language Processing: An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition},
  edition = {2nd ed},
  isbn = {978-0-13-187321-6},
  shorttitle = {Speech and Language Processing},
  pagetotal = {988},
  series = {Prentice {{Hall}} Series in Artificial Intelligence},
  publisher = {{Pearson Prentice Hall}},
  date = {2009},
  keywords = {Automatic speech recognition,Computational linguistics},
  author = {Jurafsky, Dan and Martin, James H.},
  file = {/home/dimitri/Nextcloud/Zotero/storage/5Y8X3Z9P/jurafsky_slp3.pdf;/home/dimitri/Nextcloud/Zotero/storage/KMG7LUHA/jurafsky_slp2.pdf},
  note = {OCLC: 213375806}
}

@book{manningIntroductionInformationRetrieval2008,
  location = {{New York}},
  title = {Introduction to Information Retrieval},
  isbn = {978-0-521-86571-5},
  pagetotal = {482},
  publisher = {{Cambridge University Press}},
  date = {2008},
  keywords = {Document clustering,Information retrieval,Semantic Web,Text processing (Computer science)},
  author = {Manning, Christopher D. and Raghavan, Prabhakar and Schütze, Hinrich},
  file = {/home/dimitri/Nextcloud/Zotero/storage/CTXQ7XI9/Manning et al. - 2008 - Introduction to information retrieval.pdf},
  note = {OCLC: ocn190786122}
}

@article{michelDoesGeometryWord2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1705.10900},
  primaryClass = {cs},
  title = {Does the {{Geometry}} of {{Word Embeddings Help Document Classification}}? {{A Case Study}} on {{Persistent Homology Based Representations}}},
  url = {http://arxiv.org/abs/1705.10900},
  shorttitle = {Does the {{Geometry}} of {{Word Embeddings Help Document Classification}}?},
  abstract = {We investigate the pertinence of methods from algebraic topology for text data analysis. These methods enable the development of mathematically-principled isometric-invariant mappings from a set of vectors to a document embedding, which is stable with respect to the geometry of the document in the selected metric space. In this work, we evaluate the utility of these topology-based document representations in traditional NLP tasks, specifically document clustering and sentiment classification. We find that the embeddings do not benefit text analysis. In fact, performance is worse than simple techniques like \$\textbackslash{}textit\{tf-idf\}\$, indicating that the geometry of the document does not provide enough variability for classification on the basis of topic or sentiment in the chosen datasets.},
  urldate = {2019-02-19},
  date = {2017-05-30},
  keywords = {Computer Science - Computation and Language},
  author = {Michel, Paul and Ravichander, Abhilasha and Rijhwani, Shruti},
  file = {/home/dimitri/Nextcloud/Zotero/storage/KM5ERMB9/Michel et al. - 2017 - Does the Geometry of Word Embeddings Help Document.pdf;/home/dimitri/Nextcloud/Zotero/storage/E39REUD6/1705.html}
}

@incollection{doshiMovieGenreDetection2018,
  location = {{Cham}},
  title = {Movie {{Genre Detection Using Topological Data Analysis}}},
  volume = {11171},
  isbn = {978-3-030-00809-3 978-3-030-00810-9},
  url = {http://link.springer.com/10.1007/978-3-030-00810-9_11},
  booktitle = {Statistical {{Language}} and {{Speech Processing}}},
  publisher = {{Springer International Publishing}},
  urldate = {2019-02-19},
  date = {2018},
  pages = {117-128},
  author = {Doshi, Pratik and Zadrozny, Wlodek},
  editor = {Dutoit, Thierry and Martín-Vide, Carlos and Pironkov, Gueorgui},
  file = {/home/dimitri/Nextcloud/Zotero/storage/UDN7LRLN/Doshi and Zadrozny - 2018 - Movie Genre Detection Using Topological Data Analy.pdf},
  doi = {10.1007/978-3-030-00810-9_11}
}

@inproceedings{kusnerWordEmbeddingsDocument2015,
  langid = {english},
  title = {From {{Word Embeddings To Document Distances}}},
  url = {http://proceedings.mlr.press/v37/kusnerb15.html},
  abstract = {We present the Word Mover’s Distance (WMD), a novel distance function between text documents. Our work is based on recent results in word embeddings that learn semantically meaningful representatio...},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  booktitle = {International {{Conference}} on {{Machine Learning}}},
  urldate = {2019-02-19},
  date = {2015-06-01},
  pages = {957-966},
  author = {Kusner, Matt and Sun, Yu and Kolkin, Nicholas and Weinberger, Kilian},
  file = {/home/dimitri/Nextcloud/Zotero/storage/A2XMBEIG/Kusner et al. - 2015 - From Word Embeddings To Document Distances.pdf;/home/dimitri/Nextcloud/Zotero/storage/7SHEHA48/kusnerb15.html}
}

@book{awodeyCategoryTheory2010,
  location = {{Oxford ; New York}},
  title = {Category Theory},
  edition = {2nd ed},
  isbn = {978-0-19-958736-0 978-0-19-923718-0},
  pagetotal = {311},
  number = {52},
  series = {Oxford Logic Guides},
  publisher = {{Oxford University Press}},
  date = {2010},
  keywords = {Categories (Mathematics)},
  author = {Awodey, Steve},
  file = {/home/dimitri/Nextcloud/Zotero/storage/IKPP8J7L/Awodey - 2010 - Category theory.pdf}
}

@article{andreasMeasuringCompositionalityRepresentation2019,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1902.07181},
  primaryClass = {cs, stat},
  title = {Measuring {{Compositionality}} in {{Representation Learning}}},
  url = {http://arxiv.org/abs/1902.07181},
  abstract = {Many machine learning algorithms represent input data with vector embeddings or discrete codes. When inputs exhibit compositional structure (e.g. objects built from parts or procedures from subroutines), it is natural to ask whether this compositional structure is reflected in the the inputs' learned representations. While the assessment of compositionality in languages has received significant attention in linguistics and adjacent fields, the machine learning literature lacks general-purpose tools for producing graded measurements of compositional structure in more general (e.g. vector-valued) representation spaces. We describe a procedure for evaluating compositionality by measuring how well the true representation-producing model can be approximated by a model that explicitly composes a collection of inferred representational primitives. We use the procedure to provide formal and empirical characterizations of compositional structure in a variety of settings, exploring the relationship between compositionality and learning dynamics, human judgments, representational similarity, and generalization.},
  urldate = {2019-02-20},
  date = {2019-02-19},
  keywords = {Statistics - Machine Learning,Computer Science - Computation and Language,Computer Science - Machine Learning},
  author = {Andreas, Jacob},
  file = {/home/dimitri/Nextcloud/Zotero/storage/544787IF/Andreas - 2019 - Measuring Compositionality in Representation Learn.pdf;/home/dimitri/Nextcloud/Zotero/storage/HB8T7SQL/1902.html}
}

@article{peyreComputationalOptimalTransport2019,
  langid = {english},
  title = {Computational {{Optimal Transport}}},
  volume = {11},
  issn = {1935-8237, 1935-8245},
  url = {http://www.nowpublishers.com/article/Details/MAL-073},
  doi = {10.1561/2200000073},
  number = {5-6},
  journaltitle = {Foundations and Trends® in Machine Learning},
  urldate = {2019-02-20},
  date = {2019},
  pages = {355-206},
  author = {Peyré, Gabriel and Cuturi, Marco},
  file = {/home/dimitri/Nextcloud/Zotero/storage/GLNYIRM9/Peyré and Cuturi - 2019 - Computational Optimal Transport.pdf}
}

@article{jainNonconvexOptimizationMachine2017,
  langid = {english},
  title = {Non-Convex {{Optimization}} for {{Machine Learning}}},
  volume = {10},
  issn = {1935-8237, 1935-8245},
  url = {https://www.nowpublishers.com/article/Details/MAL-058},
  doi = {10.1561/2200000058},
  abstract = {Non-convex Optimization for Machine Learning},
  number = {3-4},
  journaltitle = {Foundations and Trends® in Machine Learning},
  shortjournal = {MAL},
  urldate = {2019-02-20},
  date = {2017-12-04},
  pages = {142-336},
  author = {Jain, Prateek and Kar, Purushottam},
  file = {/home/dimitri/Nextcloud/Zotero/storage/ZWTR7SWH/Jain and Kar - 2017 - Non-convex Optimization for Machine Learning.pdf;/home/dimitri/Nextcloud/Zotero/storage/QWABTU22/MAL-058.html}
}

@article{bubeckConvexOptimizationAlgorithms2015,
  langid = {english},
  title = {Convex {{Optimization}}: {{Algorithms}} and {{Complexity}}},
  volume = {8},
  issn = {1935-8237, 1935-8245},
  url = {https://www.nowpublishers.com/article/Details/MAL-050},
  doi = {10.1561/2200000050},
  shorttitle = {Convex {{Optimization}}},
  abstract = {Convex Optimization: Algorithms and Complexity},
  number = {3-4},
  journaltitle = {Foundations and Trends® in Machine Learning},
  shortjournal = {MAL},
  urldate = {2019-02-20},
  date = {2015-11-12},
  pages = {231-357},
  author = {Bubeck, Sébastien},
  file = {/home/dimitri/Nextcloud/Zotero/storage/54GJIZGU/Bubeck - 2015 - Convex Optimization Algorithms and Complexity.pdf;/home/dimitri/Nextcloud/Zotero/storage/WWYQ2BYY/MAL-050.html}
}

@article{geramifardTutorialLinearFunction2013,
  langid = {english},
  title = {A {{Tutorial}} on {{Linear Function Approximators}} for {{Dynamic Programming}} and {{Reinforcement Learning}}},
  volume = {6},
  issn = {1935-8237, 1935-8245},
  url = {https://www.nowpublishers.com/article/Details/MAL-042},
  doi = {10.1561/2200000042},
  abstract = {A Tutorial on Linear Function Approximators for Dynamic Programming and Reinforcement Learning},
  number = {4},
  journaltitle = {Foundations and Trends® in Machine Learning},
  shortjournal = {MAL},
  urldate = {2019-02-20},
  date = {2013-12-19},
  pages = {375-451},
  author = {Geramifard, Alborz and Walsh, Thomas J. and Tellex, Stefanie and Chowdhary, Girish and Roy, Nicholas and How, Jonathan P.},
  file = {/home/dimitri/Nextcloud/Zotero/storage/FIZT2Z35/Geramifard et al. - 2013 - A Tutorial on Linear Function Approximators for Dy.pdf;/home/dimitri/Nextcloud/Zotero/storage/EPXJGGZ5/MAL-042.html}
}

@article{alvarezKernelsVectorValuedFunctions2012,
  langid = {english},
  title = {Kernels for {{Vector}}-{{Valued Functions}}: {{A Review}}},
  volume = {4},
  issn = {1935-8237, 1935-8245},
  url = {http://www.nowpublishers.com/article/Details/MAL-036},
  doi = {10.1561/2200000036},
  shorttitle = {Kernels for {{Vector}}-{{Valued Functions}}},
  number = {3},
  journaltitle = {Foundations and Trends® in Machine Learning},
  urldate = {2019-02-20},
  date = {2012},
  pages = {195-266},
  author = {Álvarez, Mauricio A. and Rosasco, Lorenzo and Lawrence, Neil D.},
  file = {/home/dimitri/Nextcloud/Zotero/storage/ZXNPZLLT/10.1561@2200000036.pdf}
}

@article{wainwrightGraphicalModelsExponential2007a,
  langid = {english},
  title = {Graphical {{Models}}, {{Exponential Families}}, and {{Variational Inference}}},
  volume = {1},
  issn = {1935-8237, 1935-8245},
  url = {http://www.nowpublishers.com/article/Details/MAL-001},
  doi = {10.1561/2200000001},
  number = {1–2},
  journaltitle = {Foundations and Trends® in Machine Learning},
  urldate = {2019-02-20},
  date = {2007},
  pages = {1-305},
  author = {Wainwright, Martin J. and Jordan, Michael I.},
  file = {/home/dimitri/Nextcloud/Zotero/storage/BZ6MAKMW/WaiJor08_FTML.pdf;/home/dimitri/Nextcloud/Zotero/storage/N8IYA7NZ/Wainwright and Jordan - 2007 - Graphical Models, Exponential Families, and Variat.pdf}
}

@article{burgesDimensionReductionGuided2009,
  langid = {english},
  title = {Dimension {{Reduction}}: {{A Guided Tour}}},
  volume = {2},
  issn = {1935-8237, 1935-8245},
  url = {http://www.nowpublishers.com/article/Details/MAL-002},
  doi = {10.1561/2200000002},
  shorttitle = {Dimension {{Reduction}}},
  number = {4},
  journaltitle = {Foundations and Trends® in Machine Learning},
  urldate = {2019-02-20},
  date = {2009},
  pages = {275-364},
  author = {Burges, Christopher J. C.},
  file = {/home/dimitri/Nextcloud/Zotero/storage/LPDTXCKM/Burges - 2009 - Dimension Reduction A Guided Tour.pdf}
}

@article{devlinBERTPretrainingDeep2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1810.04805},
  primaryClass = {cs},
  title = {{{BERT}}: {{Pre}}-Training of {{Deep Bidirectional Transformers}} for {{Language Understanding}}},
  url = {http://arxiv.org/abs/1810.04805},
  shorttitle = {{{BERT}}},
  abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT representations can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE benchmark to 80.4\% (7.6\% absolute improvement), MultiNLI accuracy to 86.7 (5.6\% absolute improvement) and the SQuAD v1.1 question answering Test F1 to 93.2 (1.5\% absolute improvement), outperforming human performance by 2.0\%.},
  urldate = {2019-02-21},
  date = {2018-10-10},
  keywords = {Computer Science - Computation and Language},
  author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  file = {/home/dimitri/Nextcloud/Zotero/storage/3M8VB4UN/Devlin et al. - 2018 - BERT Pre-training of Deep Bidirectional Transform.pdf;/home/dimitri/Nextcloud/Zotero/storage/3P9Z7WWU/1810.html}
}

@article{liuLearningRankInformation2007,
  langid = {english},
  title = {Learning to {{Rank}} for {{Information Retrieval}}},
  volume = {3},
  issn = {1554-0669, 1554-0677},
  url = {http://www.nowpublishers.com/article/Details/INR-016},
  doi = {10.1561/1500000016},
  number = {3},
  journaltitle = {Foundations and Trends® in Information Retrieval},
  urldate = {2019-02-21},
  date = {2007},
  pages = {225-331},
  author = {Liu, Tie-Yan},
  file = {/home/dimitri/Nextcloud/Zotero/storage/4TMHYG2M/Liu - 2007 - Learning to Rank for Information Retrieval.pdf}
}

@book{vershyninHighdimensionalProbabilityIntroduction2018,
  location = {{Cambridge}},
  title = {High-Dimensional Probability: An Introduction with Applications in Data Science},
  isbn = {978-1-108-41519-4},
  shorttitle = {High-Dimensional Probability},
  number = {47},
  series = {Cambridge Series in Statistical and Probabilistic Mathematics},
  publisher = {{Cambridge University Press}},
  date = {2018},
  keywords = {Probabilities,Random variables,Stochastic processes},
  author = {Vershynin, Roman},
  file = {/home/dimitri/Nextcloud/Zotero/storage/EZ284AQZ/Vershynin - 2018 - High-dimensional probability an introduction with.pdf}
}

@article{gabellaTopologyLearningArtificial2019,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1902.08160},
  primaryClass = {cs, stat},
  title = {Topology of {{Learning}} in {{Artificial Neural Networks}}},
  url = {http://arxiv.org/abs/1902.08160},
  abstract = {Understanding how neural networks learn remains one of the central challenges in machine learning research. From random at the start of training, the weights of a neural network evolve in such a way as to be able to perform a variety of tasks, like classifying images. Here we study the emergence of structure in the weights by applying methods from topological data analysis. We train simple feedforward neural networks on the MNIST dataset and monitor the evolution of the weights. When initialized to zero, the weights follow trajectories that branch off recurrently, thus generating trees that describe the growth of the effective capacity of each layer. When initialized to tiny random values, the weights evolve smoothly along two-dimensional surfaces. We show that natural coordinates on these learning surfaces correspond to important factors of variation.},
  urldate = {2019-02-22},
  date = {2019-02-21},
  keywords = {Statistics - Machine Learning,Computer Science - Machine Learning},
  author = {Gabella, Maxime and Afambo, Nitya and Ebli, Stefania and Spreemann, Gard},
  file = {/home/dimitri/Nextcloud/Zotero/storage/LMRXQ7UH/Gabella et al. - 2019 - Topology of Learning in Artificial Neural Networks.pdf}
}

@article{espadotoDeepLearningMultidimensional2019,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1902.07958},
  primaryClass = {cs, stat},
  title = {Deep {{Learning Multidimensional Projections}}},
  url = {http://arxiv.org/abs/1902.07958},
  abstract = {Dimensionality reduction methods, also known as projections, are frequently used for exploring multidimensional data in machine learning, data science, and information visualization. Among these, t-SNE and its variants have become very popular for their ability to visually separate distinct data clusters. However, such methods are computationally expensive for large datasets, suffer from stability problems, and cannot directly handle out-of-sample data. We propose a learning approach to construct such projections. We train a deep neural network based on a collection of samples from a given data universe, and their corresponding projections, and next use the network to infer projections of data from the same, or similar, universes. Our approach generates projections with similar characteristics as the learned ones, is computationally two to three orders of magnitude faster than SNE-class methods, has no complex-to-set user parameters, handles out-of-sample data in a stable manner, and can be used to learn any projection technique. We demonstrate our proposal on several real-world high dimensional datasets from machine learning.},
  urldate = {2019-02-22},
  date = {2019-02-21},
  keywords = {Statistics - Machine Learning,Computer Science - Machine Learning},
  author = {Espadoto, Mateus and Hirata, Nina S. T. and Telea, Alexandru C.},
  file = {/home/dimitri/Nextcloud/Zotero/storage/8LZENHE6/Espadoto et al. - 2019 - Deep Learning Multidimensional Projections.pdf;/home/dimitri/Nextcloud/Zotero/storage/EPWU8A22/1902.html}
}

@article{chenDeepShortText2019,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1902.08050},
  primaryClass = {cs},
  title = {Deep {{Short Text Classification}} with {{Knowledge Powered Attention}}},
  url = {http://arxiv.org/abs/1902.08050},
  abstract = {Short text classification is one of important tasks in Natural Language Processing (NLP). Unlike paragraphs or documents, short texts are more ambiguous since they have not enough contextual information, which poses a great challenge for classification. In this paper, we retrieve knowledge from external knowledge source to enhance the semantic representation of short texts. We take conceptual information as a kind of knowledge and incorporate it into deep neural networks. For the purpose of measuring the importance of knowledge, we introduce attention mechanisms and propose deep Short Text Classification with Knowledge powered Attention (STCKA). We utilize Concept towards Short Text (C- ST) attention and Concept towards Concept Set (C-CS) attention to acquire the weight of concepts from two aspects. And we classify a short text with the help of conceptual information. Unlike traditional approaches, our model acts like a human being who has intrinsic ability to make decisions based on observation (i.e., training data for machines) and pays more attention to important knowledge. We also conduct extensive experiments on four public datasets for different tasks. The experimental results and case studies show that our model outperforms the state-of-the-art methods, justifying the effectiveness of knowledge powered attention.},
  urldate = {2019-02-22},
  date = {2019-02-21},
  keywords = {Computer Science - Computation and Language},
  author = {Chen, Jindong and Hu, Yizhou and Liu, Jingping and Xiao, Yanghua and Jiang, Haiyun},
  file = {/home/dimitri/Nextcloud/Zotero/storage/SC95DIZ4/Chen et al. - 2019 - Deep Short Text Classification with Knowledge Powe.pdf;/home/dimitri/Nextcloud/Zotero/storage/EEEHSHRP/1902.html}
}

@article{nivReinforcementLearningBrain2009,
  langid = {english},
  title = {Reinforcement Learning in the Brain},
  volume = {53},
  issn = {00222496},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0022249608001181},
  doi = {10.1016/j.jmp.2008.12.005},
  number = {3},
  journaltitle = {Journal of Mathematical Psychology},
  urldate = {2019-02-22},
  date = {2009-06},
  pages = {139-154},
  author = {Niv, Yael},
  file = {/home/dimitri/Nextcloud/Zotero/storage/Z2DJT4F8/Niv - 2009 - Reinforcement learning in the brain.pdf}
}

@article{leeNeuralBasisReinforcement2012,
  langid = {english},
  title = {Neural {{Basis}} of {{Reinforcement Learning}} and {{Decision Making}}},
  volume = {35},
  issn = {0147-006X, 1545-4126},
  url = {http://www.annualreviews.org/doi/10.1146/annurev-neuro-062111-150512},
  doi = {10.1146/annurev-neuro-062111-150512},
  number = {1},
  journaltitle = {Annual Review of Neuroscience},
  urldate = {2019-02-22},
  date = {2012-07-21},
  pages = {287-308},
  author = {Lee, Daeyeol and Seo, Hyojung and Jung, Min Whan},
  file = {/home/dimitri/Nextcloud/Zotero/storage/VI7UZJFW/Lee et al. - 2012 - Neural Basis of Reinforcement Learning and Decisio.pdf}
}

@article{dayanReinforcementLearningGood2008,
  langid = {english},
  title = {Reinforcement Learning: {{The Good}}, {{The Bad}} and {{The Ugly}}},
  volume = {18},
  issn = {09594388},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0959438808000767},
  doi = {10.1016/j.conb.2008.08.003},
  shorttitle = {Reinforcement Learning},
  number = {2},
  journaltitle = {Current Opinion in Neurobiology},
  urldate = {2019-02-22},
  date = {2008-04},
  pages = {185-196},
  author = {Dayan, Peter and Niv, Yael},
  file = {/home/dimitri/Nextcloud/Zotero/storage/EXJYPW3P/Dayan and Niv - 2008 - Reinforcement learning The Good, The Bad and The .pdf}
}

@article{dayanDecisionTheoryReinforcement2008,
  langid = {english},
  title = {Decision Theory, Reinforcement Learning, and the Brain},
  volume = {8},
  issn = {1530-7026, 1531-135X},
  url = {http://www.springerlink.com/index/10.3758/CABN.8.4.429},
  doi = {10.3758/CABN.8.4.429},
  number = {4},
  journaltitle = {Cognitive, Affective, \& Behavioral Neuroscience},
  urldate = {2019-02-22},
  date = {2008-12-01},
  pages = {429-453},
  author = {Dayan, P. and Daw, N. D.},
  file = {/home/dimitri/Nextcloud/Zotero/storage/4KR869XQ/Dayan and Daw - 2008 - Decision theory, reinforcement learning, and the b.pdf}
}

@incollection{nivTheoreticalEmpiricalStudies2009,
  langid = {english},
  title = {Theoretical and {{Empirical Studies}} of {{Learning}}},
  isbn = {978-0-12-374176-9},
  url = {https://linkinghub.elsevier.com/retrieve/pii/B9780123741769000221},
  booktitle = {Neuroeconomics},
  publisher = {{Elsevier}},
  urldate = {2019-02-22},
  date = {2009},
  pages = {331-351},
  author = {Niv, Yael and Montague, P. Read},
  file = {/home/dimitri/Nextcloud/Zotero/storage/KB3FB4VH/Niv and Montague - 2009 - Theoretical and Empirical Studies of Learning.pdf},
  doi = {10.1016/B978-0-12-374176-9.00022-1}
}

@article{hassabisNeuroscienceInspiredArtificialIntelligence2017,
  langid = {english},
  title = {Neuroscience-{{Inspired Artificial Intelligence}}},
  volume = {95},
  issn = {08966273},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0896627317305093},
  doi = {10.1016/j.neuron.2017.06.011},
  number = {2},
  journaltitle = {Neuron},
  urldate = {2019-02-22},
  date = {2017-07},
  pages = {245-258},
  author = {Hassabis, Demis and Kumaran, Dharshan and Summerfield, Christopher and Botvinick, Matthew},
  file = {/home/dimitri/Nextcloud/Zotero/storage/ZFNE5B22/Hassabis et al. - 2017 - Neuroscience-Inspired Artificial Intelligence.pdf}
}

@article{heDeepReinforcementLearning2015,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1511.04636},
  primaryClass = {cs},
  title = {Deep {{Reinforcement Learning}} with a {{Natural Language Action Space}}},
  url = {http://arxiv.org/abs/1511.04636},
  abstract = {This paper introduces a novel architecture for reinforcement learning with deep neural networks designed to handle state and action spaces characterized by natural language, as found in text-based games. Termed a deep reinforcement relevance network (DRRN), the architecture represents action and state spaces with separate embedding vectors, which are combined with an interaction function to approximate the Q-function in reinforcement learning. We evaluate the DRRN on two popular text games, showing superior performance over other deep Q-learning architectures. Experiments with paraphrased action descriptions show that the model is extracting meaning rather than simply memorizing strings of text.},
  urldate = {2019-02-22},
  date = {2015-11-14},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  author = {He, Ji and Chen, Jianshu and He, Xiaodong and Gao, Jianfeng and Li, Lihong and Deng, Li and Ostendorf, Mari},
  file = {/home/dimitri/Nextcloud/Zotero/storage/SI9LY32R/He et al. - 2015 - Deep Reinforcement Learning with a Natural Languag.pdf;/home/dimitri/Nextcloud/Zotero/storage/EHITNS3A/1511.html}
}

@article{binzWhereHumanHeuristics2019,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1902.07580},
  primaryClass = {cs, stat},
  title = {Where {{Do Human Heuristics Come From}}?},
  url = {http://arxiv.org/abs/1902.07580},
  abstract = {Human decision-making deviates from the optimal solution, that maximizes cumulative rewards, in many situations. Here we approach this discrepancy from the perspective of bounded rationality and our goal is to provide a justification for such seemingly sub-optimal strategies. More specifically we investigate the hypothesis, that humans do not know optimal decision-making algorithms in advance, but instead employ a learned, resource-bounded approximation. The idea is formalized through combining a recently proposed meta-learning model based on Recurrent Neural Networks with a resource-bounded objective. The resulting approach is closely connected to variational inference and the Minimum Description Length principle. Empirical evidence is obtained from a two-armed bandit task. Here we observe patterns in our family of models that resemble differences between individual human participants.},
  urldate = {2019-02-22},
  date = {2019-02-20},
  keywords = {Statistics - Machine Learning,Computer Science - Machine Learning},
  author = {Binz, Marcel and Endres, Dominik},
  file = {/home/dimitri/Nextcloud/Zotero/storage/JQPX2ZX3/Binz and Endres - 2019 - Where Do Human Heuristics Come From.pdf;/home/dimitri/Nextcloud/Zotero/storage/WFPQMPBN/1902.html}
}

@article{rowlandStatisticsSamplesDistributional2019,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1902.08102},
  primaryClass = {cs, stat},
  title = {Statistics and {{Samples}} in {{Distributional Reinforcement Learning}}},
  url = {http://arxiv.org/abs/1902.08102},
  abstract = {We present a unifying framework for designing and analysing distributional reinforcement learning (DRL) algorithms in terms of recursively estimating statistics of the return distribution. Our key insight is that DRL algorithms can be decomposed as the combination of some statistical estimator and a method for imputing a return distribution consistent with that set of statistics. With this new understanding, we are able to provide improved analyses of existing DRL algorithms as well as construct a new algorithm (EDRL) based upon estimation of the expectiles of the return distribution. We compare EDRL with existing methods on a variety of MDPs to illustrate concrete aspects of our analysis, and develop a deep RL variant of the algorithm, ER-DQN, which we evaluate on the Atari-57 suite of games.},
  urldate = {2019-02-23},
  date = {2019-02-21},
  keywords = {Statistics - Machine Learning,Computer Science - Machine Learning},
  author = {Rowland, Mark and Dadashi, Robert and Kumar, Saurabh and Munos, Rémi and Bellemare, Marc G. and Dabney, Will},
  file = {/home/dimitri/Nextcloud/Zotero/storage/X62P89VT/Rowland et al. - 2019 - Statistics and Samples in Distributional Reinforce.pdf;/home/dimitri/Nextcloud/Zotero/storage/92DDSS8C/1902.html}
}

@article{cerUniversalSentenceEncoder2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1803.11175},
  primaryClass = {cs},
  title = {Universal {{Sentence Encoder}}},
  url = {http://arxiv.org/abs/1803.11175},
  abstract = {We present models for encoding sentences into embedding vectors that specifically target transfer learning to other NLP tasks. The models are efficient and result in accurate performance on diverse transfer tasks. Two variants of the encoding models allow for trade-offs between accuracy and compute resources. For both variants, we investigate and report the relationship between model complexity, resource consumption, the availability of transfer task training data, and task performance. Comparisons are made with baselines that use word level transfer learning via pretrained word embeddings as well as baselines do not use any transfer learning. We find that transfer learning using sentence embeddings tends to outperform word level transfer. With transfer learning via sentence embeddings, we observe surprisingly good performance with minimal amounts of supervised training data for a transfer task. We obtain encouraging results on Word Embedding Association Tests (WEAT) targeted at detecting model bias. Our pre-trained sentence encoding models are made freely available for download and on TF Hub.},
  urldate = {2019-02-24},
  date = {2018-03-29},
  keywords = {Computer Science - Computation and Language},
  author = {Cer, Daniel and Yang, Yinfei and Kong, Sheng-yi and Hua, Nan and Limtiaco, Nicole and John, Rhomni St and Constant, Noah and Guajardo-Cespedes, Mario and Yuan, Steve and Tar, Chris and Sung, Yun-Hsuan and Strope, Brian and Kurzweil, Ray},
  file = {/home/dimitri/Nextcloud/Zotero/storage/HQI5EHPN/Cer et al. - 2018 - Universal Sentence Encoder.pdf;/home/dimitri/Nextcloud/Zotero/storage/AM7EA25D/1803.html}
}

@article{youngPOMDPBasedStatisticalSpoken2013,
  title = {{{POMDP}}-{{Based Statistical Spoken Dialog Systems}}: {{A Review}}},
  volume = {101},
  issn = {0018-9219},
  doi = {10.1109/JPROC.2012.2225812},
  shorttitle = {{{POMDP}}-{{Based Statistical Spoken Dialog Systems}}},
  abstract = {Statistical dialog systems (SDSs) are motivated by the need for a data-driven framework that reduces the cost of laboriously handcrafting complex dialog managers and that provides robustness against the errors created by speech recognizers operating in noisy environments. By including an explicit Bayesian model of uncertainty and by optimizing the policy via a reward-driven process, partially observable Markov decision processes (POMDPs) provide such a framework. However, exact model representation and optimization is computationally intractable. Hence, the practical application of POMDP-based systems requires efficient algorithms and carefully constructed approximations. This review article provides an overview of the current state of the art in the development of POMDP-based spoken dialog systems.},
  number = {5},
  journaltitle = {Proceedings of the IEEE},
  date = {2013-05},
  pages = {1160-1179},
  keywords = {reinforcement learning,Bayes methods,Belief monitoring,Belief propagation,data-driven framework,exact model representation,explicit Bayesian model,Information processing,Learning systems,Markov processes,Mathematical model,noisy environments,optimisation,optimization,Optimization,partially observable Markov decision process (POMDP),partially observable Markov decision processes,policy optimization,POMDP-based statistical spoken dialog systems,reward-driven process,SDS,Speech processing,speech recognition,Speech recognition,speech recognizers,spoken dialog systems (SDSs),statistical dialog systems},
  author = {Young, S. and Gašić, M. and Thomson, B. and Williams, J. D.},
  file = {/home/dimitri/Nextcloud/Zotero/storage/8M4JHYJ5/Young et al. - 2013 - POMDP-Based Statistical Spoken Dialog Systems A R.pdf;/home/dimitri/Nextcloud/Zotero/storage/GXZF6FQP/6407655.html}
}

@article{elliottSimpleEssenceAutomatic2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1804.00746},
  primaryClass = {cs},
  title = {The Simple Essence of Automatic Differentiation},
  url = {http://arxiv.org/abs/1804.00746},
  abstract = {Automatic differentiation (AD) in reverse mode (RAD) is a central component of deep learning and other uses of large-scale optimization. Commonly used RAD algorithms such as backpropagation, however, are complex and stateful, hindering deep understanding, improvement, and parallel execution. This paper develops a simple, generalized AD algorithm calculated from a simple, natural specification. The general algorithm is then specialized by varying the representation of derivatives. In particular, applying well-known constructions to a naive representation yields two RAD algorithms that are far simpler than previously known. In contrast to commonly used RAD implementations, the algorithms defined here involve no graphs, tapes, variables, partial derivatives, or mutation. They are inherently parallel-friendly, correct by construction, and usable directly from an existing programming language with no need for new data types or programming style, thanks to use of an AD-agnostic compiler plugin.},
  urldate = {2019-02-25},
  date = {2018-04-02},
  keywords = {Computer Science - Programming Languages},
  author = {Elliott, Conal},
  file = {/home/dimitri/Nextcloud/Zotero/storage/3UAJ5FS5/Elliott - 2018 - The simple essence of automatic differentiation.pdf;/home/dimitri/Nextcloud/Zotero/storage/6AEYAUSD/1804.html}
}

@incollection{laueComputingHigherOrder2018,
  title = {Computing {{Higher Order Derivatives}} of {{Matrix}} and {{Tensor Expressions}}},
  url = {http://papers.nips.cc/paper/7540-computing-higher-order-derivatives-of-matrix-and-tensor-expressions.pdf},
  booktitle = {Advances in {{Neural Information Processing Systems}} 31},
  publisher = {{Curran Associates, Inc.}},
  urldate = {2019-02-26},
  date = {2018},
  pages = {2750--2759},
  author = {Laue, Soeren and Mitterreiter, Matthias and Giesen, Joachim},
  editor = {Bengio, S. and Wallach, H. and Larochelle, H. and Grauman, K. and Cesa-Bianchi, N. and Garnett, R.},
  file = {/home/dimitri/Nextcloud/Zotero/storage/XTFT9D7A/Laue et al. - 2018 - Computing Higher Order Derivatives of Matrix and T.pdf;/home/dimitri/Nextcloud/Zotero/storage/BR6Z3GAI/7540-computing-higher-order-derivatives-of-matrix-and-tensor-expressions.html}
}

@article{seeWhatMakesGood2019,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1902.08654},
  primaryClass = {cs},
  title = {What Makes a Good Conversation? {{How}} Controllable Attributes Affect Human Judgments},
  url = {http://arxiv.org/abs/1902.08654},
  shorttitle = {What Makes a Good Conversation?},
  abstract = {A good conversation requires balance -- between simplicity and detail; staying on topic and changing it; asking questions and answering them. Although dialogue agents are commonly evaluated via human judgments of overall quality, the relationship between quality and these individual factors is less well-studied. In this work, we examine two controllable neural text generation methods, conditional training and weighted decoding, in order to control four important attributes for chitchat dialogue: repetition, specificity, response-relatedness and question-asking. We conduct a large-scale human evaluation to measure the effect of these control parameters on multi-turn interactive conversations on the PersonaChat task. We provide a detailed analysis of their relationship to high-level aspects of conversation, and show that by controlling combinations of these variables our models obtain clear improvements in human quality judgments.},
  urldate = {2019-02-26},
  date = {2019-02-22},
  keywords = {Computer Science - Computation and Language},
  author = {See, Abigail and Roller, Stephen and Kiela, Douwe and Weston, Jason},
  file = {/home/dimitri/Nextcloud/Zotero/storage/XYNKD9EJ/See et al. - 2019 - What makes a good conversation How controllable a.pdf;/home/dimitri/Nextcloud/Zotero/storage/RB32Z4LI/1902.html}
}

@article{barrettAnalyzingBiologicalArtificial2019,
  title = {Analyzing Biological and Artificial Neural Networks: Challenges with Opportunities for Synergy?},
  volume = {55},
  issn = {0959-4388},
  url = {http://www.sciencedirect.com/science/article/pii/S0959438818301569},
  doi = {10.1016/j.conb.2019.01.007},
  shorttitle = {Analyzing Biological and Artificial Neural Networks},
  abstract = {Deep neural networks (DNNs) transform stimuli across multiple processing stages to produce representations that can be used to solve complex tasks, such as object recognition in images. However, a full understanding of how they achieve this remains elusive. The complexity of biological neural networks substantially exceeds the complexity of DNNs, making it even more challenging to understand the representations they learn. Thus, both machine learning and computational neuroscience are faced with a shared challenge: how can we analyze their representations in order to understand how they solve complex tasks? We review how data-analysis concepts and techniques developed by computational neuroscientists can be useful for analyzing representations in DNNs, and in turn, how recently developed techniques for analysis of DNNs can be useful for understanding representations in biological neural networks. We explore opportunities for synergy between the two fields, such as the use of DNNs as in silico model systems for neuroscience, and how this synergy can lead to new hypotheses about the operating principles of biological neural networks.},
  journaltitle = {Current Opinion in Neurobiology},
  shortjournal = {Current Opinion in Neurobiology},
  series = {Machine {{Learning}}, {{Big Data}}, and {{Neuroscience}}},
  urldate = {2019-02-27},
  date = {2019-04-01},
  pages = {55-64},
  author = {Barrett, David GT and Morcos, Ari S and Macke, Jakob H},
  file = {/home/dimitri/Nextcloud/Zotero/storage/PLZA4SNL/Barrett et al. - 2019 - Analyzing biological and artificial neural network.pdf;/home/dimitri/Nextcloud/Zotero/storage/ZCJ8NEAX/S0959438818301569.html}
}

@article{chenBERTJointIntent2019,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1902.10909},
  primaryClass = {cs},
  title = {{{BERT}} for {{Joint Intent Classification}} and {{Slot Filling}}},
  url = {http://arxiv.org/abs/1902.10909},
  abstract = {Intent classification and slot filling are two essential tasks for natural language understanding. They often suffer from small-scale human-labeled training data, resulting in poor generalization capability, especially for rare words. Recently a new language representation model, BERT (Bidirectional Encoder Representations from Transformers), facilitates pre-training deep bidirectional representations on large-scale unlabeled corpora, and has created state-of-the-art models for a wide variety of natural language processing tasks after simple fine-tuning. However, there has not been much effort on exploring BERT for natural language understanding. In this work, we propose a joint intent classification and slot filling model based on BERT. Experimental results demonstrate that our proposed model achieves significant improvement on intent classification accuracy, slot filling F1, and sentence-level semantic frame accuracy on several public benchmark datasets, compared to the attention-based recurrent neural network models and slot-gated models.},
  urldate = {2019-03-01},
  date = {2019-02-28},
  keywords = {Computer Science - Computation and Language},
  author = {Chen, Qian and Zhuo, Zhu and Wang, Wen},
  file = {/home/dimitri/Nextcloud/Zotero/storage/ZNEXFXZ9/Chen et al. - 2019 - BERT for Joint Intent Classification and Slot Fill.pdf;/home/dimitri/Nextcloud/Zotero/storage/QK6RL8QE/1902.html}
}

@book{hatcherAlgebraicTopology2002,
  location = {{Cambridge ; New York}},
  title = {Algebraic Topology},
  isbn = {978-0-521-79160-1 978-0-521-79540-1},
  pagetotal = {544},
  publisher = {{Cambridge University Press}},
  date = {2002},
  keywords = {Algebraic topology},
  author = {Hatcher, Allen},
  file = {/home/dimitri/Nextcloud/Zotero/storage/RLR9D4E7/Hatcher - 2002 - Algebraic topology.pdf}
}

@incollection{selingerSurveyGraphicalLanguages2010,
  location = {{Berlin, Heidelberg}},
  title = {A {{Survey}} of {{Graphical Languages}} for {{Monoidal Categories}}},
  volume = {813},
  isbn = {978-3-642-12820-2 978-3-642-12821-9},
  url = {http://link.springer.com/10.1007/978-3-642-12821-9_4},
  booktitle = {New {{Structures}} for {{Physics}}},
  publisher = {{Springer Berlin Heidelberg}},
  urldate = {2019-03-03},
  date = {2010},
  pages = {289-355},
  author = {Selinger, P.},
  editor = {Coecke, Bob},
  file = {/home/dimitri/Nextcloud/Zotero/storage/KC4I98FN/selinger2010.pdf;/home/dimitri/Nextcloud/Zotero/storage/SL4ZQNDI/Selinger - 2010 - A Survey of Graphical Languages for Monoidal Categ.pdf},
  doi = {10.1007/978-3-642-12821-9_4}
}

@article{thurstonProofProgressMathematics1994,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {math/9404236},
  title = {On Proof and Progress in Mathematics},
  url = {http://arxiv.org/abs/math/9404236},
  abstract = {In response to Jaffe and Quinn [math.HO/9307227], the author discusses forms of progress in mathematics that are not captured by formal proofs of theorems, especially in his own work in the theory of foliations and geometrization of 3-manifolds and dynamical systems.},
  urldate = {2019-03-03},
  date = {1994-03-31},
  keywords = {Mathematics - History and Overview},
  author = {Thurston, William P.},
  file = {/home/dimitri/Nextcloud/Zotero/storage/H2ALBXMG/Thurston - 1994 - On proof and progress in mathematics.pdf;/home/dimitri/Nextcloud/Zotero/storage/RZ72RXLT/9404236.html}
}

@article{dysonMissedOpportunities1972,
  langid = {english},
  title = {Missed Opportunities},
  volume = {78},
  issn = {0002-9904, 1936-881X},
  url = {https://projecteuclid.org/euclid.bams/1183533964},
  abstract = {Project Euclid - mathematics and statistics online},
  number = {5},
  journaltitle = {Bulletin of the American Mathematical Society},
  shortjournal = {Bull. Amer. Math. Soc.},
  urldate = {2019-03-03},
  date = {1972-09},
  pages = {635-652},
  author = {Dyson, Freeman J.},
  file = {/home/dimitri/Nextcloud/Zotero/storage/H3LX3XXA/Dyson - 1972 - Missed opportunities.pdf;/home/dimitri/Nextcloud/Zotero/storage/U74WURKB/DPubS.html}
}

@article{atiyahYangMillsEquationsRiemann1983,
  langid = {english},
  title = {The {{Yang}}-{{Mills Equations}} over {{Riemann Surfaces}}},
  volume = {308},
  issn = {1364-503X, 1471-2962},
  url = {http://rsta.royalsocietypublishing.org/cgi/doi/10.1098/rsta.1983.0017},
  doi = {10.1098/rsta.1983.0017},
  number = {1505},
  journaltitle = {Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences},
  urldate = {2019-03-03},
  date = {1983-03-17},
  pages = {523-615},
  author = {Atiyah, M. F. and Bott, R.},
  file = {/home/dimitri/Nextcloud/Zotero/storage/GBS72BZL/atiyah1983.pdf;/home/dimitri/Nextcloud/Zotero/storage/YN8V8F95/Atiyah and Bott - 1983 - The Yang-Mills Equations over Riemann Surfaces.pdf}
}

@article{chowBeginnerGuideForcing2007,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {0712.1320},
  primaryClass = {math},
  title = {A Beginner's Guide to Forcing},
  url = {http://arxiv.org/abs/0712.1320},
  abstract = {This expository paper, aimed at the reader without much background in set theory or logic, gives an overview of Cohen's proof (via forcing) of the independence of the continuum hypothesis. It emphasizes the broad outlines and the intuitive motivation while omitting most of the proofs. The reader must of course consult standard textbooks for the missing details, but this article provides a map of the forest so that the beginner will not get lost while forging through the trees.},
  urldate = {2019-03-03},
  date = {2007-12-09},
  keywords = {Mathematics - History and Overview,03E35,Mathematics - Logic},
  author = {Chow, Timothy Y.},
  file = {/home/dimitri/Nextcloud/Zotero/storage/4IDDXM7R/Chow - 2007 - A beginner's guide to forcing.pdf;/home/dimitri/Nextcloud/Zotero/storage/KCKK5CUV/0712.html}
}

@article{taoWhatGoodMathematics2007,
  langid = {english},
  title = {What Is Good Mathematics?},
  volume = {44},
  issn = {0273-0979},
  url = {http://www.ams.org/journal-getitem?pii=S0273-0979-07-01168-8},
  doi = {10.1090/S0273-0979-07-01168-8},
  number = {04},
  journaltitle = {Bulletin of the American Mathematical Society},
  urldate = {2019-03-03},
  date = {2007-05-02},
  pages = {623-635},
  author = {Tao, Terence},
  file = {/home/dimitri/Nextcloud/Zotero/storage/PTPNS3TY/Tao - 2007 - What is good mathematics.pdf}
}

@article{doyleDivisionThree2006,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {math/0605779},
  title = {Division by Three},
  url = {http://arxiv.org/abs/math/0605779},
  abstract = {We prove without appeal to the Axiom of Choice that for any sets A and B, if there is a one-to-one correspondence between 3 cross A and 3 cross B then there is a one-to-one correspondence between A and B. The first such proof, due to Lindenbaum, was announced by Lindenbaum and Tarski in 1926, and subsequently `lost'; Tarski published an alternative proof in 1949. We argue that the proof presented here follows Lindenbaum's original.},
  urldate = {2019-03-03},
  date = {2006-05-31},
  keywords = {Mathematics - Combinatorics,Mathematics - Logic,03E10 (Primary),03E25 (Secondary)},
  author = {Doyle, Peter G. and Conway, John Horton},
  file = {/home/dimitri/Nextcloud/Zotero/storage/GIR879I2/Doyle and Conway - 2006 - Division by three.pdf;/home/dimitri/Nextcloud/Zotero/storage/UZJYUJPP/0605779.html}
}

@inbook{wignerUnreasonableEffectivenessMathematics1990,
  langid = {english},
  title = {The {{Unreasonable Effectiveness}} of {{Mathematics}} in the {{Natural Sciences}}},
  isbn = {978-981-02-0233-0 978-981-4503-48-8},
  url = {http://www.worldscientific.com/doi/abs/10.1142/9789814503488_0018},
  booktitle = {Mathematics and {{Science}}},
  publisher = {{WORLD SCIENTIFIC}},
  urldate = {2019-03-03},
  date = {1990-08},
  pages = {291-306},
  author = {Wigner, Eugene P.},
  bookauthor = {Mickens, Ronald E},
  file = {/home/dimitri/Nextcloud/Zotero/storage/DSC9HUFQ/Wigner - 1990 - The Unreasonable Effectiveness of Mathematics in t.pdf},
  doi = {10.1142/9789814503488_0018}
}

@article{lawvereElementaryTheoryCategory1964,
  title = {An {{Elementary Theory}} of the {{Category}} of {{Sets}}},
  volume = {52},
  issn = {0027-8424},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC300477/},
  number = {6},
  journaltitle = {Proceedings of the National Academy of Sciences of the United States of America},
  shortjournal = {Proc Natl Acad Sci U S A},
  urldate = {2019-03-03},
  date = {1964-12},
  pages = {1506-1511},
  author = {Lawvere, F. William},
  file = {/home/dimitri/Nextcloud/Zotero/storage/2NDFYPCM/tr11.pdf;/home/dimitri/Nextcloud/Zotero/storage/CSCSBBDJ/Lawvere - 1964 - AN ELEMENTARY THEORY OF THE CATEGORY OF SETS.pdf},
  eprinttype = {pmid},
  eprint = {16591243},
  pmcid = {PMC300477}
}

@article{neftciReinforcementLearningArtificial2019,
  langid = {english},
  title = {Reinforcement Learning in Artificial and Biological Systems},
  issn = {2522-5839},
  url = {https://www.nature.com/articles/s42256-019-0025-4},
  doi = {10.1038/s42256-019-0025-4},
  abstract = {Research on reinforcement learning in artificial agents focuses on a single complex problem within a static environment. In biological agents, research focuses on simple learning problems embedded in flexible, dynamic environments. The authors review the literature on these topics and suggest areas of synergy between them.},
  journaltitle = {Nature Machine Intelligence},
  urldate = {2019-03-05},
  date = {2019-03-04},
  pages = {1},
  author = {Neftci, Emre O. and Averbeck, Bruno B.},
  file = {/home/dimitri/Nextcloud/Zotero/storage/99HN7D26/Neftci and Averbeck - 2019 - Reinforcement learning in artificial and biologica.pdf;/home/dimitri/Nextcloud/Zotero/storage/95NTAIEJ/s42256-019-0025-4.html}
}

@article{shannonPredictionEntropyPrinted1951,
  langid = {english},
  title = {Prediction and {{Entropy}} of {{Printed English}}},
  volume = {30},
  issn = {00058580},
  url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6773263},
  doi = {10.1002/j.1538-7305.1951.tb01366.x},
  number = {1},
  journaltitle = {Bell System Technical Journal},
  urldate = {2019-03-05},
  date = {1951-01},
  pages = {50-64},
  author = {Shannon, C. E.},
  file = {/home/dimitri/Nextcloud/Zotero/storage/UU5UGDA5/Shannon - 1951 - Prediction and Entropy of Printed English.pdf}
}

@article{gaoNeuralApproachesConversational2019,
  langid = {english},
  title = {Neural {{Approaches}} to {{Conversational AI}}},
  volume = {13},
  issn = {1554-0669, 1554-0677},
  url = {http://www.nowpublishers.com/article/Details/INR-074},
  doi = {10.1561/1500000074},
  number = {2-3},
  journaltitle = {Foundations and Trends® in Information Retrieval},
  urldate = {2019-03-06},
  date = {2019},
  pages = {127-298},
  author = {Gao, Jianfeng and Galley, Michel and Li, Lihong},
  file = {/home/dimitri/Nextcloud/Zotero/storage/M8Z7CBD8/Gao et al. - 2019 - Neural Approaches to Conversational AI.pdf}
}

@article{agrawalTensorFlowEagerMultiStage2019,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1903.01855},
  primaryClass = {cs},
  title = {{{TensorFlow Eager}}: {{A Multi}}-{{Stage}}, {{Python}}-{{Embedded DSL}} for {{Machine Learning}}},
  url = {http://arxiv.org/abs/1903.01855},
  shorttitle = {{{TensorFlow Eager}}},
  abstract = {TensorFlow Eager is a multi-stage, Python-embedded domain-specific language for hardware-accelerated machine learning, suitable for both interactive research and production. TensorFlow, which TensorFlow Eager extends, requires users to represent computations as dataflow graphs; this permits compiler optimizations and simplifies deployment but hinders rapid prototyping and run-time dynamism. TensorFlow Eager eliminates these usability costs without sacrificing the benefits furnished by graphs: It provides an imperative front-end to TensorFlow that executes operations immediately and a JIT tracer that translates Python functions composed of TensorFlow operations into executable dataflow graphs. TensorFlow Eager thus offers a multi-stage programming model that makes it easy to interpolate between imperative and staged execution in a single package.},
  urldate = {2019-03-06},
  date = {2019-02-26},
  keywords = {Computer Science - Machine Learning,Computer Science - Programming Languages},
  author = {Agrawal, Akshay and Modi, Akshay Naresh and Passos, Alexandre and Lavoie, Allen and Agarwal, Ashish and Shankar, Asim and Ganichev, Igor and Levenberg, Josh and Hong, Mingsheng and Monga, Rajat and Cai, Shanqing},
  file = {/home/dimitri/Nextcloud/Zotero/storage/ELIZ6PBQ/Agrawal et al. - 2019 - TensorFlow Eager A Multi-Stage, Python-Embedded D.pdf;/home/dimitri/Nextcloud/Zotero/storage/CAPFB9ER/1903.html}
}

@article{zielinskiPersistenceBagofWordsTopological2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1812.09245},
  primaryClass = {cs, math, stat},
  title = {Persistence {{Bag}}-of-{{Words}} for {{Topological Data Analysis}}},
  url = {http://arxiv.org/abs/1812.09245},
  abstract = {Persistent homology (PH) is a rigorous mathematical theory that provides a robust descriptor of data in the form of persistence diagrams (PDs). PDs exhibit, however, complex structure and are difficult to integrate in today's machine learning workflows. This paper introduces persistence bag-of-words: a novel and stable vectorized representation of PDs that enables the seamless integration with machine learning. Comprehensive experiments show that the new representation achieves state-of-the-art performance and beyond in much less time than alternative approaches.},
  urldate = {2019-03-06},
  date = {2018-12-21},
  keywords = {Statistics - Machine Learning,Mathematics - Algebraic Topology,Computer Science - Machine Learning},
  author = {Zieliński, Bartosz and Lipiński, Michał and Juda, Mateusz and Zeppelzauer, Matthias and Dłotko, Paweł},
  file = {/home/dimitri/Nextcloud/Zotero/storage/EUKEE2W9/Zieliński et al. - 2018 - Persistence Bag-of-Words for Topological Data Anal.pdf;/home/dimitri/Nextcloud/Zotero/storage/YRQJMESH/1812.html}
}

@article{saddikiPrimerCausalityData2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1809.02408},
  primaryClass = {stat},
  title = {A {{Primer}} on {{Causality}} in {{Data Science}}},
  url = {http://arxiv.org/abs/1809.02408},
  abstract = {Many questions in Data Science are fundamentally causal in that our objective is to learn the effect of some exposure, randomized or not, on an outcome interest. Even studies that are seemingly non-causal, such as those with the goal of prediction or prevalence estimation, have causal elements, including differential censoring or measurement. As a result, we, as Data Scientists, need to consider the underlying causal mechanisms that gave rise to the data, rather than simply the pattern or association observed in those data. In this work, we review the 'Causal Roadmap' of Petersen and van der Laan (2014) to provide an introduction to some key concepts in causal inference. Similar to other causal frameworks, the steps of the Roadmap include clearly stating the scientific question, defining of the causal model, translating the scientific question into a causal parameter, assessing the assumptions needed to express the causal parameter as a statistical estimand, implementation of statistical estimators including parametric and semi-parametric methods, and interpretation of our findings. We believe that using such a framework in Data Science will help to ensure that our statistical analyses are guided by the scientific question driving our research, while avoiding over-interpreting our results. We focus on the effect of an exposure occurring at a single time point and highlight the use of targeted maximum likelihood estimation (TMLE) with Super Learner.},
  urldate = {2019-03-06},
  date = {2018-09-07},
  keywords = {Statistics - Machine Learning,Statistics - Methodology,Statistics - Applications},
  author = {Saddiki, Hachem and Balzer, Laura B.},
  file = {/home/dimitri/Nextcloud/Zotero/storage/TXNFG8VZ/Saddiki and Balzer - 2018 - A Primer on Causality in Data Science.pdf;/home/dimitri/Nextcloud/Zotero/storage/BTIMHBEM/1809.html}
}

@article{carterActivationAtlas2019,
  langid = {english},
  title = {Activation {{Atlas}}},
  volume = {4},
  issn = {2476-0757},
  url = {https://distill.pub/2019/activation-atlas},
  doi = {10.23915/distill.00015},
  abstract = {By using feature inversion to visualize millions of activations from an image classification network, we create an explorable activation atlas of features the network has learned and what concepts it typically represents.},
  number = {3},
  journaltitle = {Distill},
  shortjournal = {Distill},
  urldate = {2019-03-07},
  date = {2019-03-06},
  pages = {e15},
  author = {Carter, Shan and Armstrong, Zan and Schubert, Ludwig and Johnson, Ian and Olah, Chris},
  file = {/home/dimitri/Nextcloud/Zotero/storage/ZT5ASFQ3/activation-atlas.html}
}

@article{gauciHorizonFacebookOpen2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1811.00260},
  primaryClass = {cs, stat},
  title = {Horizon: {{Facebook}}'s {{Open Source Applied Reinforcement Learning Platform}}},
  url = {http://arxiv.org/abs/1811.00260},
  shorttitle = {Horizon},
  abstract = {In this paper we present Horizon, Facebook's open source applied reinforcement learning (RL) platform. Horizon is an end-to-end platform designed to solve industry applied RL problems where datasets are large (millions to billions of observations), the feedback loop is slow (vs. a simulator), and experiments must be done with care because they don't run in a simulator. Unlike other RL platforms, which are often designed for fast prototyping and experimentation, Horizon is designed with production use cases as top of mind. The platform contains workflows to train popular deep RL algorithms and includes data preprocessing, feature transformation, distributed training, counterfactual policy evaluation, and optimized serving. We also showcase real examples of where models trained with Horizon significantly outperformed and replaced supervised learning systems at Facebook.},
  urldate = {2019-03-07},
  date = {2018-11-01},
  keywords = {Statistics - Machine Learning,Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  author = {Gauci, Jason and Conti, Edoardo and Liang, Yitao and Virochsiri, Kittipat and He, Yuchen and Kaden, Zachary and Narayanan, Vivek and Ye, Xiaohui},
  file = {/home/dimitri/Nextcloud/Zotero/storage/HL9JW6U7/Gauci et al. - 2018 - Horizon Facebook's Open Source Applied Reinforcem.pdf;/home/dimitri/Nextcloud/Zotero/storage/7KW2UHCC/1811.html}
}

@inproceedings{ultesDomainIndependentUserSatisfaction2017,
  langid = {english},
  title = {Domain-{{Independent User Satisfaction Reward Estimation}} for {{Dialogue Policy Learning}}},
  url = {http://www.isca-speech.org/archive/Interspeech_2017/abstracts/1032.html},
  doi = {10.21437/Interspeech.2017-1032},
  eventtitle = {Interspeech 2017},
  booktitle = {Interspeech 2017},
  publisher = {{ISCA}},
  urldate = {2019-03-07},
  date = {2017-08-20},
  pages = {1721-1725},
  author = {Ultes, Stefan and Budzianowski, Paweł and Casanueva, Iñigo and Mrkšić, Nikola and Rojas-Barahona, Lina and Su, Pei-Hao and Wen, Tsung-Hsien and Gašić, Milica and Young, Steve},
  file = {/home/dimitri/Nextcloud/Zotero/storage/V5JG59H8/Ultes et al. - 2017 - Domain-Independent User Satisfaction Reward Estima.PDF}
}

@article{schmittInteractionQualityAssessing2015,
  langid = {english},
  title = {Interaction {{Quality}}: {{Assessing}} the Quality of Ongoing Spoken Dialog Interaction by Experts—{{And}} How It Relates to User Satisfaction},
  volume = {74},
  issn = {01676393},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0167639315000679},
  doi = {10.1016/j.specom.2015.06.003},
  shorttitle = {Interaction {{Quality}}},
  journaltitle = {Speech Communication},
  urldate = {2019-03-07},
  date = {2015-11},
  pages = {12-36},
  author = {Schmitt, Alexander and Ultes, Stefan},
  file = {/home/dimitri/Nextcloud/Zotero/storage/CQ4NGB4H/Schmitt and Ultes - 2015 - Interaction Quality Assessing the quality of ongo.pdf}
}

@book{dasguptaAlgorithms2008,
  langid = {english},
  location = {{Boston}},
  title = {Algorithms},
  edition = {1. ed},
  isbn = {978-0-07-352340-8},
  pagetotal = {320},
  publisher = {{McGraw-Hill Higher Education}},
  date = {2008},
  author = {Dasgupta, Sanjoy and Papadimitriou, Christos H. and Vazirani, Umesh Virkumar},
  file = {/home/dimitri/Nextcloud/Zotero/storage/S4W6S72R/algorithms.pdf},
  note = {OCLC: 255769739}
}

@incollection{mitchellTypeSystemsProgramming1990,
  langid = {english},
  title = {Type {{Systems}} for {{Programming Languages}}},
  isbn = {978-0-444-88074-1},
  url = {https://linkinghub.elsevier.com/retrieve/pii/B9780444880741500135},
  booktitle = {Formal {{Models}} and {{Semantics}}},
  publisher = {{Elsevier}},
  urldate = {2019-03-07},
  date = {1990},
  pages = {365-458},
  author = {Mitchell, John C.},
  file = {/home/dimitri/Nextcloud/Zotero/storage/CMXVGCJH/pierce_type_systems.pdf},
  doi = {10.1016/B978-0-444-88074-1.50013-5}
}

@article{jungDatadrivenUserSimulation2009,
  langid = {english},
  title = {Data-Driven User Simulation for Automated Evaluation of Spoken Dialog Systems},
  volume = {23},
  issn = {08852308},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0885230809000151},
  doi = {10.1016/j.csl.2009.03.002},
  number = {4},
  journaltitle = {Computer Speech \& Language},
  urldate = {2019-03-11},
  date = {2009-10},
  pages = {479-509},
  author = {Jung, Sangkeun and Lee, Cheongjae and Kim, Kyungduk and Jeong, Minwoo and Lee, Gary Geunbae},
  file = {/home/dimitri/Nextcloud/Zotero/storage/9YJELEVL/Jung et al. - 2009 - Data-driven user simulation for automated evaluati.pdf}
}

@article{argallSurveyRobotLearning2009,
  langid = {english},
  title = {A Survey of Robot Learning from Demonstration},
  volume = {57},
  issn = {09218890},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0921889008001772},
  doi = {10.1016/j.robot.2008.10.024},
  number = {5},
  journaltitle = {Robotics and Autonomous Systems},
  urldate = {2019-03-11},
  date = {2009-05},
  pages = {469-483},
  author = {Argall, Brenna D. and Chernova, Sonia and Veloso, Manuela and Browning, Brett},
  file = {/home/dimitri/Nextcloud/Zotero/storage/P2XTNIX9/argall2009.pdf}
}

@article{koberReinforcementLearningRobotics2013,
  langid = {english},
  title = {Reinforcement Learning in Robotics: {{A}} Survey},
  volume = {32},
  issn = {0278-3649, 1741-3176},
  url = {http://journals.sagepub.com/doi/10.1177/0278364913495721},
  doi = {10.1177/0278364913495721},
  shorttitle = {Reinforcement Learning in Robotics},
  number = {11},
  journaltitle = {The International Journal of Robotics Research},
  urldate = {2019-03-11},
  date = {2013-09},
  pages = {1238-1274},
  author = {Kober, Jens and Bagnell, J. Andrew and Peters, Jan},
  file = {/home/dimitri/Nextcloud/Zotero/storage/79PR99G5/Kober et al. - 2013 - Reinforcement learning in robotics A survey.pdf;/home/dimitri/Nextcloud/Zotero/storage/8PVC5N5V/kober-ijrr2013-rl-in-robotics-survey.pdf}
}

@inproceedings{cuayahuitlHumancomputerDialogueSimulation2005,
  location = {{San Juan, Puerto Rico}},
  title = {Human-Computer Dialogue Simulation Using Hidden {{Markov}} Models},
  isbn = {978-0-7803-9479-7},
  url = {http://ieeexplore.ieee.org/document/1566485/},
  doi = {10.1109/ASRU.2005.1566485},
  eventtitle = {{{IEEE Workshop}} on {{Automatic Speech Recognition}} and {{Understanding}}, 2005.},
  booktitle = {{{IEEE Workshop}} on {{Automatic Speech Recognition}} and {{Understanding}}, 2005.},
  publisher = {{IEEE}},
  urldate = {2019-03-11},
  date = {2005},
  pages = {290-295},
  author = {Cuayahuitl, H. and Renals, S. and Lemon, O. and Shimodaira, H.},
  file = {/home/dimitri/Nextcloud/Zotero/storage/U7AMV3YR/Cuayahuitl et al. - 2005 - Human-computer dialogue simulation using hidden Ma.pdf}
}

@article{schatzmannStatisticalUserSimulation2007,
  title = {Statistical User Simulation with a Hidden Agenda},
  volume = {273282},
  number = {9},
  journaltitle = {Proc SIGDial, Antwerp},
  date = {2007},
  author = {Schatzmann, Jost and Thomson, Blaise and Young, Steve},
  file = {/home/dimitri/Nextcloud/Zotero/storage/4C7NBMBV/Schatzmann et al. - 2007 - Statistical user simulation with a hidden agenda.pdf}
}

@article{aroraSurveyInverseReinforcement2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1806.06877},
  primaryClass = {cs, stat},
  title = {A {{Survey}} of {{Inverse Reinforcement Learning}}: {{Challenges}}, {{Methods}} and {{Progress}}},
  url = {http://arxiv.org/abs/1806.06877},
  shorttitle = {A {{Survey}} of {{Inverse Reinforcement Learning}}},
  abstract = {Inverse reinforcement learning is the problem of inferring the reward function of an observed agent, given its policy or behavior. Researchers perceive IRL both as a problem and as a class of methods. By categorically surveying the current literature in IRL, this article serves as a reference for researchers and practitioners in machine learning to understand the challenges of IRL and select the approaches best suited for the problem on hand. The survey formally introduces the IRL problem along with its central challenges which include accurate inference, generalizability, correctness of prior knowledge, and growth in solution complexity with problem size. The article elaborates how the current methods mitigate these challenges. We further discuss the extensions of traditional IRL methods: (i) inaccurate and incomplete perception, (ii) incomplete model, (iii) multiple rewards, and (iv) non-linear reward functions. This discussion concludes with some broad advances in the research area and currently open research questions.},
  urldate = {2019-03-11},
  date = {2018-06-18},
  keywords = {Statistics - Machine Learning,Computer Science - Machine Learning},
  author = {Arora, Saurabh and Doshi, Prashant},
  file = {/home/dimitri/Nextcloud/Zotero/storage/43IDKDFF/Arora and Doshi - 2018 - A Survey of Inverse Reinforcement Learning Challe.pdf;/home/dimitri/Nextcloud/Zotero/storage/J3HWY677/1806.html}
}

@article{wulfmeierMaximumEntropyDeep2015,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1507.04888},
  primaryClass = {cs},
  title = {Maximum {{Entropy Deep Inverse Reinforcement Learning}}},
  url = {http://arxiv.org/abs/1507.04888},
  abstract = {This paper presents a general framework for exploiting the representational capacity of neural networks to approximate complex, nonlinear reward functions in the context of solving the inverse reinforcement learning (IRL) problem. We show in this context that the Maximum Entropy paradigm for IRL lends itself naturally to the efficient training of deep architectures. At test time, the approach leads to a computational complexity independent of the number of demonstrations, which makes it especially well-suited for applications in life-long learning scenarios. Our approach achieves performance commensurate to the state-of-the-art on existing benchmarks while exceeding on an alternative benchmark based on highly varying reward structures. Finally, we extend the basic architecture - which is equivalent to a simplified subclass of Fully Convolutional Neural Networks (FCNNs) with width one - to include larger convolutions in order to eliminate dependency on precomputed spatial features and work on raw input representations.},
  urldate = {2019-03-11},
  date = {2015-07-17},
  keywords = {Computer Science - Machine Learning},
  author = {Wulfmeier, Markus and Ondruska, Peter and Posner, Ingmar},
  file = {/home/dimitri/Nextcloud/Zotero/storage/IQ8S2MNH/Wulfmeier et al. - 2015 - Maximum Entropy Deep Inverse Reinforcement Learnin.pdf;/home/dimitri/Nextcloud/Zotero/storage/VYI79XYA/1507.html}
}

@article{orusMathematicalFoundationsMatrix2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1710.00372},
  primaryClass = {quant-ph},
  title = {Mathematical Foundations of Matrix Syntax},
  url = {http://arxiv.org/abs/1710.00372},
  abstract = {Matrix syntax is a formal model of syntactic relations in language. The purpose of this paper is to explain its mathematical foundations, for an audience with some formal background. We make an axiomatic presentation, motivating each axiom on linguistic and practical grounds. The resulting mathematical structure resembles some aspects of quantum mechanics. Matrix syntax allows us to describe a number of language phenomena that are otherwise very difficult to explain, such as linguistic chains, and is arguably a more economical theory of language than most of the theories proposed in the context of the minimalist program in linguistics. In particular, sentences are naturally modelled as vectors in a Hilbert space with a tensor product structure, built from 2x2 matrices belonging to some specific group.},
  urldate = {2019-03-12},
  date = {2017-10-01},
  keywords = {Computer Science - Computation and Language,Quantum Physics},
  author = {Orus, Roman and Martin, Roger and Uriagereka, Juan},
  file = {/home/dimitri/Nextcloud/Zotero/storage/PS8P9IJR/Orus et al. - 2017 - Mathematical foundations of matrix syntax.pdf;/home/dimitri/Nextcloud/Zotero/storage/TMDU8A7F/1710.html}
}

@article{dahlinGettingStartedParticle2015,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1511.01707},
  primaryClass = {q-fin, stat},
  title = {Getting {{Started}} with {{Particle Metropolis}}-{{Hastings}} for {{Inference}} in {{Nonlinear Dynamical Models}}},
  url = {http://arxiv.org/abs/1511.01707},
  abstract = {This tutorial provides a gentle introduction to the particle Metropolis-Hastings (PMH) algorithm for parameter inference in nonlinear state-space models together with a software implementation in the statistical programming language R. We employ a step-by-step approach to develop an implementation of the PMH algorithm (and the particle filter within) together with the reader. This final implementation is also available as the package pmhtutorial in the CRAN repository. Throughout the tutorial, we provide some intuition as to how the algorithm operates and discuss some solutions to problems that might occur in practice. To illustrate the use of PMH, we consider parameter inference in a linear Gaussian state-space model with synthetic data and a nonlinear stochastic volatility model with real-world data.},
  urldate = {2019-03-13},
  date = {2015-11-05},
  keywords = {Statistics - Machine Learning,Statistics - Computation,Quantitative Finance - Statistical Finance},
  author = {Dahlin, Johan and Schön, Thomas B.},
  file = {/home/dimitri/Nextcloud/Zotero/storage/FAM2AQDY/Dahlin and Schön - 2015 - Getting Started with Particle Metropolis-Hastings .pdf;/home/dimitri/Nextcloud/Zotero/storage/JQ57B8PK/1511.html}
}

@book{teschlOrdinaryDifferentialEquations2012,
  langid = {english},
  title = {Ordinary Differential Equations and Dynamical Systems},
  isbn = {978-0-8218-9104-9 978-0-8218-8328-0},
  date = {2012},
  author = {Teschl, Gerald},
  file = {/home/dimitri/Nextcloud/Zotero/storage/RZDA2JYB/Teschl - 2012 - Ordinary differential equations and dynamical syst.pdf},
  note = {OCLC: 975369323}
}

@book{alloucheAutomaticSequencesTheory2003,
  langid = {english},
  location = {{New York}},
  title = {Automatic Sequences: Theory, Applications, Generalizations.},
  isbn = {978-0-511-07054-9},
  url = {http://public.eblib.com/choice/publicfullrecord.aspx?p=218240},
  shorttitle = {Automatic Sequences},
  abstract = {This is a book about the sequences of symbols that can be generated by simple models of computation called 'finite automata'. It starts from first principles and develops the basic theory, then demonstrates applications to problems in number theory and physics. Suitable for graduates or advanced undergraduates.},
  publisher = {{Cambridge Univ Press}},
  urldate = {2019-03-13},
  date = {2003},
  author = {Allouche, Jean-Paul},
  file = {/home/dimitri/Nextcloud/Zotero/storage/9UZ52ARD/Allouche - 2003 - Automatic sequences theory, applications, general.pdf},
  note = {OCLC: 171135843}
}

@book{langeOptimization2013,
  location = {{New York}},
  title = {Optimization},
  edition = {2nd ed},
  isbn = {978-1-4614-5837-1 978-1-4614-5838-8},
  abstract = {Finite-dimensional optimization problems occur throughout the mathematical sciences. The majority of these problems cannot be solved analytically. This introduction to optimization attempts to strike a balance between presentation of mathematical theory and development of numerical algorithms. Building on students' skills in calculus and linear algebra, the text provides a rigorous exposition without undue abstraction. Its stress on statistical applications will be especially appealing to graduate students of statistics and biostatistics. The intended audience also includes students in applied mathematics, computational biology, computer science, economics, and physics who want to see rigorous mathematics combined with real applications. In this second edition, the emphasis remains on finite-dimensional optimization. New material has been added on the MM algorithm, block descent and ascent, and the calculus of variations. Convex calculus is now treated in much greater depth. Advanced topics such as the Fenchel conjugate, subdifferentials, duality, feasibility, alternating projections, projected gradient methods, exact penalty methods, and Bregman iteration will equip students with the essentials for understanding modern data mining techniques in high dimensions --},
  pagetotal = {529},
  number = {95},
  series = {Springer Texts in Statistics},
  publisher = {{Springer}},
  date = {2013},
  keywords = {Mathematical optimization},
  author = {Lange, Kenneth},
  file = {/home/dimitri/Nextcloud/Zotero/storage/K7XQIA4Q/Lange - 2013 - Optimization.pdf},
  note = {OCLC: ocn814705995}
}

@book{wassermanAllStatisticsConcise2010,
  langid = {english},
  location = {{New York, NY}},
  title = {All of Statistics: A Concise Course in Statistical Inference},
  edition = {Corrected second printing, 2005},
  isbn = {978-0-387-21736-9 978-1-4419-2322-6},
  shorttitle = {All of Statistics},
  pagetotal = {442},
  series = {Springer Texts in Statistics},
  publisher = {{Springer}},
  date = {2010},
  author = {Wasserman, Larry},
  file = {/home/dimitri/Nextcloud/Zotero/storage/SJEJFNC3/Wasserman - 2010 - All of statistics a concise course in statistical.pdf},
  note = {OCLC: 837651382}
}

@book{wassermanAllNonparametricStatistics2010,
  langid = {english},
  location = {{New York, NY}},
  title = {All of Nonparametric Statistics},
  isbn = {978-0-387-30623-0 978-1-4419-2044-7},
  pagetotal = {268},
  series = {Springer {{Texts}} in {{Statistics}}},
  publisher = {{Springer}},
  date = {2010},
  author = {Wasserman, Larry},
  file = {/home/dimitri/Nextcloud/Zotero/storage/FGYXWAST/Wasserman - 2010 - All of nonparametric statistics.pdf},
  note = {OCLC: 837652071}
}

@book{robertMonteCarloStatistical2004,
  location = {{New York}},
  title = {Monte {{Carlo}} Statistical Methods},
  edition = {2nd ed},
  isbn = {978-0-387-21239-5},
  pagetotal = {645},
  series = {Springer Texts in Statistics},
  publisher = {{Springer}},
  date = {2004},
  keywords = {Mathematical statistics,Monte Carlo method},
  author = {Robert, Christian P. and Casella, George},
  file = {/home/dimitri/Nextcloud/Zotero/storage/ULGV2I96/Robert and Casella - 2004 - Monte Carlo statistical methods.pdf}
}

@book{liuMonteCarloStrategies2013,
  langid = {english},
  location = {{New York, NY}},
  title = {Monte {{Carlo Strategies}} in {{Scientific Computing}}.},
  isbn = {978-0-387-76371-2},
  url = {http://public.eblib.com/choice/publicfullrecord.aspx?p=5575065},
  abstract = {This book provides an up-to-date treatment of the Monte Carlo method and develops a common framework under which various Monte Carlo techniques can be ""standardized"" and compared. It can be used as a textbook for a graduate-level course on Monte Carlo methods.},
  publisher = {{Springer}},
  urldate = {2019-03-13},
  date = {2013},
  author = {Liu, Jun S},
  file = {/home/dimitri/Nextcloud/Zotero/storage/IHHWUG4F/Liu - 2013 - Monte Carlo Strategies in Scientific Computing..pdf},
  note = {OCLC: 1066196614}
}

@book{norvigParadigmsArtificialIntelligence2014,
  langid = {english},
  title = {Paradigms of Artificial Intelligence Programming: Case Studies in {{Common Lisp}}},
  isbn = {978-1-55860-191-8 978-0-08-057115-7},
  shorttitle = {Paradigms of Artificial Intelligence Programming},
  abstract = {"Paradigms of Artificial Intelligence Programming is the first text to teach advanced Common Lisp techniques in the context of building major AI systems. By reconstructing authentic, complex AI programs using state-of-the-art Common Lisp, the book teaches students and professionals how to build and debug robust practical programs, while demonstrating superior programming style and important AI concepts. The author strongly emphasizes the practical performance issues involved in writing real working programs of significant size. Chapters on troubleshooting and efficiency are included, along with a discussion of the fundamentals of object-oriented programming and a description of the main CLOS functions. This volume is an excellent text for a course on AI programming, a useful supplement for general AI courses and an indispensable reference for the professional programmer."--Provided by publisher.},
  date = {2014},
  author = {Norvig, Peter and {Safari Books Online (Firm)}},
  file = {/home/dimitri/Nextcloud/Zotero/storage/CCUFZEE4/PAIP-part1.pdf;/home/dimitri/Nextcloud/Zotero/storage/Z4ML33PJ/PAIP-part2.pdf},
  note = {OCLC: 987937635}
}

@book{russellArtificialIntelligenceModern2010,
  location = {{Upper Saddle River}},
  title = {Artificial Intelligence: A Modern Approach},
  edition = {3rd ed},
  isbn = {978-0-13-604259-4},
  shorttitle = {Artificial Intelligence},
  pagetotal = {1132},
  series = {Prentice {{Hall}} Series in Artificial Intelligence},
  publisher = {{Prentice Hall}},
  date = {2010},
  keywords = {Artificial intelligence},
  author = {Russell, Stuart J. and Norvig, Peter and Davis, Ernest},
  file = {/home/dimitri/Nextcloud/Zotero/storage/7LFXZBE3/Russell et al. - 2010 - Artificial intelligence a modern approach.pdf}
}

@collection{highamPrincetonCompanionApplied2015,
  location = {{Princeton}},
  title = {The {{Princeton}} Companion to Applied Mathematics},
  isbn = {978-0-691-15039-0},
  pagetotal = {994},
  publisher = {{Princeton University Press}},
  date = {2015},
  keywords = {Mathematics,Algebra,Mathematical models},
  editor = {Higham, Nicholas J. and Dennis, Mark R.},
  file = {/home/dimitri/Nextcloud/Zotero/storage/D9HISQD3/Higham and Dennis - 2015 - The Princeton companion to applied mathematics.pdf}
}

@article{cohenBackFutureDialogue2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1812.01144},
  primaryClass = {cs},
  title = {Back to the {{Future}} for {{Dialogue Research}}: {{A Position Paper}}},
  url = {http://arxiv.org/abs/1812.01144},
  shorttitle = {Back to the {{Future}} for {{Dialogue Research}}},
  abstract = {This short position paper is intended to provide a critique of current approaches to dialogue, as well as a roadmap for collaborative dialogue research. It is unapologetically opinionated, but informed by 40 years of dialogue re-search. No attempt is made to be comprehensive. The paper will discuss current research into building so-called "chatbots", slot-filling dialogue systems, and plan-based dialogue systems. For further discussion of some of these issues, please see (Allen et al., in press).},
  urldate = {2019-03-14},
  date = {2018-12-03},
  keywords = {Computer Science - Artificial Intelligence},
  author = {Cohen, Philip R.},
  file = {/home/dimitri/Nextcloud/Zotero/storage/UGTD3F38/Cohen - 2018 - Back to the Future for Dialogue Research A Positi.pdf;/home/dimitri/Nextcloud/Zotero/storage/VCX8PMGT/1812.html}
}

@article{agrawalTrustworthyResponsibleInterpretable2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1811.07600},
  primaryClass = {cs},
  title = {A {{Trustworthy}}, {{Responsible}} and {{Interpretable System}} to {{Handle Chit Chat}} in {{Conversational Bots}}},
  url = {http://arxiv.org/abs/1811.07600},
  abstract = {Most often, chat-bots are built to solve the purpose of a search engine or a human assistant: Their primary goal is to provide information to the user or help them complete a task. However, these chat-bots are incapable of responding to unscripted queries like "Hi, what's up", "What's your favourite food". Human evaluation judgments show that 4 humans come to a consensus on the intent of a given query which is from chat domain only 77\% of the time, thus making it evident how non-trivial this task is. In our work, we show why it is difficult to break the chitchat space into clearly defined intents. We propose a system to handle this task in chat-bots, keeping in mind scalability, interpretability, appropriateness, trustworthiness, relevance and coverage. Our work introduces a pipeline for query understanding in chitchat using hierarchical intents as well as a way to use seq-seq auto-generation models in professional bots. We explore an interpretable model for chat domain detection and also show how various components such as adult/offensive classification, grammars/regex patterns, curated personality based responses, generic guided evasive responses and response generation models can be combined in a scalable way to solve this problem.},
  urldate = {2019-03-14},
  date = {2018-11-19},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  author = {Agrawal, Parag and Suri, Anshuman and Menon, Tulasi},
  file = {/home/dimitri/Nextcloud/Zotero/storage/U6RGSIUQ/Agrawal et al. - 2018 - A Trustworthy, Responsible and Interpretable Syste.pdf;/home/dimitri/Nextcloud/Zotero/storage/PII5TDN8/1811.html}
}

@collection{furnkranzPreferenceLearning2010,
  langid = {english},
  location = {{Berlin}},
  title = {Preference Learning},
  isbn = {978-3-642-14125-6 978-3-642-14124-9},
  pagetotal = {466},
  publisher = {{Springer}},
  date = {2010},
  editor = {Fürnkranz, Johannes and Hüllermeier, Eyke and ECML PKDD and ECML PKDD},
  file = {/home/dimitri/Nextcloud/Zotero/storage/5DNWZPA2/Fürnkranz et al. - 2010 - Preference learning.pdf},
  note = {OCLC: 698574743}
}

@article{gutierrezOrdinalRegressionMethods2016,
  title = {Ordinal {{Regression Methods}}: {{Survey}} and {{Experimental Study}}},
  volume = {28},
  issn = {1041-4347},
  url = {http://ieeexplore.ieee.org/document/7161338/},
  doi = {10.1109/TKDE.2015.2457911},
  shorttitle = {Ordinal {{Regression Methods}}},
  number = {1},
  journaltitle = {IEEE Transactions on Knowledge and Data Engineering},
  urldate = {2019-03-14},
  date = {2016-01-01},
  pages = {127-146},
  author = {Gutierrez, Pedro Antonio and Perez-Ortiz, Maria and Sanchez-Monedero, Javier and Fernandez-Navarro, Francisco and Hervas-Martinez, Cesar},
  file = {/home/dimitri/Nextcloud/Zotero/storage/8J2HSYUP/Gutierrez et al. - 2016 - Ordinal Regression Methods Survey and Experimenta.pdf}
}

@article{portTopologicalAnalysisSyntactic2019,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1903.05181},
  primaryClass = {cs, math},
  title = {Topological {{Analysis}} of {{Syntactic Structures}}},
  url = {http://arxiv.org/abs/1903.05181},
  abstract = {We use the persistent homology method of topological data analysis and dimensional analysis techniques to study data of syntactic structures of world languages. We analyze relations between syntactic parameters in terms of dimensionality, of hierarchical clustering structures, and of non-trivial loops. We show there are relations that hold across language families and additional relations that are family-specific. We then analyze the trees describing the merging structure of persistent connected components for languages in different language families and we show that they partly correlate to historical phylogenetic trees but with significant differences. We also show the existence of interesting non-trivial persistent first homology groups in various language families. We give examples where explicit generators for the persistent first homology can be identified, some of which appear to correspond to homoplasy phenomena, while others may have an explanation in terms of historical linguistics, corresponding to known cases of syntactic borrowing across different language subfamilies.},
  urldate = {2019-03-14},
  date = {2019-03-12},
  keywords = {Mathematics - Algebraic Topology,Computer Science - Computation and Language,91F20; 55U10; 55N35; 62-07},
  author = {Port, Alexander and Karidi, Taelin and Marcolli, Matilde},
  file = {/home/dimitri/Nextcloud/Zotero/storage/2BUT8P2A/Port et al. - 2019 - Topological Analysis of Syntactic Structures.pdf;/home/dimitri/Nextcloud/Zotero/storage/YJRPW7VM/1903.html}
}

@article{takhanovDimensionReductionOptimization2019,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1903.05083},
  primaryClass = {cs, math, stat},
  title = {Dimension Reduction as an Optimization Problem over a Set of Generalized Functions},
  url = {http://arxiv.org/abs/1903.05083},
  abstract = {Classical dimension reduction problem can be loosely formulated as a problem of finding a \$k\$-dimensional affine subspace of \$\{\textbackslash{}mathbb R\}\^n\$ onto which data points \$\{\textbackslash{}mathbf x\}\_1,\textbackslash{}cdots, \{\textbackslash{}mathbf x\}\_N\$ can be projected without loss of valuable information. We reformulate this problem in the language of tempered distributions, i.e. as a problem of approximating an empirical probability density function \$p\_\{\textbackslash{}rm\{emp\}\}(\{\textbackslash{}mathbf x\}) = \textbackslash{}frac\{1\}\{N\} \textbackslash{}sum\_\{i=1\}\^N \textbackslash{}delta\^n (\textbackslash{}bold\{x\} - \textbackslash{}bold\{x\}\_i)\$, where \$\textbackslash{}delta\^n\$ is an \$n\$-dimensional Dirac delta function, by another tempered distribution \$q(\{\textbackslash{}mathbf x\})\$ whose density is supported in some \$k\$-dimensional subspace. Thus, our problem is reduced to the minimization of a certain loss function \$I(q)\$ measuring the distance from \$q\$ to \$p\_\{\textbackslash{}rm\{emp\}\}\$ over a pertinent set of generalized functions, denoted \$\textbackslash{}mathcal\{G\}\_k\$. Another classical problem of data analysis is the sufficient dimension reduction problem. We show that it can be reduced to the following problem: given a function \$f: \{\textbackslash{}mathbb R\}\^n\textbackslash{}rightarrow \{\textbackslash{}mathbb R\}\$ and a probability density function \$p(\{\textbackslash{}mathbf x\})\$, find a function of the form \$g(\{\textbackslash{}mathbf w\}\^T\_1\{\textbackslash{}mathbf x\}, \textbackslash{}cdots, \{\textbackslash{}mathbf w\}\^T\_k\{\textbackslash{}mathbf x\})\$ that minimizes the loss \$\{\textbackslash{}mathbb E\}\_\{\{\textbackslash{}mathbf x\}\textbackslash{}sim p\} |f(\{\textbackslash{}mathbf x\})-g(\{\textbackslash{}mathbf w\}\^T\_1\{\textbackslash{}mathbf x\}, \textbackslash{}cdots, \{\textbackslash{}mathbf w\}\^T\_k\{\textbackslash{}mathbf x\})|\^2\$. We first show that search spaces of the latter two problems are in one-to-one correspondence which is defined by the Fourier transform. We introduce a nonnegative penalty function \$R(f)\$ and a set of ordinary functions \$\textbackslash{}Omega\_\textbackslash{}epsilon = \textbackslash\{f| R(f)\textbackslash{}leq \textbackslash{}epsilon\textbackslash\}\$ in such a way that \$\textbackslash{}Omega\_\textbackslash{}epsilon\$ `approximates' the space \$\textbackslash{}mathcal\{G\}\_k\$ when \$\textbackslash{}epsilon \textbackslash{}rightarrow 0\$. Then we present an algorithm for minimization of \$I(f)+\textbackslash{}lambda R(f)\$, based on the idea of two-step iterative computation.},
  urldate = {2019-03-14},
  date = {2019-03-12},
  keywords = {Statistics - Machine Learning,Computer Science - Machine Learning,Mathematics - Statistics Theory},
  author = {Takhanov, Rustem},
  file = {/home/dimitri/Nextcloud/Zotero/storage/EKMS922A/Takhanov - 2019 - Dimension reduction as an optimization problem ove.pdf;/home/dimitri/Nextcloud/Zotero/storage/DTB2K5DQ/1903.html}
}

@article{bhattacharjeeWhatRelationsAre2019,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1903.05347},
  primaryClass = {cs, stat},
  title = {What Relations Are Reliably Embeddable in {{Euclidean}} Space?},
  url = {http://arxiv.org/abs/1903.05347},
  abstract = {We consider the problem of embedding a relation, represented as a directed graph, into Euclidean space. For three types of embeddings motivated by the recent literature on knowledge graphs, we obtain characterizations of which relations they are able to capture, as well as bounds on the minimal dimensionality and precision needed.},
  urldate = {2019-03-14},
  date = {2019-03-13},
  keywords = {Statistics - Machine Learning,Computer Science - Machine Learning},
  author = {Bhattacharjee, Robi and Dasgupta, Sanjoy},
  file = {/home/dimitri/Nextcloud/Zotero/storage/J33WQ846/Bhattacharjee and Dasgupta - 2019 - What relations are reliably embeddable in Euclidea.pdf;/home/dimitri/Nextcloud/Zotero/storage/9KPSSFIP/1903.html}
}

@article{shahBuildingConversationalAgent2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1801.04871},
  primaryClass = {cs},
  title = {Building a {{Conversational Agent Overnight}} with {{Dialogue Self}}-{{Play}}},
  url = {http://arxiv.org/abs/1801.04871},
  abstract = {We propose Machines Talking To Machines (M2M), a framework combining automation and crowdsourcing to rapidly bootstrap end-to-end dialogue agents for goal-oriented dialogues in arbitrary domains. M2M scales to new tasks with just a task schema and an API client from the dialogue system developer, but it is also customizable to cater to task-specific interactions. Compared to the Wizard-of-Oz approach for data collection, M2M achieves greater diversity and coverage of salient dialogue flows while maintaining the naturalness of individual utterances. In the first phase, a simulated user bot and a domain-agnostic system bot converse to exhaustively generate dialogue "outlines", i.e. sequences of template utterances and their semantic parses. In the second phase, crowd workers provide contextual rewrites of the dialogues to make the utterances more natural while preserving their meaning. The entire process can finish within a few hours. We propose a new corpus of 3,000 dialogues spanning 2 domains collected with M2M, and present comparisons with popular dialogue datasets on the quality and diversity of the surface forms and dialogue flows.},
  urldate = {2019-03-14},
  date = {2018-01-15},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  author = {Shah, Pararth and Hakkani-Tür, Dilek and Tür, Gokhan and Rastogi, Abhinav and Bapna, Ankur and Nayak, Neha and Heck, Larry},
  file = {/home/dimitri/Nextcloud/Zotero/storage/7DPVMQQH/Shah et al. - 2018 - Building a Conversational Agent Overnight with Dia.pdf;/home/dimitri/Nextcloud/Zotero/storage/IRNPDINU/Shah et al. - 2018 - Building a Conversational Agent Overnight with Dia.pdf;/home/dimitri/Nextcloud/Zotero/storage/TSP9GXU6/1801.html;/home/dimitri/Nextcloud/Zotero/storage/VPIKA824/1801.html}
}

@article{cohenIntentionChoiceCommitment1990,
  langid = {english},
  title = {Intention Is Choice with Commitment},
  volume = {42},
  issn = {00043702},
  url = {http://linkinghub.elsevier.com/retrieve/pii/0004370290900555},
  doi = {10.1016/0004-3702(90)90055-5},
  number = {2-3},
  journaltitle = {Artificial Intelligence},
  urldate = {2019-03-14},
  date = {1990-03},
  pages = {213-261},
  author = {Cohen, Philip R. and Levesque, Hector J.},
  file = {/home/dimitri/Nextcloud/Zotero/storage/KPRSB3VC/Cohen and Levesque - 1990 - Intention is choice with commitment.pdf}
}

@article{peysakhovichReinforcementLearningInverse2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1811.08549},
  primaryClass = {cs, stat},
  title = {Reinforcement {{Learning}} and {{Inverse Reinforcement Learning}} with {{System}} 1 and {{System}} 2},
  url = {http://arxiv.org/abs/1811.08549},
  abstract = {Inferring a person's goal from their behavior is an important problem in applications of AI (e.g. automated assistants, recommender systems). The workhorse model for this task is the rational actor model - this amounts to assuming that people have stable reward functions, discount the future exponentially, and construct optimal plans. Under the rational actor assumption techniques such as inverse reinforcement learning (IRL) can be used to infer a person's goals from their actions. A competing model is the dual-system model. Here decisions are the result of an interplay between a fast, automatic, heuristic-based system 1 and a slower, deliberate, calculating system 2. We generalize the dual system framework to the case of Markov decision problems and show how to compute optimal plans for dual-system agents. We show that dual-system agents exhibit behaviors that are incompatible with rational actor assumption. We show that naive applications of rational-actor IRL to the behavior of dual-system agents can generate wrong inference about the agents' goals and suggest interventions that actually reduce the agent's overall utility. Finally, we adapt a simple IRL algorithm to correctly infer the goals of dual system decision-makers. This allows us to make interventions that help, rather than hinder, the dual-system agent's ability to reach their true goals.},
  urldate = {2019-03-15},
  date = {2018-11-19},
  keywords = {Statistics - Machine Learning,Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Computer Science and Game Theory},
  author = {Peysakhovich, Alexander},
  file = {/home/dimitri/Nextcloud/Zotero/storage/TXAH6E89/Peysakhovich - 2018 - Reinforcement Learning and Inverse Reinforcement L.pdf;/home/dimitri/Nextcloud/Zotero/storage/AHRR6CHP/1811.html}
}

@article{fawazDeepLearningTime2019,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1809.04356},
  title = {Deep Learning for Time Series Classification: A Review},
  issn = {1384-5810, 1573-756X},
  url = {http://arxiv.org/abs/1809.04356},
  doi = {10.1007/s10618-019-00619-1},
  shorttitle = {Deep Learning for Time Series Classification},
  abstract = {Time Series Classification (TSC) is an important and challenging problem in data mining. With the increase of time series data availability, hundreds of TSC algorithms have been proposed. Among these methods, only a few have considered Deep Neural Networks (DNNs) to perform this task. This is surprising as deep learning has seen very successful applications in the last years. DNNs have indeed revolutionized the field of computer vision especially with the advent of novel deeper architectures such as Residual and Convolutional Neural Networks. Apart from images, sequential data such as text and audio can also be processed with DNNs to reach state-of-the-art performance for document classification and speech recognition. In this article, we study the current state-of-the-art performance of deep learning algorithms for TSC by presenting an empirical study of the most recent DNN architectures for TSC. We give an overview of the most successful deep learning applications in various time series domains under a unified taxonomy of DNNs for TSC. We also provide an open source deep learning framework to the TSC community where we implemented each of the compared approaches and evaluated them on a univariate TSC benchmark (the UCR/UEA archive) and 12 multivariate time series datasets. By training 8,730 deep learning models on 97 time series datasets, we propose the most exhaustive study of DNNs for TSC to date.},
  journaltitle = {Data Mining and Knowledge Discovery},
  urldate = {2019-03-15},
  date = {2019-03-02},
  keywords = {Statistics - Machine Learning,Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  author = {Fawaz, H. Ismail and Forestier, G. and Weber, J. and Idoumghar, L. and Muller, P.},
  file = {/home/dimitri/Nextcloud/Zotero/storage/S67NMLNU/Fawaz et al. - 2019 - Deep learning for time series classification a re.pdf;/home/dimitri/Nextcloud/Zotero/storage/7HGR48DK/1809.html}
}

@book{mcelreathStatisticalRethinkingBayesian2016,
  location = {{Boca Raton}},
  title = {Statistical Rethinking: A {{Bayesian}} Course with Examples in {{R}} and {{Stan}}},
  isbn = {978-1-4822-5344-3},
  shorttitle = {Statistical Rethinking},
  pagetotal = {469},
  number = {122},
  series = {Chapman \& {{Hall}}/{{CRC}} Texts in Statistical Science Series},
  publisher = {{CRC Press/Taylor \& Francis Group}},
  date = {2016},
  keywords = {R (Computer program language),Bayesian statistical decision theory},
  author = {McElreath, Richard},
  file = {/home/dimitri/Nextcloud/Zotero/storage/2FLRRWL4/McElreath - 2016 - Statistical rethinking a Bayesian course with exa.pdf}
}

@article{hairerIntroductionStochasticPDEs2009,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {0907.4178},
  primaryClass = {math},
  title = {An {{Introduction}} to {{Stochastic PDEs}}},
  url = {http://arxiv.org/abs/0907.4178},
  abstract = {These notes are based on a series of lectures given first at the University of Warwick in spring 2008 and then at the Courant Institute in spring 2009. It is an attempt to give a reasonably self-contained presentation of the basic theory of stochastic partial differential equations, taking for granted basic measure theory, functional analysis and probability theory, but nothing else. The approach taken in these notes is to focus on semilinear parabolic problems driven by additive noise. These can be treated as stochastic evolution equations in some infinite-dimensional Banach or Hilbert space that usually have nice regularising properties and they already form a very rich class of problems with many interesting properties. Furthermore, this class of problems has the advantage of allowing to completely pass under silence many subtle problems arising from stochastic integration in infinite-dimensional spaces.},
  urldate = {2019-03-18},
  date = {2009-07-23},
  keywords = {Mathematics - Probability,60H15,Mathematics - Analysis of PDEs},
  author = {Hairer, Martin},
  file = {/home/dimitri/Nextcloud/Zotero/storage/D2BYI3QA/Hairer - 2009 - An Introduction to Stochastic PDEs.pdf;/home/dimitri/Nextcloud/Zotero/storage/D48N9T9I/0907.html}
}

@book{petersElementsCausalInference2017,
  location = {{Cambridge, Massachuestts}},
  title = {Elements of Causal Inference: Foundations and Learning Algorithms},
  isbn = {978-0-262-03731-0},
  shorttitle = {Elements of Causal Inference},
  pagetotal = {265},
  series = {Adaptive Computation and Machine Learning Series},
  publisher = {{The MIT Press}},
  date = {2017},
  keywords = {Machine learning,Computer algorithms,Causation,Inference,Logic; Symbolic and mathematical},
  author = {Peters, Jonas and Janzing, Dominik and Schölkopf, Bernhard},
  file = {/home/dimitri/Nextcloud/Zotero/storage/QN9PUTP4/Peters et al. - 2017 - Elements of causal inference foundations and lear.pdf}
}

@article{heredia-gomezOCAPISPackageOrdinal2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1810.09733},
  primaryClass = {cs, stat},
  title = {{{OCAPIS}}: {{R}} Package for {{Ordinal Classification And Preprocessing In Scala}}},
  url = {http://arxiv.org/abs/1810.09733},
  shorttitle = {{{OCAPIS}}},
  abstract = {Ordinal Data are those where a natural order exist between the labels. The classification and pre-processing of this type of data is attracting more and more interest in the area of machine learning, due to its presence in many common problems. Traditionally, ordinal classification problems have been approached as nominal problems. However, that implies not taking into account their natural order constraints. In this paper, an innovative R package named ocapis (Ordinal Classification and Preprocessing In Scala) is introduced. Implemented mainly in Scala and available through Github, this library includes four learners and two pre-processing algorithms for ordinal and monotonic data. Main features of the package and examples of installation and use are explained throughout this manuscript.},
  urldate = {2019-03-19},
  date = {2018-10-23},
  keywords = {Statistics - Machine Learning,Computer Science - Machine Learning},
  author = {Heredia-Gómez, M. Cristina and García, Salvador and Gutiérrez, Pedro Antonio and Herrera, Francisco},
  file = {/home/dimitri/Nextcloud/Zotero/storage/ERLUKYDP/Heredia-Gómez et al. - 2018 - OCAPIS R package for Ordinal Classification And P.pdf;/home/dimitri/Nextcloud/Zotero/storage/2VYVZLZ2/1810.html}
}

@article{jawanpuriaLowrankApproximationsHyperbolic2019,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1903.07307},
  primaryClass = {cs, math, stat},
  title = {Low-Rank Approximations of Hyperbolic Embeddings},
  url = {http://arxiv.org/abs/1903.07307},
  abstract = {The hyperbolic manifold is a smooth manifold of negative constant curvature. While the hyperbolic manifold is well-studied in the literature, it has gained interest in the machine learning and natural language processing communities lately due to its usefulness in modeling continuous hierarchies. Tasks with hierarchical structures are ubiquitous in those fields and there is a general interest to learning hyperbolic representations or embeddings of such tasks. Additionally, these embeddings of related tasks may also share a low-rank subspace. In this work, we propose to learn hyperbolic embeddings such that they also lie in a low-dimensional subspace. In particular, we consider the problem of learning a low-rank factorization of hyperbolic embeddings. We cast these problems as manifold optimization problems and propose computationally efficient algorithms. Empirical results illustrate the efficacy of the proposed approach.},
  urldate = {2019-03-19},
  date = {2019-03-18},
  keywords = {Statistics - Machine Learning,Computer Science - Machine Learning,Mathematics - Optimization and Control},
  author = {Jawanpuria, Pratik and Meghwanshi, Mayank and Mishra, Bamdev},
  file = {/home/dimitri/Nextcloud/Zotero/storage/HSJ9EB3M/Jawanpuria et al. - 2019 - Low-rank approximations of hyperbolic embeddings.pdf;/home/dimitri/Nextcloud/Zotero/storage/88FXB97C/1903.html}
}

@article{bauerPersistenceDiagramsDiagrams2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1610.10085},
  primaryClass = {cs, math},
  title = {Persistence {{Diagrams}} as {{Diagrams}}: {{A Categorification}} of the {{Stability Theorem}}},
  url = {http://arxiv.org/abs/1610.10085},
  shorttitle = {Persistence {{Diagrams}} as {{Diagrams}}},
  abstract = {Persistent homology, a central tool of topological data analysis, provides invariants of data called barcodes (also known as persistence diagrams). A barcode is simply a multiset of real intervals. Recent work of Edelsbrunner, Jablonski, and Mrozek suggests an equivalent description of barcodes as functors R -{$>$} Mch, where R is the poset category of real numbers and Mch is the category whose objects are sets and whose morphisms are matchings (i.e., partial injective functions). Such functors form a category Mch\^R whose morphisms are the natural transformations. Thus, this interpretation of barcodes gives us a hitherto unstudied categorical structure on barcodes. The aim of this note is to show that this categorical structure leads to surprisingly simple reformulations of both the well-known stability theorem for persistent homology and a recent generalization called the induced matching theorem.},
  urldate = {2019-03-19},
  date = {2016-10-31},
  keywords = {Computer Science - Computational Geometry,Mathematics - Algebraic Topology,Mathematics - Category Theory,13P20; 55U99},
  author = {Bauer, Ulrich and Lesnick, Michael},
  file = {/home/dimitri/Nextcloud/Zotero/storage/9YK34HBQ/Bauer and Lesnick - 2016 - Persistence Diagrams as Diagrams A Categorificati.pdf;/home/dimitri/Nextcloud/Zotero/storage/25LJ568T/1610.html}
}

@incollection{kandasamyNeuralArchitectureSearch2018,
  title = {Neural {{Architecture Search}} with {{Bayesian Optimisation}} and {{Optimal Transport}}},
  url = {http://papers.nips.cc/paper/7472-neural-architecture-search-with-bayesian-optimisation-and-optimal-transport.pdf},
  booktitle = {Advances in {{Neural Information Processing Systems}} 31},
  publisher = {{Curran Associates, Inc.}},
  urldate = {2019-03-19},
  date = {2018},
  pages = {2016--2025},
  author = {Kandasamy, Kirthevasan and Neiswanger, Willie and Schneider, Jeff and Poczos, Barnabas and Xing, Eric P},
  editor = {Bengio, S. and Wallach, H. and Larochelle, H. and Grauman, K. and Cesa-Bianchi, N. and Garnett, R.},
  file = {/home/dimitri/Nextcloud/Zotero/storage/Z7EIR5JQ/1802.07191.pdf;/home/dimitri/Nextcloud/Zotero/storage/7EYH2VIS/7472-neural-architecture-search-with-bayesian-optimisation-and-optimal-transport.html}
}

@incollection{rasmussenGaussianProcessesMachine2004,
  location = {{Berlin, Heidelberg}},
  title = {Gaussian {{Processes}} in {{Machine Learning}}},
  volume = {3176},
  isbn = {978-3-540-23122-6 978-3-540-28650-9},
  url = {http://link.springer.com/10.1007/978-3-540-28650-9_4},
  booktitle = {Advanced {{Lectures}} on {{Machine Learning}}},
  publisher = {{Springer Berlin Heidelberg}},
  urldate = {2019-03-19},
  date = {2004},
  pages = {63-71},
  author = {Rasmussen, Carl Edward},
  editor = {Bousquet, Olivier and family=Luxburg, given=Ulrike, prefix=von, useprefix=true and Rätsch, Gunnar},
  file = {/home/dimitri/Nextcloud/Zotero/storage/KSCYH3JC/rasmussen2004.pdf;/home/dimitri/Nextcloud/Zotero/storage/T22JWMGV/Rasmussen - 2004 - Gaussian Processes in Machine Learning.pdf},
  doi = {10.1007/978-3-540-28650-9_4}
}

@inproceedings{adamekAbstractConcreteCategories2009,
  title = {Abstract and {{Concrete Categories}} - {{The Joy}} of {{Cats}}},
  abstract = {and Concrete Categories The Joy of Cats Dedicated to Bernhard Banaschewski The newest edition of the file of the present book can be downloaded from http://katmat.math.uni-bremen.de/acc The authors are grateful for any improvements, corrections, and remarks, and can be reached at the addresses Jǐŕı Adámek, email: adamek@iti.cs.tu-bs.de Horst Herrlich, email: horst.herrlich@t-online.de George E. Strecker, email: strecker@math.ksu.edu All corrections will be awarded, besides eternal gratefulness, with a piece of delicious cake! You can claim your cake at the KatMAT Seminar, University of Bremen, at any Tuesday (during terms). Copyright c © 2004 Jǐŕı Adámek, Horst Herrlich, and George E. Strecker. Permission is granted to copy, distribute and/or modify this document under the terms of the GNU Free Documentation License, Version 1.2 or any later version published by the Free Software Foundation; with no Invariant Sections, no Front-Cover Texts, and no Back-Cover Texts. A copy of the license is included in the section entitled “GNU Free Documentation License”. See p. 512 ff.},
  date = {2009},
  keywords = {Horst Rittel,John D. Wiley,Published Comment,Scientific Publication,Tacrolimus},
  author = {Adámek, Jirí and Herrlich, Horst and Strecker, George E.},
  file = {/home/dimitri/Nextcloud/Zotero/storage/DSR8Y46T/Adámek et al. - 2009 - Abstract and Concrete Categories - The Joy of Cats.pdf}
}

@article{bonatoSketchyTweetsTen2012,
  langid = {english},
  title = {Sketchy {{Tweets}}: {{Ten Minute Conjectures}} in {{Graph Theory}}},
  volume = {34},
  issn = {0343-6993, 1866-7414},
  url = {http://link.springer.com/10.1007/s00283-012-9275-2},
  doi = {10.1007/s00283-012-9275-2},
  shorttitle = {Sketchy {{Tweets}}},
  number = {1},
  journaltitle = {The Mathematical Intelligencer},
  urldate = {2019-03-20},
  date = {2012-03},
  pages = {8-15},
  author = {Bonato, Anthony and Nowakowski, Richard J.},
  file = {/home/dimitri/Nextcloud/Zotero/storage/53BNHQAN/Bonato and Nowakowski - 2012 - Sketchy Tweets Ten Minute Conjectures in Graph Th.pdf}
}

@article{marcusDeepLearningCritical2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1801.00631},
  primaryClass = {cs, stat},
  title = {Deep {{Learning}}: {{A Critical Appraisal}}},
  url = {http://arxiv.org/abs/1801.00631},
  shorttitle = {Deep {{Learning}}},
  abstract = {Although deep learning has historical roots going back decades, neither the term "deep learning" nor the approach was popular just over five years ago, when the field was reignited by papers such as Krizhevsky, Sutskever and Hinton's now classic (2012) deep network model of Imagenet. What has the field discovered in the five subsequent years? Against a background of considerable progress in areas such as speech recognition, image recognition, and game playing, and considerable enthusiasm in the popular press, I present ten concerns for deep learning, and suggest that deep learning must be supplemented by other techniques if we are to reach artificial general intelligence.},
  urldate = {2019-03-20},
  date = {2018-01-02},
  keywords = {Statistics - Machine Learning,Computer Science - Artificial Intelligence,Computer Science - Machine Learning,97R40,I.2.0,I.2.6},
  author = {Marcus, Gary},
  file = {/home/dimitri/Nextcloud/Zotero/storage/ZR84SFVJ/Marcus - 2018 - Deep Learning A Critical Appraisal.pdf;/home/dimitri/Nextcloud/Zotero/storage/K6A9KJBG/1801.html}
}

@article{mnasriRecentAdvancesConversational2019,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1903.09025},
  primaryClass = {cs},
  title = {Recent Advances in Conversational {{NLP}} : {{Towards}} the Standardization of {{Chatbot}} Building},
  url = {http://arxiv.org/abs/1903.09025},
  shorttitle = {Recent Advances in Conversational {{NLP}}},
  abstract = {Dialogue systems have become recently essential in our life. Their use is getting more and more fluid and easy throughout the time. This boils down to the improvements made in NLP and AI fields. In this paper, we try to provide an overview to the current state of the art of dialogue systems, their categories and the different approaches to build them. We end up with a discussion that compares all the techniques and analyzes the strengths and weaknesses of each. Finally, we present an opinion piece suggesting to orientate the research towards the standardization of dialogue systems building.},
  urldate = {2019-03-22},
  date = {2019-03-21},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  author = {Mnasri, Maali},
  file = {/home/dimitri/Nextcloud/Zotero/storage/RCRAVW89/Mnasri - 2019 - Recent advances in conversational NLP  Towards th.pdf;/home/dimitri/Nextcloud/Zotero/storage/ZKUULC3J/1903.html}
}

@article{wolfTransferTransfoTransferLearning2019,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1901.08149},
  primaryClass = {cs},
  title = {{{TransferTransfo}}: {{A Transfer Learning Approach}} for {{Neural Network Based Conversational Agents}}},
  url = {http://arxiv.org/abs/1901.08149},
  shorttitle = {{{TransferTransfo}}},
  abstract = {We introduce a new approach to generative data-driven dialogue systems (e.g. chatbots) called TransferTransfo which is a combination of a Transfer learning based training scheme and a high-capacity Transformer model. Fine-tuning is performed by using a multi-task objective which combines several unsupervised prediction tasks. The resulting fine-tuned model shows strong improvements over the current state-of-the-art end-to-end conversational models like memory augmented seq2seq and information-retrieval models. On the privately held PERSONA-CHAT dataset of the Conversational Intelligence Challenge 2, this approach obtains a new state-of-the-art, with respective perplexity, Hits@1 and F1 metrics of 16.28 (45 \% absolute improvement), 80.7 (46 \% absolute improvement) and 19.5 (20 \% absolute improvement).},
  urldate = {2019-03-28},
  date = {2019-01-23},
  keywords = {Computer Science - Computation and Language},
  author = {Wolf, Thomas and Sanh, Victor and Chaumond, Julien and Delangue, Clement},
  file = {/home/dimitri/Nextcloud/Zotero/storage/KCTAMC3V/Wolf et al. - 2019 - TransferTransfo A Transfer Learning Approach for .pdf;/home/dimitri/Nextcloud/Zotero/storage/KKVJVLC2/1901.html}
}

@article{sanhHierarchicalMultitaskApproach2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1811.06031},
  primaryClass = {cs},
  title = {A {{Hierarchical Multi}}-Task {{Approach}} for {{Learning Embeddings}} from {{Semantic Tasks}}},
  url = {http://arxiv.org/abs/1811.06031},
  abstract = {Much effort has been devoted to evaluate whether multi-task learning can be leveraged to learn rich representations that can be used in various Natural Language Processing (NLP) down-stream applications. However, there is still a lack of understanding of the settings in which multi-task learning has a significant effect. In this work, we introduce a hierarchical model trained in a multi-task learning setup on a set of carefully selected semantic tasks. The model is trained in a hierarchical fashion to introduce an inductive bias by supervising a set of low level tasks at the bottom layers of the model and more complex tasks at the top layers of the model. This model achieves state-of-the-art results on a number of tasks, namely Named Entity Recognition, Entity Mention Detection and Relation Extraction without hand-engineered features or external NLP tools like syntactic parsers. The hierarchical training supervision induces a set of shared semantic representations at lower layers of the model. We show that as we move from the bottom to the top layers of the model, the hidden states of the layers tend to represent more complex semantic information.},
  urldate = {2019-03-28},
  date = {2018-11-14},
  keywords = {Computer Science - Computation and Language},
  author = {Sanh, Victor and Wolf, Thomas and Ruder, Sebastian},
  file = {/home/dimitri/Nextcloud/Zotero/storage/94D7JXPM/Sanh et al. - 2018 - A Hierarchical Multi-task Approach for Learning Em.pdf;/home/dimitri/Nextcloud/Zotero/storage/H65A3WYX/1811.html}
}

@article{kriegeSurveyGraphKernels2019,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1903.11835},
  primaryClass = {cs, stat},
  title = {A {{Survey}} on {{Graph Kernels}}},
  url = {http://arxiv.org/abs/1903.11835},
  abstract = {Graph kernels have become an established and widely-used technique for solving classification tasks on graphs. This survey gives a comprehensive overview of techniques for kernel-based graph classification developed in the past 15 years. We describe and categorize graph kernels based on properties inherent to their design, such as the nature of their extracted graph features, their method of computation and their applicability to problems in practice. In an extensive experimental evaluation, we study the classification accuracy of a large suite of graph kernels on established benchmarks as well as new datasets. We compare the performance of popular kernels with several baseline methods and study the effect of applying a Gaussian RBF kernel to the metric induced by a graph kernel. In doing so, we find that simple baselines become competitive after this transformation on some datasets. Moreover, we study the extent to which existing graph kernels agree in their predictions (and prediction errors) and obtain a data-driven categorization of kernels as result. Finally, based on our experimental results, we derive a practitioner's guide to kernel-based graph classification.},
  urldate = {2019-03-29},
  date = {2019-03-28},
  keywords = {Statistics - Machine Learning,Computer Science - Machine Learning},
  author = {Kriege, Nils M. and Johansson, Fredrik D. and Morris, Christopher},
  file = {/home/dimitri/Nextcloud/Zotero/storage/VJRL8PUJ/Kriege et al. - 2019 - A Survey on Graph Kernels.pdf;/home/dimitri/Nextcloud/Zotero/storage/TKG2F5BS/1903.html}
}

@article{ozairWassersteinDependencyMeasure2019,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1903.11780},
  primaryClass = {cs, stat},
  title = {Wasserstein {{Dependency Measure}} for {{Representation Learning}}},
  url = {http://arxiv.org/abs/1903.11780},
  abstract = {Mutual information maximization has emerged as a powerful learning objective for unsupervised representation learning obtaining state-of-the-art performance in applications such as object recognition, speech recognition, and reinforcement learning. However, such approaches are fundamentally limited since a tight lower bound of mutual information requires sample size exponential in the mutual information. This limits the applicability of these approaches for prediction tasks with high mutual information, such as in video understanding or reinforcement learning. In these settings, such techniques are prone to overfit, both in theory and in practice, and capture only a few of the relevant factors of variation. This leads to incomplete representations that are not optimal for downstream tasks. In this work, we empirically demonstrate that mutual information-based representation learning approaches do fail to learn complete representations on a number of designed and real-world tasks. To mitigate these problems we introduce the Wasserstein dependency measure, which learns more complete representations by using the Wasserstein distance instead of the KL divergence in the mutual information estimator. We show that a practical approximation to this theoretically motivated solution, constructed using Lipschitz constraint techniques from the GAN literature, achieves substantially improved results on tasks where incomplete representations are a major challenge.},
  urldate = {2019-03-29},
  date = {2019-03-27},
  keywords = {Statistics - Machine Learning,Computer Science - Machine Learning},
  author = {Ozair, Sherjil and Lynch, Corey and Bengio, Yoshua and family=Oord, given=Aaron, prefix=van den, useprefix=false and Levine, Sergey and Sermanet, Pierre},
  file = {/home/dimitri/Nextcloud/Zotero/storage/ZYQH2Y9K/Ozair et al. - 2019 - Wasserstein Dependency Measure for Representation .pdf;/home/dimitri/Nextcloud/Zotero/storage/PPDX5S4W/1903.html}
}

@article{gongNaturalLanguageInference2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1709.04348},
  primaryClass = {cs},
  title = {Natural {{Language Inference}} over {{Interaction Space}}},
  url = {http://arxiv.org/abs/1709.04348},
  abstract = {Natural Language Inference (NLI) task requires an agent to determine the logical relationship between a natural language premise and a natural language hypothesis. We introduce Interactive Inference Network (IIN), a novel class of neural network architectures that is able to achieve high-level understanding of the sentence pair by hierarchically extracting semantic features from interaction space. We show that an interaction tensor (attention weight) contains semantic information to solve natural language inference, and a denser interaction tensor contains richer semantic information. One instance of such architecture, Densely Interactive Inference Network (DIIN), demonstrates the state-of-the-art performance on large scale NLI copora and large-scale NLI alike corpus. It's noteworthy that DIIN achieve a greater than 20\% error reduction on the challenging Multi-Genre NLI (MultiNLI) dataset with respect to the strongest published system.},
  urldate = {2019-03-29},
  date = {2017-09-13},
  keywords = {Computer Science - Computation and Language},
  author = {Gong, Yichen and Luo, Heng and Zhang, Jian},
  file = {/home/dimitri/Nextcloud/Zotero/storage/6RVGCDED/Gong et al. - 2017 - Natural Language Inference over Interaction Space.pdf;/home/dimitri/Nextcloud/Zotero/storage/GXLTYIMJ/1709.html}
}

@article{oudeyerComputationalRoboticModels2019,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1903.10246},
  primaryClass = {cs},
  title = {Computational and {{Robotic Models}} of {{Early Language Development}}: {{A Review}}},
  url = {http://arxiv.org/abs/1903.10246},
  shorttitle = {Computational and {{Robotic Models}} of {{Early Language Development}}},
  abstract = {We review computational and robotics models of early language learning and development. We first explain why and how these models are used to understand better how children learn language. We argue that they provide concrete theories of language learning as a complex dynamic system, complementing traditional methods in psychology and linguistics. We review different modeling formalisms, grounded in techniques from machine learning and artificial intelligence such as Bayesian and neural network approaches. We then discuss their role in understanding several key mechanisms of language development: cross-situational statistical learning, embodiment, situated social interaction, intrinsically motivated learning, and cultural evolution. We conclude by discussing future challenges for research, including modeling of large-scale empirical data about language acquisition in real-world environments. Keywords: Early language learning, Computational and robotic models, machine learning, development, embodiment, social interaction, intrinsic motivation, self-organization, dynamical systems, complexity.},
  urldate = {2019-03-29},
  date = {2019-03-25},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  author = {Oudeyer, Pierre-Yves and Kachergis, George and Schueller, William},
  file = {/home/dimitri/Nextcloud/Zotero/storage/E7MCT6GY/Oudeyer et al. - 2019 - Computational and Robotic Models of Early Language.pdf;/home/dimitri/Nextcloud/Zotero/storage/DFZTSTFU/1903.html}
}

@article{hjelmLearningDeepRepresentations2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1808.06670},
  primaryClass = {cs, stat},
  title = {Learning Deep Representations by Mutual Information Estimation and Maximization},
  url = {http://arxiv.org/abs/1808.06670},
  abstract = {In this work, we perform unsupervised learning of representations by maximizing mutual information between an input and the output of a deep neural network encoder. Importantly, we show that structure matters: incorporating knowledge about locality of the input to the objective can greatly influence a representation's suitability for downstream tasks. We further control characteristics of the representation by matching to a prior distribution adversarially. Our method, which we call Deep InfoMax (DIM), outperforms a number of popular unsupervised learning methods and competes with fully-supervised learning on several classification tasks. DIM opens new avenues for unsupervised learning of representations and is an important step towards flexible formulations of representation-learning objectives for specific end-goals.},
  urldate = {2019-04-01},
  date = {2018-08-20},
  keywords = {Statistics - Machine Learning,Computer Science - Machine Learning},
  author = {Hjelm, R. Devon and Fedorov, Alex and Lavoie-Marchildon, Samuel and Grewal, Karan and Bachman, Phil and Trischler, Adam and Bengio, Yoshua},
  file = {/home/dimitri/Nextcloud/Zotero/storage/36AV9H8R/Hjelm et al. - 2018 - Learning deep representations by mutual informatio.pdf;/home/dimitri/Nextcloud/Zotero/storage/IXX3GXTN/1808.html}
}

@article{belghaziMINEMutualInformation2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1801.04062},
  primaryClass = {cs, stat},
  title = {{{MINE}}: {{Mutual Information Neural Estimation}}},
  url = {http://arxiv.org/abs/1801.04062},
  shorttitle = {{{MINE}}},
  abstract = {We argue that the estimation of mutual information between high dimensional continuous random variables can be achieved by gradient descent over neural networks. We present a Mutual Information Neural Estimator (MINE) that is linearly scalable in dimensionality as well as in sample size, trainable through back-prop, and strongly consistent. We present a handful of applications on which MINE can be used to minimize or maximize mutual information. We apply MINE to improve adversarially trained generative models. We also use MINE to implement Information Bottleneck, applying it to supervised classification; our results demonstrate substantial improvement in flexibility and performance in these settings.},
  urldate = {2019-04-01},
  date = {2018-01-12},
  keywords = {Statistics - Machine Learning,Computer Science - Machine Learning},
  author = {Belghazi, Mohamed Ishmael and Baratin, Aristide and Rajeswar, Sai and Ozair, Sherjil and Bengio, Yoshua and Courville, Aaron and Hjelm, R. Devon},
  file = {/home/dimitri/Nextcloud/Zotero/storage/3FZITPZ2/Belghazi et al. - 2018 - MINE Mutual Information Neural Estimation.pdf;/home/dimitri/Nextcloud/Zotero/storage/VM34ZB9U/1801.html}
}

@article{oordRepresentationLearningContrastive2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1807.03748},
  primaryClass = {cs, stat},
  title = {Representation {{Learning}} with {{Contrastive Predictive Coding}}},
  url = {http://arxiv.org/abs/1807.03748},
  abstract = {While supervised learning has enabled great progress in many applications, unsupervised learning has not seen such widespread adoption, and remains an important and challenging endeavor for artificial intelligence. In this work, we propose a universal unsupervised learning approach to extract useful representations from high-dimensional data, which we call Contrastive Predictive Coding. The key insight of our model is to learn such representations by predicting the future in latent space by using powerful autoregressive models. We use a probabilistic contrastive loss which induces the latent space to capture information that is maximally useful to predict future samples. It also makes the model tractable by using negative sampling. While most prior work has focused on evaluating representations for a particular modality, we demonstrate that our approach is able to learn useful representations achieving strong performance on four distinct domains: speech, images, text and reinforcement learning in 3D environments.},
  urldate = {2019-04-01},
  date = {2018-07-10},
  keywords = {Statistics - Machine Learning,Computer Science - Machine Learning},
  author = {family=Oord, given=Aaron, prefix=van den, useprefix=false and Li, Yazhe and Vinyals, Oriol},
  file = {/home/dimitri/Nextcloud/Zotero/storage/DNB5PFBL/Oord et al. - 2018 - Representation Learning with Contrastive Predictiv.pdf;/home/dimitri/Nextcloud/Zotero/storage/FW23WSJQ/1807.html}
}

@book{bremaudDiscreteProbabilityModels2017,
  location = {{Cham}},
  title = {Discrete {{Probability Models}} and {{Methods}}},
  volume = {78},
  isbn = {978-3-319-43475-9 978-3-319-43476-6},
  url = {http://link.springer.com/10.1007/978-3-319-43476-6},
  series = {Probability {{Theory}} and {{Stochastic Modelling}}},
  publisher = {{Springer International Publishing}},
  urldate = {2019-04-01},
  date = {2017},
  author = {Brémaud, Pierre},
  file = {/home/dimitri/Nextcloud/Zotero/storage/ASIRATMH/2017 - Discrete probability models and methods.pdf},
  doi = {10.1007/978-3-319-43476-6}
}

@article{baroniLinguisticGeneralizationCompositionality2019,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1904.00157},
  primaryClass = {cs},
  title = {Linguistic Generalization and Compositionality in Modern Artificial Neural Networks},
  url = {http://arxiv.org/abs/1904.00157},
  abstract = {In the last decade, deep artificial neural networks have achieved astounding performance in many natural language processing tasks. Given the high productivity of language, these models must possess effective generalization abilities. It is widely assumed that humans handle linguistic productivity by means of algebraic compositional rules: Are deep networks similarly compositional? After reviewing the main innovations characterizing current deep language processing networks, I discuss a set of studies suggesting that deep networks are capable of subtle grammar-dependent generalizations, but also that they do not rely on systematic compositional rules. I argue that the intriguing behaviour of these devices (still awaiting a full understanding) should be of interest to linguists and cognitive scientists, as it offers a new perspective on possible computational strategies to deal with linguistic productivity beyond rule-based compositionality, and it might lead to new insights into the less systematic generalization patterns that also appear in natural language.},
  urldate = {2019-04-02},
  date = {2019-03-30},
  keywords = {Computer Science - Computation and Language},
  author = {Baroni, Marco},
  file = {/home/dimitri/Nextcloud/Zotero/storage/EZHMUUU5/Baroni - 2019 - Linguistic generalization and compositionality in .pdf;/home/dimitri/Nextcloud/Zotero/storage/C8S7C7RF/1904.html}
}

@book{raoNaturalLanguageProcessing2019,
  langid = {english},
  location = {{Beijing Boston Farnham}},
  title = {Natural Language Processing with {{PyTorch}}: Build Intelligent Language Applications Using Deep Learning},
  edition = {First edition},
  isbn = {978-1-4919-7823-8},
  shorttitle = {Natural Language Processing with {{PyTorch}}},
  pagetotal = {238},
  publisher = {{O´Reilly}},
  date = {2019},
  author = {Rao, Delip and McMahan, Brian},
  file = {/home/dimitri/Nextcloud/Zotero/storage/S5HLRUL8/Rao and McMahan - 2019 - Natural language processing with PyTorch build in.pdf}
}

@article{gortlerVisualExplorationGaussian2019,
  title = {A {{Visual Exploration}} of {{Gaussian Processes}}},
  volume = {4},
  issn = {2476-0757},
  url = {https://distill.pub/2019/visual-exploration-gaussian-processes},
  doi = {10.23915/distill.00017},
  number = {4},
  journaltitle = {Distill},
  urldate = {2019-04-03},
  date = {2019-04-02},
  author = {Görtler, Jochen and Kehlbeck, Rebecca and Deussen, Oliver}
}

@article{liuLinguisticKnowledgeTransferability2019,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1903.08855},
  primaryClass = {cs},
  title = {Linguistic {{Knowledge}} and {{Transferability}} of {{Contextual Representations}}},
  url = {http://arxiv.org/abs/1903.08855},
  abstract = {Contextual word representations derived from large-scale neural language models are successful across a diverse set of NLP tasks, suggesting that they encode useful and transferable features of language. To shed light on the linguistic knowledge they capture, we study the representations produced by several recent pretrained contextualizers (variants of ELMo, the OpenAI transformer language model, and BERT) with a suite of sixteen diverse probing tasks. We find that linear models trained on top of frozen contextual representations are competitive with state-of-the-art task-specific models in many cases, but fail on tasks requiring fine-grained linguistic knowledge (e.g., conjunct identification). To investigate the transferability of contextual word representations, we quantify differences in the transferability of individual layers within contextualizers, especially between recurrent neural networks (RNNs) and transformers. For instance, higher layers of RNNs are more task-specific, while transformer layers do not exhibit the same monotonic trend. In addition, to better understand what makes contextual word representations transferable, we compare language model pretraining with eleven supervised pretraining tasks. For any given task, pretraining on a closely related task yields better performance than language model pretraining (which is better on average) when the pretraining dataset is fixed. However, language model pretraining on more data gives the best results.},
  urldate = {2019-04-03},
  date = {2019-03-21},
  keywords = {Computer Science - Computation and Language},
  author = {Liu, Nelson F. and Gardner, Matt and Belinkov, Yonatan and Peters, Matthew E. and Smith, Noah A.},
  file = {/home/dimitri/Nextcloud/Zotero/storage/VFCRFKBW/Liu et al. - 2019 - Linguistic Knowledge and Transferability of Contex.pdf;/home/dimitri/Nextcloud/Zotero/storage/BQTBRD84/1903.html}
}

@inproceedings{meloGaussianProcessesRegression2012,
  title = {Gaussian {{Processes}} for Regression : A Tutorial},
  shorttitle = {Gaussian {{Processes}} for Regression},
  abstract = {Gaussian processes are a powerful, non-parametric tool that can be be used in supervised learning, namely in regression but also in classification problems. The main advantages of this method are the ability of GPs to provide uncertainty estimates and to learn the noise and smoothness parameters from training data. The aim of this short tutorial is to provide the basic theoretical aspects of Gaussian Processes, as well as a brief practical overview on implementation. The main motivation of this work was to develop a new approach to detect outliers on acoustic navigation algorithms for Autonomous Underwater Vehicles, capable of adjusting to different operation scenarios, since this is a major problem in the majority of Autonomous Underwater Vehicles. In the last part of the tutorial, a brief insight on this actual problem, and the solution proposed, that involves Gaussian Processes as a predictor, and some background subtraction techniques is described.},
  date = {2012},
  keywords = {Acoustic cryptanalysis,Activation function,Algorithm,Approximation,Background subtraction,Effective method,Estimated,Expectation propagation,Gaussian process,Kalman filter,Kerrison Predictor,Normal Statistical Distribution,Subtraction Technique,Supervised learning},
  author = {Melo, José},
  file = {/home/dimitri/Nextcloud/Zotero/storage/2BMFEMFB/Melo - 2012 - Gaussian Processes for regression  a tutorial.pdf}
}

@article{damianouDeepGaussianProcesses2012,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1211.0358},
  primaryClass = {cs, math, stat},
  title = {Deep {{Gaussian Processes}}},
  url = {http://arxiv.org/abs/1211.0358},
  abstract = {In this paper we introduce deep Gaussian process (GP) models. Deep GPs are a deep belief network based on Gaussian process mappings. The data is modeled as the output of a multivariate GP. The inputs to that Gaussian process are then governed by another GP. A single layer model is equivalent to a standard GP or the GP latent variable model (GP-LVM). We perform inference in the model by approximate variational marginalization. This results in a strict lower bound on the marginal likelihood of the model which we use for model selection (number of layers and nodes per layer). Deep belief networks are typically applied to relatively large data sets using stochastic gradient descent for optimization. Our fully Bayesian treatment allows for the application of deep models even when data is scarce. Model selection by our variational bound shows that a five layer hierarchy is justified even when modelling a digit data set containing only 150 examples.},
  urldate = {2019-04-04},
  date = {2012-11-01},
  keywords = {Statistics - Machine Learning,Mathematics - Probability,Computer Science - Machine Learning,I.2.6,60G15; 58E30,G.1.2,G.3},
  author = {Damianou, Andreas C. and Lawrence, Neil D.},
  file = {/home/dimitri/Nextcloud/Zotero/storage/FCZ2J52I/Damianou and Lawrence - 2012 - Deep Gaussian Processes.pdf;/home/dimitri/Nextcloud/Zotero/storage/EPABC27J/1211.html}
}

@article{leeDeepNeuralNetworks2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1711.00165},
  primaryClass = {cs, stat},
  title = {Deep {{Neural Networks}} as {{Gaussian Processes}}},
  url = {http://arxiv.org/abs/1711.00165},
  abstract = {It has long been known that a single-layer fully-connected neural network with an i.i.d. prior over its parameters is equivalent to a Gaussian process (GP), in the limit of infinite network width. This correspondence enables exact Bayesian inference for infinite width neural networks on regression tasks by means of evaluating the corresponding GP. Recently, kernel functions which mimic multi-layer random neural networks have been developed, but only outside of a Bayesian framework. As such, previous work has not identified that these kernels can be used as covariance functions for GPs and allow fully Bayesian prediction with a deep neural network. In this work, we derive the exact equivalence between infinitely wide deep networks and GPs. We further develop a computationally efficient pipeline to compute the covariance function for these GPs. We then use the resulting GPs to perform Bayesian inference for wide deep neural networks on MNIST and CIFAR-10. We observe that trained neural network accuracy approaches that of the corresponding GP with increasing layer width, and that the GP uncertainty is strongly correlated with trained network prediction error. We further find that test performance increases as finite-width trained networks are made wider and more similar to a GP, and thus that GP predictions typically outperform those of finite-width networks. Finally we connect the performance of these GPs to the recent theory of signal propagation in random neural networks.},
  urldate = {2019-04-04},
  date = {2017-10-31},
  keywords = {Statistics - Machine Learning,Computer Science - Machine Learning},
  author = {Lee, Jaehoon and Bahri, Yasaman and Novak, Roman and Schoenholz, Samuel S. and Pennington, Jeffrey and Sohl-Dickstein, Jascha},
  file = {/home/dimitri/Nextcloud/Zotero/storage/DE6S84PZ/Lee et al. - 2017 - Deep Neural Networks as Gaussian Processes.pdf;/home/dimitri/Nextcloud/Zotero/storage/RSNF5RDD/1711.html}
}

@article{sileoCompositionSentenceEmbeddings2019,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1904.02464},
  primaryClass = {cs},
  title = {Composition of {{Sentence Embeddings}}:{{Lessons}} from {{Statistical Relational Learning}}},
  url = {http://arxiv.org/abs/1904.02464},
  shorttitle = {Composition of {{Sentence Embeddings}}},
  abstract = {Various NLP problems -- such as the prediction of sentence similarity, entailment, and discourse relations -- are all instances of the same general task: the modeling of semantic relations between a pair of textual elements. A popular model for such problems is to embed sentences into fixed size vectors, and use composition functions (e.g. concatenation or sum) of those vectors as features for the prediction. At the same time, composition of embeddings has been a main focus within the field of Statistical Relational Learning (SRL) whose goal is to predict relations between entities (typically from knowledge base triples). In this article, we show that previous work on relation prediction between texts implicitly uses compositions from baseline SRL models. We show that such compositions are not expressive enough for several tasks (e.g. natural language inference). We build on recent SRL models to address textual relational problems, showing that they are more expressive, and can alleviate issues from simpler compositions. The resulting models significantly improve the state of the art in both transferable sentence representation learning and relation prediction.},
  urldate = {2019-04-05},
  date = {2019-04-04},
  keywords = {Computer Science - Computation and Language},
  author = {Sileo, Damien and Van-De-Cruys, Tim and Pradel, Camille and Muller, Philippe},
  file = {/home/dimitri/Nextcloud/Zotero/storage/H8RDMLJF/Sileo et al. - 2019 - Composition of Sentence EmbeddingsLessons from St.pdf;/home/dimitri/Nextcloud/Zotero/storage/AP9TBIRV/1904.html}
}

@article{jainAttentionNotExplanation2019,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1902.10186},
  primaryClass = {cs},
  title = {Attention Is Not {{Explanation}}},
  url = {http://arxiv.org/abs/1902.10186},
  abstract = {Attention mechanisms have seen wide adoption in neural NLP models. In addition to improving predictive performance, these are often touted as affording transparency: models equipped with attention provide a distribution over attended-to input units, and this is often presented (at least implicitly) as communicating the relative importance of inputs. However, it is unclear what relationship exists between attention weights and model outputs. In this work, we perform extensive experiments across a variety of NLP tasks that aim to assess the degree to which attention weights provide meaningful `explanations' for predictions. We find that they largely do not. For example, learned attention weights are frequently uncorrelated with gradient-based measures of feature importance, and one can identify very different attention distributions that nonetheless yield equivalent predictions. Our findings show that standard attention modules do not provide meaningful explanations and should not be treated as though they do. Code for all experiments is available at https://github.com/successar/AttentionExplanation.},
  urldate = {2019-04-05},
  date = {2019-02-26},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  author = {Jain, Sarthak and Wallace, Byron C.},
  file = {/home/dimitri/Nextcloud/Zotero/storage/3K7ULXYM/Jain and Wallace - 2019 - Attention is not Explanation.pdf;/home/dimitri/Nextcloud/Zotero/storage/K7N98VCP/1902.html}
}

@article{hanUnsupervisedDomainAdaptation2019,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1904.02817},
  primaryClass = {cs},
  title = {Unsupervised {{Domain Adaptation}} of {{Contextualized Embeddings}}: {{A Case Study}} in {{Early Modern English}}},
  url = {http://arxiv.org/abs/1904.02817},
  shorttitle = {Unsupervised {{Domain Adaptation}} of {{Contextualized Embeddings}}},
  abstract = {Contextualized word embeddings such as ELMo and BERT provide a foundation for strong performance across a range of natural language processing tasks, in part by pretraining on a large and topically-diverse corpus. However, the applicability of this approach is unknown when the target domain varies substantially from the text used during pretraining. Specifically, we are interested the scenario in which labeled data is available in only a canonical source domain such as newstext, and the target domain is distinct from both the labeled corpus and the pretraining data. To address this scenario, we propose domain-adaptive fine-tuning, in which the contextualized embeddings are adapted by masked language modeling on the target domain. We test this approach on the challenging domain of Early Modern English, which differs substantially from existing pretraining corpora. Domain-adaptive fine-tuning yields an improvement of 4\textbackslash\% in part-of-speech tagging accuracy over a BERT baseline, substantially improving on prior work on this task.},
  urldate = {2019-04-08},
  date = {2019-04-04},
  keywords = {Computer Science - Digital Libraries,Computer Science - Computation and Language,Computer Science - Machine Learning},
  author = {Han, Xiaochuang and Eisenstein, Jacob},
  file = {/home/dimitri/Nextcloud/Zotero/storage/RT8FLXP4/Han and Eisenstein - 2019 - Unsupervised Domain Adaptation of Contextualized E.pdf;/home/dimitri/Nextcloud/Zotero/storage/4B9LSUQF/1904.html}
}

@article{dziriEvaluatingCoherenceDialogue2019,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1904.03371},
  primaryClass = {cs},
  title = {Evaluating {{Coherence}} in {{Dialogue Systems}} Using {{Entailment}}},
  url = {http://arxiv.org/abs/1904.03371},
  abstract = {Evaluating open-domain dialogue systems is difficult due to the diversity of possible correct answers. Automatic metrics such as BLEU correlate weakly with human annotations, resulting in a significant bias across different models and datasets. Some researchers resort to human judgment experimentation for assessing response quality, which is expensive, time consuming, and not scalable. Moreover, judges tend to evaluate a small number of dialogues, meaning that minor differences in evaluation configuration may lead to dissimilar results. In this paper, we present interpretable metrics for evaluating topic coherence by making use of distributed sentence representations. Furthermore, we introduce calculable approximations of human judgment based on conversational coherence by adopting state-of-the-art entailment techniques. Results show that our metrics can be used as a surrogate for human judgment, making it easy to evaluate dialogue systems on large-scale datasets and allowing an unbiased estimate for the quality of the responses.},
  urldate = {2019-04-09},
  date = {2019-04-06},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  author = {Dziri, Nouha and Kamalloo, Ehsan and Mathewson, Kory W. and Zaiane, Osmar},
  file = {/home/dimitri/Nextcloud/Zotero/storage/MVHTK4YK/Dziri et al. - 2019 - Evaluating Coherence in Dialogue Systems using Ent.pdf;/home/dimitri/Nextcloud/Zotero/storage/BCUIVB6S/1904.html}
}

@article{shevlinApplyRichPsychological2019,
  langid = {english},
  title = {Apply Rich Psychological Terms in {{AI}} with Care},
  volume = {1},
  issn = {2522-5839},
  url = {https://www.nature.com/articles/s42256-019-0039-y},
  doi = {10.1038/s42256-019-0039-y},
  abstract = {There is much to be gained from interdisciplinary efforts to tackle complex psychological notions such as ‘theory of mind’. However, careful and consistent communication is essential when comparing artificial and biological intelligence, say Henry Shevlin and Marta Halina.},
  number = {4},
  journaltitle = {Nature Machine Intelligence},
  urldate = {2019-04-09},
  date = {2019-04},
  pages = {165},
  author = {Shevlin, Henry and Halina, Marta},
  file = {/home/dimitri/Nextcloud/Zotero/storage/QDYI99KV/Shevlin and Halina - 2019 - Apply rich psychological terms in AI with care.pdf;/home/dimitri/Nextcloud/Zotero/storage/BI9FKKDU/s42256-019-0039-y.html}
}

@article{richLessonsArtificialIntelligence2019,
  langid = {english},
  title = {Lessons for Artificial Intelligence from the Study of Natural Stupidity},
  volume = {1},
  issn = {2522-5839},
  url = {http://www.nature.com/articles/s42256-019-0038-z},
  doi = {10.1038/s42256-019-0038-z},
  number = {4},
  journaltitle = {Nature Machine Intelligence},
  urldate = {2019-04-10},
  date = {2019-04},
  pages = {174-180},
  author = {Rich, Alexander S. and Gureckis, Todd M.},
  file = {/home/dimitri/Nextcloud/Zotero/storage/3C4SDZWS/Rich and Gureckis - 2019 - Lessons for artificial intelligence from the study.pdf}
}

@article{whitakerCharacterizingImpactGeometric2019,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1904.04866},
  primaryClass = {cs},
  title = {Characterizing the Impact of Geometric Properties of Word Embeddings on Task Performance},
  url = {http://arxiv.org/abs/1904.04866},
  abstract = {Analysis of word embedding properties to inform their use in downstream NLP tasks has largely been studied by assessing nearest neighbors. However, geometric properties of the continuous feature space contribute directly to the use of embedding features in downstream models, and are largely unexplored. We consider four properties of word embedding geometry, namely: position relative to the origin, distribution of features in the vector space, global pairwise distances, and local pairwise distances. We define a sequence of transformations to generate new embeddings that expose subsets of these properties to downstream models and evaluate change in task performance to understand the contribution of each property to NLP models. We transform publicly available pretrained embeddings from three popular toolkits (word2vec, GloVe, and FastText) and evaluate on a variety of intrinsic tasks, which model linguistic information in the vector space, and extrinsic tasks, which use vectors as input to machine learning models. We find that intrinsic evaluations are highly sensitive to absolute position, while extrinsic tasks rely primarily on local similarity. Our findings suggest that future embedding models and post-processing techniques should focus primarily on similarity to nearby points in vector space.},
  urldate = {2019-04-11},
  date = {2019-04-09},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  author = {Whitaker, Brendan and Newman-Griffis, Denis and Haldar, Aparajita and Ferhatosmanoglu, Hakan and Fosler-Lussier, Eric},
  file = {/home/dimitri/Nextcloud/Zotero/storage/XFEKYZAI/Whitaker et al. - 2019 - Characterizing the impact of geometric properties .pdf;/home/dimitri/Nextcloud/Zotero/storage/NZYCGS9F/1904.html}
}

@article{wangBERTHasMouth2019,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1902.04094},
  primaryClass = {cs},
  title = {{{BERT}} Has a {{Mouth}}, and {{It Must Speak}}: {{BERT}} as a {{Markov Random Field Language Model}}},
  url = {http://arxiv.org/abs/1902.04094},
  shorttitle = {{{BERT}} Has a {{Mouth}}, and {{It Must Speak}}},
  abstract = {We show that BERT (Devlin et al., 2018) is a Markov random field language model. This formulation gives way to a natural procedure to sample sentences from BERT. We generate from BERT and find that it can produce high-quality, fluent generations. Compared to the generations of a traditional left-to-right language model, BERT generates sentences that are more diverse but of slightly worse quality.},
  urldate = {2019-04-11},
  date = {2019-02-11},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  author = {Wang, Alex and Cho, Kyunghyun},
  file = {/home/dimitri/Nextcloud/Zotero/storage/S8QLHLQN/Wang and Cho - 2019 - BERT has a Mouth, and It Must Speak BERT as a Mar.pdf;/home/dimitri/Nextcloud/Zotero/storage/7JNBY733/1902.html}
}

@article{wangFewshotLearningSurvey2019,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1904.05046},
  primaryClass = {cs},
  title = {Few-Shot {{Learning}}: {{A Survey}}},
  url = {http://arxiv.org/abs/1904.05046},
  shorttitle = {Few-Shot {{Learning}}},
  abstract = {The quest of `can machines think' and `can machines do what human do' are quests that drive the development of artificial intelligence. Although recent artificial intelligence succeeds in many data intensive applications, it still lacks the ability of learning from limited exemplars and fast generalizing to new tasks. To tackle this problem, one has to turn to machine learning, which supports the scientific study of artificial intelligence. Particularly, a machine learning problem called Few-Shot Learning (FSL) targets at this case. It can rapidly generalize to new tasks of limited supervised experience by turning to prior knowledge, which mimics human's ability to acquire knowledge from few examples through generalization and analogy. It has been seen as a test-bed for real artificial intelligence, a way to reduce laborious data gathering and computationally costly training, and antidote for rare cases learning. With extensive works on FSL emerging, we give a comprehensive survey for it. We first give the formal definition for FSL. Then we point out the core issues of FSL, which turns the problem from "how to solve FSL" to "how to deal with the core issues". Accordingly, existing works from the birth of FSL to the most recent published ones are categorized in a unified taxonomy, with thorough discussion of the pros and cons for different categories. Finally, we envision possible future directions for FSL in terms of problem setup, techniques, applications and theory, hoping to provide insights to both beginners and experienced researchers.},
  urldate = {2019-04-11},
  date = {2019-04-10},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  author = {Wang, Yaqing and Yao, Quanming},
  file = {/home/dimitri/Nextcloud/Zotero/storage/8B4V9HNB/Wang and Yao - 2019 - Few-shot Learning A Survey.pdf;/home/dimitri/Nextcloud/Zotero/storage/G9J3I6XP/1904.html}
}

@thesis{galUncertaintyDeepLearning2016,
  title = {Uncertainty in {{Deep Learning}}},
  url = {https://www.semanticscholar.org/paper/Uncertainty-in-Deep-Learning-Gal/3c623c08329e129e784a5d03f7606ec8feba3a28},
  abstract = {Deep learning has attracted tremendous attention from researchers in various fields of information engineering such as AI, computer vision, and language processing [Kalchbrenner and Blunsom, 2013; Krizhevsky et al., 2012; Mnih et al., 2013], but also from more traditional sciences such as physics, biology, and manufacturing [Anjos et al., 2015; Baldi et al., 2014; Bergmann et al., 2014]. Neural networks, image processing tools such as convolutional neural networks, sequence processing models such as recurrent neural networks, and regularisation tools such as dropout, are used extensively. However, fields such as physics, biology, and manufacturing are ones in which representing model uncertainty is of crucial importance [Ghahramani, 2015; Krzywinski and Altman, 2013]. With the recent shift in many of these fields towards the use of Bayesian uncertainty [Herzog and Ostwald, 2013; Nuzzo, 2014; Trafimow and Marks, 2015], new needs arise from deep learning. In this work we develop tools to obtain practical uncertainty estimates in deep learning, casting recent deep learning tools as Bayesian models without changing either the models or the optimisation. In the first part of this thesis we develop the theory for such tools, providing applications and illustrative examples. We tie approximate inference in Bayesian models to dropout and other stochastic regularisation techniques, and assess the approximations empirically. We give example applications arising from this connection between modern deep learning and Bayesian modelling such as active learning of image data and data efficient deep reinforcement learning. We further demonstrate the method’s practicality through a survey of recent applications making use of the suggested tools in language applications, medical diagnostics, bioinformatics, image processing, and autonomous driving. In the second part of the thesis we explore its theoretical implications, and the insights stemming from the link between Bayesian modelling and deep learning. We discuss what determines model uncertainty properties, analyse the approximate inference analytically in the linear case, and theoretically examine various priors such as spike and slab priors.},
  institution = {{University of Cambridge}},
  date = {2016},
  keywords = {Reinforcement learning,Mathematical optimization,Estimated,Action potential,Approximation algorithm,Attention deficit hyperactivity disorder,Autonomous car,Bayesian network,Bioinformatics,Computer vision,Convolutional neural network,Deep learning,Dropout (neural networks),Image processing,Information engineering,Manufacturing Facilities,Neural Networks,Recurrent neural network,Science,Slab allocation,Stemming},
  author = {Gal, Yarin},
  file = {/home/dimitri/Nextcloud/Zotero/storage/4UXBH6G3/Gal - 2016 - Uncertainty in Deep Learning.pdf}
}

@article{ghahramaniProbabilisticMachineLearning2015,
  langid = {english},
  title = {Probabilistic Machine Learning and Artificial Intelligence},
  volume = {521},
  issn = {1476-4687},
  url = {https://www.nature.com/articles/nature14541},
  doi = {10.1038/nature14541},
  abstract = {How can a machine learn from experience? Probabilistic modelling provides a framework for understanding what learning is, and has therefore emerged as one of the principal theoretical and practical approaches for designing machines that learn from data acquired through experience. The probabilistic framework, which describes how to represent and manipulate uncertainty about models and predictions, has a central role in scientific data analysis, machine learning, robotics, cognitive science and artificial intelligence. This Review provides an introduction to this framework, and discusses some of the state-of-the-art advances in the field, namely, probabilistic programming, Bayesian optimization, data compression and automatic model discovery.},
  number = {7553},
  journaltitle = {Nature},
  urldate = {2019-04-11},
  date = {2015-05},
  pages = {452-459},
  author = {Ghahramani, Zoubin},
  file = {/home/dimitri/Nextcloud/Zotero/storage/A98SXN69/Ghahramani - 2015 - Probabilistic machine learning and artificial inte.pdf;/home/dimitri/Nextcloud/Zotero/storage/A5TPUMHQ/nature14541.html}
}

@article{settlesActiveLearning2012,
  langid = {english},
  title = {Active {{Learning}}},
  volume = {6},
  issn = {1939-4608, 1939-4616},
  url = {http://www.morganclaypool.com/doi/abs/10.2200/S00429ED1V01Y201207AIM018},
  doi = {10.2200/S00429ED1V01Y201207AIM018},
  number = {1},
  journaltitle = {Synthesis Lectures on Artificial Intelligence and Machine Learning},
  urldate = {2019-04-11},
  date = {2012-06-30},
  pages = {1-114},
  author = {Settles, Burr},
  file = {/home/dimitri/Nextcloud/Zotero/storage/W2QS5HUK/Settles - 2012 - Active Learning.pdf}
}

@article{shiUnsupervisedDialogStructure2019,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1904.03736},
  primaryClass = {cs},
  title = {Unsupervised {{Dialog Structure Learning}}},
  url = {http://arxiv.org/abs/1904.03736},
  abstract = {Learning a shared dialog structure from a set of task-oriented dialogs is an important challenge in computational linguistics. The learned dialog structure can shed light on how to analyze human dialogs, and more importantly contribute to the design and evaluation of dialog systems. We propose to extract dialog structures using a modified VRNN model with discrete latent vectors. Different from existing HMM-based models, our model is based on variational-autoencoder (VAE). Such model is able to capture more dynamics in dialogs beyond the surface forms of the language. We find that qualitatively, our method extracts meaningful dialog structure, and quantitatively, outperforms previous models on the ability to predict unseen data. We further evaluate the model's effectiveness in a downstream task, the dialog system building task. Experiments show that, by integrating the learned dialog structure into the reward function design, the model converges faster and to a better outcome in a reinforcement learning setting.},
  urldate = {2019-04-12},
  date = {2019-04-07},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  author = {Shi, Weiyan and Zhao, Tiancheng and Yu, Zhou},
  file = {/home/dimitri/Nextcloud/Zotero/storage/HZ9I7A2E/Shi et al. - 2019 - Unsupervised Dialog Structure Learning.pdf;/home/dimitri/Nextcloud/Zotero/storage/R6UBNIII/1904.html}
}

@article{bronsteinGeometricDeepLearning2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1611.08097},
  title = {Geometric Deep Learning: Going beyond {{Euclidean}} Data},
  volume = {34},
  issn = {1053-5888},
  url = {http://arxiv.org/abs/1611.08097},
  doi = {10.1109/MSP.2017.2693418},
  shorttitle = {Geometric Deep Learning},
  abstract = {Many scientific fields study data with an underlying structure that is a non-Euclidean space. Some examples include social networks in computational social sciences, sensor networks in communications, functional networks in brain imaging, regulatory networks in genetics, and meshed surfaces in computer graphics. In many applications, such geometric data are large and complex (in the case of social networks, on the scale of billions), and are natural targets for machine learning techniques. In particular, we would like to use deep neural networks, which have recently proven to be powerful tools for a broad range of problems from computer vision, natural language processing, and audio analysis. However, these tools have been most successful on data with an underlying Euclidean or grid-like structure, and in cases where the invariances of these structures are built into networks used to model them. Geometric deep learning is an umbrella term for emerging techniques attempting to generalize (structured) deep neural models to non-Euclidean domains such as graphs and manifolds. The purpose of this paper is to overview different examples of geometric deep learning problems and present available solutions, key difficulties, applications, and future research directions in this nascent field.},
  number = {4},
  journaltitle = {IEEE Signal Processing Magazine},
  urldate = {2019-04-12},
  date = {2017-07},
  pages = {18-42},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  author = {Bronstein, Michael M. and Bruna, Joan and LeCun, Yann and Szlam, Arthur and Vandergheynst, Pierre},
  file = {/home/dimitri/Nextcloud/Zotero/storage/KQKTILDN/Bronstein et al. - 2017 - Geometric deep learning going beyond Euclidean da.pdf;/home/dimitri/Nextcloud/Zotero/storage/UZNRDHAP/1611.html}
}

@inproceedings{ferreiraExpertbasedRewardShaping2013,
  location = {{Olomouc, Czech Republic}},
  title = {Expert-Based Reward Shaping and Exploration Scheme for Boosting Policy Learning of Dialogue Management},
  isbn = {978-1-4799-2756-2},
  url = {http://ieeexplore.ieee.org/document/6707714/},
  doi = {10.1109/ASRU.2013.6707714},
  eventtitle = {2013 {{IEEE Workshop}} on {{Automatic Speech Recognition}} \& {{Understanding}} ({{ASRU}})},
  booktitle = {2013 {{IEEE Workshop}} on {{Automatic Speech Recognition}} and {{Understanding}}},
  publisher = {{IEEE}},
  urldate = {2019-04-12},
  date = {2013-12},
  pages = {108-113},
  author = {Ferreira, Emmanuel and Lefevre, Fabrice},
  file = {/home/dimitri/Nextcloud/Zotero/storage/ND7G7QVU/Ferreira and Lefevre - 2013 - Expert-based reward shaping and exploration scheme.pdf}
}

@article{zhangAdversarialAttacksDeep2019,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1901.06796},
  primaryClass = {cs},
  title = {Adversarial {{Attacks}} on {{Deep Learning Models}} in {{Natural Language Processing}}: {{A Survey}}},
  url = {http://arxiv.org/abs/1901.06796},
  shorttitle = {Adversarial {{Attacks}} on {{Deep Learning Models}} in {{Natural Language Processing}}},
  abstract = {With the development of high computational devices, deep neural networks (DNNs), in recent years, have gained significant popularity in many Artificial Intelligence (AI) applications. However, previous efforts have shown that DNNs were vulnerable to strategically modified samples, named adversarial examples. These samples are generated with some imperceptible perturbations but can fool the DNNs to give false predictions. Inspired by the popularity of generating adversarial examples for image DNNs, research efforts on attacking DNNs for textual applications emerges in recent years. However, existing perturbation methods for images cannotbe directly applied to texts as text data is discrete. In this article, we review research works that address this difference and generatetextual adversarial examples on DNNs. We collect, select, summarize, discuss and analyze these works in a comprehensive way andcover all the related information to make the article self-contained. Finally, drawing on the reviewed literature, we provide further discussions and suggestions on this topic.},
  urldate = {2019-04-12},
  date = {2019-01-21},
  keywords = {Computer Science - Computation and Language},
  author = {Zhang, Wei Emma and Sheng, Quan Z. and Alhazmi, Ahoud and Li, Chenliang},
  file = {/home/dimitri/Nextcloud/Zotero/storage/3YEISHLH/Zhang et al. - 2019 - Adversarial Attacks on Deep Learning Models in Nat.pdf;/home/dimitri/Nextcloud/Zotero/storage/HKVIKPLC/1901.html}
}

@inproceedings{zahavyGrayingBlackBox2016,
  langid = {english},
  title = {Graying the Black Box: {{Understanding DQNs}}},
  url = {http://proceedings.mlr.press/v48/zahavy16.html},
  shorttitle = {Graying the Black Box},
  abstract = {In recent years there is a growing interest in using deep representations for reinforcement learning. In this paper, we present a methodology and tools to analyze Deep Q-networks (DQNs) in a non-bl...},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  booktitle = {International {{Conference}} on {{Machine Learning}}},
  urldate = {2019-04-12},
  date = {2016-06-11},
  pages = {1899-1908},
  author = {Zahavy, Tom and Ben-Zrihem, Nir and Mannor, Shie},
  file = {/home/dimitri/Nextcloud/Zotero/storage/A9VKBUZI/zahavy16-supp.pdf;/home/dimitri/Nextcloud/Zotero/storage/FXTSNYTW/Zahavy et al. - 2016 - Graying the black box Understanding DQNs.pdf;/home/dimitri/Nextcloud/Zotero/storage/IVTBF7LM/zahavy16.html}
}

@article{hendersonDeepReinforcementLearning2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1709.06560},
  primaryClass = {cs, stat},
  title = {Deep {{Reinforcement Learning}} That {{Matters}}},
  url = {http://arxiv.org/abs/1709.06560},
  abstract = {In recent years, significant progress has been made in solving challenging problems across various domains using deep reinforcement learning (RL). Reproducing existing work and accurately judging the improvements offered by novel methods is vital to sustaining this progress. Unfortunately, reproducing results for state-of-the-art deep RL methods is seldom straightforward. In particular, non-determinism in standard benchmark environments, combined with variance intrinsic to the methods, can make reported results tough to interpret. Without significance metrics and tighter standardization of experimental reporting, it is difficult to determine whether improvements over the prior state-of-the-art are meaningful. In this paper, we investigate challenges posed by reproducibility, proper experimental techniques, and reporting procedures. We illustrate the variability in reported metrics and results when comparing against common baselines and suggest guidelines to make future results in deep RL more reproducible. We aim to spur discussion about how to ensure continued progress in the field by minimizing wasted effort stemming from results that are non-reproducible and easily misinterpreted.},
  urldate = {2019-04-16},
  date = {2017-09-19},
  keywords = {Statistics - Machine Learning,Computer Science - Machine Learning},
  author = {Henderson, Peter and Islam, Riashat and Bachman, Philip and Pineau, Joelle and Precup, Doina and Meger, David},
  file = {/home/dimitri/Nextcloud/Zotero/storage/PJEJP7R9/Henderson et al. - 2017 - Deep Reinforcement Learning that Matters.pdf;/home/dimitri/Nextcloud/Zotero/storage/IV2G8XEY/1709.html}
}

@article{maniaSimpleRandomSearch2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1803.07055},
  primaryClass = {cs, math, stat},
  title = {Simple Random Search Provides a Competitive Approach to Reinforcement Learning},
  url = {http://arxiv.org/abs/1803.07055},
  abstract = {A common belief in model-free reinforcement learning is that methods based on random search in the parameter space of policies exhibit significantly worse sample complexity than those that explore the space of actions. We dispel such beliefs by introducing a random search method for training static, linear policies for continuous control problems, matching state-of-the-art sample efficiency on the benchmark MuJoCo locomotion tasks. Our method also finds a nearly optimal controller for a challenging instance of the Linear Quadratic Regulator, a classical problem in control theory, when the dynamics are not known. Computationally, our random search algorithm is at least 15 times more efficient than the fastest competing model-free methods on these benchmarks. We take advantage of this computational efficiency to evaluate the performance of our method over hundreds of random seeds and many different hyperparameter configurations for each benchmark task. Our simulations highlight a high variability in performance in these benchmark tasks, suggesting that commonly used estimations of sample efficiency do not adequately evaluate the performance of RL algorithms.},
  urldate = {2019-04-16},
  date = {2018-03-19},
  keywords = {Statistics - Machine Learning,Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Mathematics - Optimization and Control},
  author = {Mania, Horia and Guy, Aurelia and Recht, Benjamin},
  file = {/home/dimitri/Nextcloud/Zotero/storage/S5MWXRFU/Mania et al. - 2018 - Simple random search provides a competitive approa.pdf;/home/dimitri/Nextcloud/Zotero/storage/FHCCPQ7M/1803.html}
}

@article{zhaoRethinkingActionSpaces2019,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1902.08858},
  primaryClass = {cs},
  title = {Rethinking {{Action Spaces}} for {{Reinforcement Learning}} in {{End}}-to-End {{Dialog Agents}} with {{Latent Variable Models}}},
  url = {http://arxiv.org/abs/1902.08858},
  abstract = {Defining action spaces for conversational agents and optimizing their decision-making process with reinforcement learning is an enduring challenge. Common practice has been to use handcrafted dialog acts, or the output vocabulary, e.g. in neural encoder decoders, as the action spaces. Both have their own limitations. This paper proposes a novel latent action framework that treats the action spaces of an end-to-end dialog agent as latent variables and develops unsupervised methods in order to induce its own action space from the data. Comprehensive experiments are conducted examining both continuous and discrete action types and two different optimization methods based on stochastic variational inference. Results show that the proposed latent actions achieve superior empirical performance improvement over previous word-level policy gradient methods on both DealOrNoDeal and MultiWoz dialogs. Our detailed analysis also provides insights about various latent variable approaches for policy learning and can serve as a foundation for developing better latent actions in future research.},
  urldate = {2019-04-16},
  date = {2019-02-23},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  author = {Zhao, Tiancheng and Xie, Kaige and Eskenazi, Maxine},
  file = {/home/dimitri/Nextcloud/Zotero/storage/75K4H5SF/Zhao et al. - 2019 - Rethinking Action Spaces for Reinforcement Learnin.pdf;/home/dimitri/Nextcloud/Zotero/storage/HLT5EGC8/1902.html}
}

@article{ratnerDataProgrammingCreating2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1605.07723},
  primaryClass = {cs, stat},
  title = {Data {{Programming}}: {{Creating Large Training Sets}}, {{Quickly}}},
  url = {http://arxiv.org/abs/1605.07723},
  shorttitle = {Data {{Programming}}},
  abstract = {Large labeled training sets are the critical building blocks of supervised learning methods and are key enablers of deep learning techniques. For some applications, creating labeled training sets is the most time-consuming and expensive part of applying machine learning. We therefore propose a paradigm for the programmatic creation of training sets called data programming in which users express weak supervision strategies or domain heuristics as labeling functions, which are programs that label subsets of the data, but that are noisy and may conflict. We show that by explicitly representing this training set labeling process as a generative model, we can "denoise" the generated training set, and establish theoretically that we can recover the parameters of these generative models in a handful of settings. We then show how to modify a discriminative loss function to make it noise-aware, and demonstrate our method over a range of discriminative models including logistic regression and LSTMs. Experimentally, on the 2014 TAC-KBP Slot Filling challenge, we show that data programming would have led to a new winning score, and also show that applying data programming to an LSTM model leads to a TAC-KBP score almost 6 F1 points over a state-of-the-art LSTM baseline (and into second place in the competition). Additionally, in initial user studies we observed that data programming may be an easier way for non-experts to create machine learning models when training data is limited or unavailable.},
  urldate = {2019-04-16},
  date = {2016-05-25},
  keywords = {Statistics - Machine Learning,Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  author = {Ratner, Alexander and De Sa, Christopher and Wu, Sen and Selsam, Daniel and Ré, Christopher},
  file = {/home/dimitri/Nextcloud/Zotero/storage/DM9SNV2M/Ratner et al. - 2016 - Data Programming Creating Large Training Sets, Qu.pdf;/home/dimitri/Nextcloud/Zotero/storage/RJD8M3JQ/1605.html}
}

@article{ratnerSnorkelRapidTraining2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1711.10160},
  title = {Snorkel: {{Rapid Training Data Creation}} with {{Weak Supervision}}},
  volume = {11},
  issn = {21508097},
  url = {http://arxiv.org/abs/1711.10160},
  doi = {10.14778/3157794.3157797},
  shorttitle = {Snorkel},
  abstract = {Labeling training data is increasingly the largest bottleneck in deploying machine learning systems. We present Snorkel, a first-of-its-kind system that enables users to train state-of-the-art models without hand labeling any training data. Instead, users write labeling functions that express arbitrary heuristics, which can have unknown accuracies and correlations. Snorkel denoises their outputs without access to ground truth by incorporating the first end-to-end implementation of our recently proposed machine learning paradigm, data programming. We present a flexible interface layer for writing labeling functions based on our experience over the past year collaborating with companies, agencies, and research labs. In a user study, subject matter experts build models 2.8x faster and increase predictive performance an average 45.5\% versus seven hours of hand labeling. We study the modeling tradeoffs in this new setting and propose an optimizer for automating tradeoff decisions that gives up to 1.8x speedup per pipeline execution. In two collaborations, with the U.S. Department of Veterans Affairs and the U.S. Food and Drug Administration, and on four open-source text and image data sets representative of other deployments, Snorkel provides 132\% average improvements to predictive performance over prior heuristic approaches and comes within an average 3.60\% of the predictive performance of large hand-curated training sets.},
  number = {3},
  journaltitle = {Proceedings of the VLDB Endowment},
  urldate = {2019-04-16},
  date = {2017-11-01},
  pages = {269-282},
  keywords = {Statistics - Machine Learning,Computer Science - Machine Learning},
  author = {Ratner, Alexander and Bach, Stephen H. and Ehrenberg, Henry and Fries, Jason and Wu, Sen and Ré, Christopher},
  file = {/home/dimitri/Nextcloud/Zotero/storage/KENDFREY/Ratner et al. - 2017 - Snorkel Rapid Training Data Creation with Weak Su.pdf;/home/dimitri/Nextcloud/Zotero/storage/YWL8DGM3/1711.html}
}

@article{bachLearningStructureGenerative2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1703.00854},
  primaryClass = {cs, stat},
  title = {Learning the {{Structure}} of {{Generative Models}} without {{Labeled Data}}},
  url = {http://arxiv.org/abs/1703.00854},
  abstract = {Curating labeled training data has become the primary bottleneck in machine learning. Recent frameworks address this bottleneck with generative models to synthesize labels at scale from weak supervision sources. The generative model's dependency structure directly affects the quality of the estimated labels, but selecting a structure automatically without any labeled data is a distinct challenge. We propose a structure estimation method that maximizes the \$\textbackslash{}ell\_1\$-regularized marginal pseudolikelihood of the observed data. Our analysis shows that the amount of unlabeled data required to identify the true structure scales sublinearly in the number of possible dependencies for a broad class of models. Simulations show that our method is 100\$\textbackslash{}times\$ faster than a maximum likelihood approach and selects \$1/4\$ as many extraneous dependencies. We also show that our method provides an average of 1.5 F1 points of improvement over existing, user-developed information extraction applications on real-world data such as PubMed journal abstracts.},
  urldate = {2019-04-16},
  date = {2017-03-02},
  keywords = {Statistics - Machine Learning,Computer Science - Machine Learning},
  author = {Bach, Stephen H. and He, Bryan and Ratner, Alexander and Ré, Christopher},
  file = {/home/dimitri/Nextcloud/Zotero/storage/CVGAF7ZB/Bach et al. - 2017 - Learning the Structure of Generative Models withou.pdf;/home/dimitri/Nextcloud/Zotero/storage/DZ7NGBTE/1703.html}
}

@article{varmaLearningDependencyStructures2019,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1903.05844},
  primaryClass = {cs, stat},
  title = {Learning {{Dependency Structures}} for {{Weak Supervision Models}}},
  url = {http://arxiv.org/abs/1903.05844},
  abstract = {Labeling training data is a key bottleneck in the modern machine learning pipeline. Recent weak supervision approaches combine labels from multiple noisy sources by estimating their accuracies without access to ground truth labels; however, estimating the dependencies among these sources is a critical challenge. We focus on a robust PCA-based algorithm for learning these dependency structures, establish improved theoretical recovery rates, and outperform existing methods on various real-world tasks. Under certain conditions, we show that the amount of unlabeled data needed can scale sublinearly or even logarithmically with the number of sources \$m\$, improving over previous efforts that ignore the sparsity pattern in the dependency structure and scale linearly in \$m\$. We provide an information-theoretic lower bound on the minimum sample complexity of the weak supervision setting. Our method outperforms weak supervision approaches that assume conditionally-independent sources by up to 4.64 F1 points and previous structure learning approaches by up to 4.41 F1 points on real-world relation extraction and image classification tasks.},
  urldate = {2019-04-16},
  date = {2019-03-14},
  keywords = {Statistics - Machine Learning,Computer Science - Machine Learning},
  author = {Varma, Paroma and Sala, Frederic and He, Ann and Ratner, Alexander and Ré, Christopher},
  file = {/home/dimitri/Nextcloud/Zotero/storage/DEMUUSM8/Varma et al. - 2019 - Learning Dependency Structures for Weak Supervisio.pdf;/home/dimitri/Nextcloud/Zotero/storage/QVVYCHJT/1903.html}
}

@article{ratnerTrainingComplexModels2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1810.02840},
  primaryClass = {cs, stat},
  title = {Training {{Complex Models}} with {{Multi}}-{{Task Weak Supervision}}},
  url = {http://arxiv.org/abs/1810.02840},
  abstract = {As machine learning models continue to increase in complexity, collecting large hand-labeled training sets has become one of the biggest roadblocks in practice. Instead, weaker forms of supervision that provide noisier but cheaper labels are often used. However, these weak supervision sources have diverse and unknown accuracies, may output correlated labels, and may label different tasks or apply at different levels of granularity. We propose a framework for integrating and modeling such weak supervision sources by viewing them as labeling different related sub-tasks of a problem, which we refer to as the multi-task weak supervision setting. We show that by solving a matrix completion-style problem, we can recover the accuracies of these multi-task sources given their dependency structure, but without any labeled data, leading to higher-quality supervision for training an end model. Theoretically, we show that the generalization error of models trained with this approach improves with the number of unlabeled data points, and characterize the scaling with respect to the task and dependency structures. On three fine-grained classification problems, we show that our approach leads to average gains of 20.2 points in accuracy over a traditional supervised approach, 6.8 points over a majority vote baseline, and 4.1 points over a previously proposed weak supervision method that models tasks separately.},
  urldate = {2019-04-16},
  date = {2018-10-05},
  keywords = {Statistics - Machine Learning,Computer Science - Machine Learning},
  author = {Ratner, Alexander and Hancock, Braden and Dunnmon, Jared and Sala, Frederic and Pandey, Shreyash and Ré, Christopher},
  file = {/home/dimitri/Nextcloud/Zotero/storage/E38MTCZJ/Ratner et al. - 2018 - Training Complex Models with Multi-Task Weak Super.pdf;/home/dimitri/Nextcloud/Zotero/storage/VIS28T3Q/1810.html}
}

@article{mallinarBootstrappingConversationalAgents2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1812.06176},
  primaryClass = {cs},
  title = {Bootstrapping {{Conversational Agents With Weak Supervision}}},
  url = {http://arxiv.org/abs/1812.06176},
  abstract = {Many conversational agents in the market today follow a standard bot development framework which requires training intent classifiers to recognize user input. The need to create a proper set of training examples is often the bottleneck in the development process. In many occasions agent developers have access to historical chat logs that can provide a good quantity as well as coverage of training examples. However, the cost of labeling them with tens to hundreds of intents often prohibits taking full advantage of these chat logs. In this paper, we present a framework called \textbackslash{}textit\{search, label, and propagate\} (SLP) for bootstrapping intents from existing chat logs using weak supervision. The framework reduces hours to days of labeling effort down to minutes of work by using a search engine to find examples, then relies on a data programming approach to automatically expand the labels. We report on a user study that shows positive user feedback for this new approach to build conversational agents, and demonstrates the effectiveness of using data programming for auto-labeling. While the system is developed for training conversational agents, the framework has broader application in significantly reducing labeling effort for training text classifiers.},
  urldate = {2019-04-16},
  date = {2018-12-14},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  author = {Mallinar, Neil and Shah, Abhishek and Ugrani, Rajendra and Gupta, Ayush and Gurusankar, Manikandan and Ho, Tin Kam and Liao, Q. Vera and Zhang, Yunfeng and Bellamy, Rachel K. E. and Yates, Robert and Desmarais, Chris and McGregor, Blake},
  file = {/home/dimitri/Nextcloud/Zotero/storage/GHW24QAZ/Mallinar et al. - 2018 - Bootstrapping Conversational Agents With Weak Supe.pdf;/home/dimitri/Nextcloud/Zotero/storage/9VGL3XXM/1812.html}
}

@article{bachSnorkelDryBellCase2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1812.00417},
  primaryClass = {cs, stat},
  title = {Snorkel {{DryBell}}: {{A Case Study}} in {{Deploying Weak Supervision}} at {{Industrial Scale}}},
  url = {http://arxiv.org/abs/1812.00417},
  shorttitle = {Snorkel {{DryBell}}},
  abstract = {Labeling training data is one of the most costly bottlenecks in developing or modifying machine learning-based applications. We survey how resources from across an organization can be used as weak supervision sources for three classification tasks at Google, in order to bring development time and cost down by an order of magnitude. We build on the Snorkel framework, extending it as a new system, Snorkel DryBell, which integrates with Google's distributed production systems and enables engineers to develop and execute weak supervision strategies over millions of examples in less than thirty minutes. We find that Snorkel DryBell creates classifiers of comparable quality to ones trained using up to tens of thousands of hand-labeled examples, in part by leveraging organizational resources not servable in production which contribute an average 52\% performance improvement to the weakly supervised classifiers.},
  urldate = {2019-04-16},
  date = {2018-12-02},
  keywords = {Statistics - Machine Learning,Computer Science - Machine Learning},
  author = {Bach, Stephen H. and Rodriguez, Daniel and Liu, Yintao and Luo, Chong and Shao, Haidong and Xia, Cassandra and Sen, Souvik and Ratner, Alexander and Hancock, Braden and Alborzi, Houman and Kuchhal, Rahul and Ré, Christopher and Malkin, Rob},
  file = {/home/dimitri/Nextcloud/Zotero/storage/HCP7XGTD/Bach et al. - 2018 - Snorkel DryBell A Case Study in Deploying Weak Su.pdf;/home/dimitri/Nextcloud/Zotero/storage/Q44AHQN4/1812.html}
}

@online{HarnessingOrganizationalKnowledge,
  langid = {english},
  title = {Harnessing {{Organizational Knowledge}} for {{Machine Learning}}},
  url = {http://ai.googleblog.com/2019/03/harnessing-organizational-knowledge-for.html},
  abstract = {Posted by Alex Ratner, Stanford University and Cassandra Xia, Google AI     One of the biggest bottlenecks in developing machine learning (M...},
  journaltitle = {Google AI Blog},
  urldate = {2019-04-16},
  file = {/home/dimitri/Nextcloud/Zotero/storage/496PMGC5/harnessing-organizational-knowledge-for.html}
}

@article{keneshlooDeepReinforcementLearning2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1805.09461},
  primaryClass = {cs, stat},
  title = {Deep {{Reinforcement Learning For Sequence}} to {{Sequence Models}}},
  url = {http://arxiv.org/abs/1805.09461},
  abstract = {In recent times, sequence-to-sequence (seq2seq) models have gained a lot of popularity and provide state-of-the-art performance in a wide variety of tasks such as machine translation, headline generation, text summarization, speech to text conversion, and image caption generation. The underlying framework for all these models is usually a deep neural network comprising an encoder and a decoder. Although simple encoder-decoder models produce competitive results, many researchers have proposed additional improvements over these sequence-to-sequence models, e.g., using an attention-based model over the input, pointer-generation models, and self-attention models. However, such seq2seq models suffer from two common problems: 1) exposure bias and 2) inconsistency between train/test measurement. Recently, a completely novel point of view has emerged in addressing these two problems in seq2seq models, leveraging methods from reinforcement learning (RL). In this survey, we consider seq2seq problems from the RL point of view and provide a formulation combining the power of RL methods in decision-making with sequence-to-sequence models that enable remembering long-term memories. We present some of the most recent frameworks that combine concepts from RL and deep neural networks and explain how these two areas could benefit from each other in solving complex seq2seq tasks. Our work aims to provide insights into some of the problems that inherently arise with current approaches and how we can address them with better RL models. We also provide the source code for implementing most of the RL models discussed in this paper to support the complex task of abstractive text summarization.},
  urldate = {2019-04-17},
  date = {2018-05-23},
  keywords = {Statistics - Machine Learning,Computer Science - Machine Learning,I.2.6,I.2.10,I.2.7},
  author = {Keneshloo, Yaser and Shi, Tian and Ramakrishnan, Naren and Reddy, Chandan K.},
  file = {/home/dimitri/Nextcloud/Zotero/storage/I9CLUT8Y/Keneshloo et al. - 2018 - Deep Reinforcement Learning For Sequence to Sequen.pdf;/home/dimitri/Nextcloud/Zotero/storage/4RIQDPI5/1805.html}
}

@article{hendersonRepositoryConversationalDatasets2019,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1904.06472},
  primaryClass = {cs},
  title = {A {{Repository}} of {{Conversational Datasets}}},
  url = {http://arxiv.org/abs/1904.06472},
  abstract = {Progress in Machine Learning is often driven by the availability of large datasets, and consistent evaluation metrics for comparing modeling approaches. To this end, we present a repository of conversational datasets consisting of hundreds of millions of examples, and a standardised evaluation procedure for conversational response selection models using '1-of-100 accuracy'. The repository contains scripts that allow researchers to reproduce the standard datasets, or to adapt the pre-processing and data filtering steps to their needs. We introduce and evaluate several competitive baselines for conversational response selection, whose implementations are shared in the repository, as well as a neural encoder model that is trained on the entire training set.},
  urldate = {2019-04-18},
  date = {2019-04-12},
  keywords = {Computer Science - Computation and Language},
  author = {Henderson, Matthew and Budzianowski, Paweł and Casanueva, Iñigo and Coope, Sam and Gerz, Daniela and Kumar, Girish and Mrkšić, Nikola and Spithourakis, Georgios and Su, Pei-Hao and Vulić, Ivan and Wen, Tsung-Hsien},
  file = {/home/dimitri/Nextcloud/Zotero/storage/ZPI7GB2I/Henderson et al. - 2019 - A Repository of Conversational Datasets.pdf;/home/dimitri/Nextcloud/Zotero/storage/II559GXU/1904.html}
}

@article{wangWassersteinFisherRaoDocumentDistance2019,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1904.10294},
  primaryClass = {cs, stat},
  title = {Wasserstein-{{Fisher}}-{{Rao Document Distance}}},
  url = {http://arxiv.org/abs/1904.10294},
  abstract = {As a fundamental problem of natural language processing, it is important to measure the distance between different documents. Among the existing methods, the Word Mover's Distance (WMD) has shown remarkable success in document semantic matching for its clear physical insight as a parameter-free model. However, WMD is essentially based on the classical Wasserstein metric, thus it often fails to robustly represent the semantic similarity between texts of different lengths. In this paper, we apply the newly developed Wasserstein-Fisher-Rao (WFR) metric from unbalanced optimal transport theory to measure the distance between different documents. The proposed WFR document distance maintains the great interpretability and simplicity as WMD. We demonstrate that the WFR document distance has significant advantages when comparing the texts of different lengths. In addition, an accelerated Sinkhorn based algorithm with GPU implementation has been developed for the fast computation of WFR distances. The KNN classification results on eight datasets have shown its clear improvement over WMD.},
  urldate = {2019-04-24},
  date = {2019-04-23},
  keywords = {Statistics - Machine Learning,Computer Science - Computation and Language,Computer Science - Machine Learning},
  author = {Wang, Zihao and Zhou, Datong and Zhang, Yong and Wu, Hao and Bao, Chenglong},
  file = {/home/dimitri/Nextcloud/Zotero/storage/CBKF4YMF/Wang et al. - 2019 - Wasserstein-Fisher-Rao Document Distance.pdf;/home/dimitri/Nextcloud/Zotero/storage/VN8HQ7Z5/1904.html}
}

@article{fordeScientificMethodScience2019,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1904.10922},
  primaryClass = {cs, stat},
  title = {The {{Scientific Method}} in the {{Science}} of {{Machine Learning}}},
  url = {http://arxiv.org/abs/1904.10922},
  abstract = {In the quest to align deep learning with the sciences to address calls for rigor, safety, and interpretability in machine learning systems, this contribution identifies key missing pieces: the stages of hypothesis formulation and testing, as well as statistical and systematic uncertainty estimation -- core tenets of the scientific method. This position paper discusses the ways in which contemporary science is conducted in other domains and identifies potentially useful practices. We present a case study from physics and describe how this field has promoted rigor through specific methodological practices, and provide recommendations on how machine learning researchers can adopt these practices into the research ecosystem. We argue that both domain-driven experiments and application-agnostic questions of the inner workings of fundamental building blocks of machine learning models ought to be examined with the tools of the scientific method, to ensure we not only understand effect, but also begin to understand cause, which is the raison d'\textbackslash\^\{e\}tre of science.},
  urldate = {2019-04-25},
  date = {2019-04-24},
  keywords = {Statistics - Machine Learning,Computer Science - Machine Learning},
  author = {Forde, Jessica Zosa and Paganini, Michela},
  file = {/home/dimitri/Nextcloud/Zotero/storage/YNMX99HU/Forde and Paganini - 2019 - The Scientific Method in the Science of Machine Le.pdf;/home/dimitri/Nextcloud/Zotero/storage/HEEYUG86/1904.html}
}

@article{debieStochasticDeepNetworks2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1811.07429},
  primaryClass = {cs, stat},
  title = {Stochastic {{Deep Networks}}},
  url = {http://arxiv.org/abs/1811.07429},
  abstract = {Machine learning is increasingly targeting areas where input data cannot be accurately described by a single vector, but can be modeled instead using the more flexible concept of random vectors, namely probability measures or more simply point clouds of varying cardinality. Using deep architectures on measures poses, however, many challenging issues. Indeed, deep architectures are originally designed to handle fixedlength vectors, or, using recursive mechanisms, ordered sequences thereof. In sharp contrast, measures describe a varying number of weighted observations with no particular order. We propose in this work a deep framework designed to handle crucial aspects of measures, namely permutation invariances, variations in weights and cardinality. Architectures derived from this pipeline can (i) map measures to measures - using the concept of push-forward operators; (ii) bridge the gap between measures and Euclidean spaces - through integration steps. This allows to design discriminative networks (to classify or reduce the dimensionality of input measures), generative architectures (to synthesize measures) and recurrent pipelines (to predict measure dynamics). We provide a theoretical analysis of these building blocks, review our architectures' approximation abilities and robustness w.r.t. perturbation, and try them on various discriminative and generative tasks.},
  urldate = {2019-04-25},
  date = {2018-11-18},
  keywords = {Statistics - Machine Learning,Computer Science - Machine Learning},
  author = {family=Bie, given=Gwendoline, prefix=de, useprefix=true and Peyré, Gabriel and Cuturi, Marco},
  file = {/home/dimitri/Nextcloud/Zotero/storage/6P7NQ9RK/de Bie et al. - 2018 - Stochastic Deep Networks.pdf;/home/dimitri/Nextcloud/Zotero/storage/PIGZRQRU/1811.html}
}

@article{grothendieckQuelquesPointsAlgebre1957,
  langid = {english},
  title = {Sur Quelques Points d'algèbre Homologique, {{I}}},
  volume = {9},
  issn = {0040-8735},
  url = {http://projecteuclid.org/euclid.tmj/1178244839},
  doi = {10.2748/tmj/1178244839},
  number = {2},
  journaltitle = {Tohoku Mathematical Journal},
  shortjournal = {Tohoku Math. J.},
  urldate = {2019-04-29},
  date = {1957},
  pages = {119-221},
  author = {Grothendieck, Alexander},
  file = {/home/dimitri/Nextcloud/Zotero/storage/UF4CX7FF/Grothendieck - 1957 - Sur quelques points d'algèbre homologique, I.pdf}
}

@article{fongSevenSketchesCompositionality2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1803.05316},
  primaryClass = {math},
  title = {Seven {{Sketches}} in {{Compositionality}}: {{An Invitation}} to {{Applied Category Theory}}},
  url = {http://arxiv.org/abs/1803.05316},
  shorttitle = {Seven {{Sketches}} in {{Compositionality}}},
  abstract = {This book is an invitation to discover advanced topics in category theory through concrete, real-world examples. It aims to give a tour: a gentle, quick introduction to guide later exploration. The tour takes place over seven sketches, each pairing an evocative application, such as databases, electric circuits, or dynamical systems, with the exploration of a categorical structure, such as adjoint functors, enriched categories, or toposes. No prior knowledge of category theory is assumed. A feedback form for typos, comments, questions, and suggestions is available here: https://docs.google.com/document/d/160G9OFcP5DWT8Stn7TxdVx83DJnnf7d5GML0\_FOD5Wg/edit},
  urldate = {2019-04-29},
  date = {2018-03-14},
  keywords = {Mathematics - Category Theory,18-01},
  author = {Fong, Brendan and Spivak, David I.},
  file = {/home/dimitri/Nextcloud/Zotero/storage/WBUCWRPK/Fong and Spivak - 2018 - Seven Sketches in Compositionality An Invitation .pdf;/home/dimitri/Nextcloud/Zotero/storage/MT7MPULY/1803.html}
}

@article{chenNeuralNaturalLanguage2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1711.04289},
  primaryClass = {cs},
  title = {Neural {{Natural Language Inference Models Enhanced}} with {{External Knowledge}}},
  url = {http://arxiv.org/abs/1711.04289},
  abstract = {Modeling natural language inference is a very challenging task. With the availability of large annotated data, it has recently become feasible to train complex models such as neural-network-based inference models, which have shown to achieve the state-of-the-art performance. Although there exist relatively large annotated data, can machines learn all knowledge needed to perform natural language inference (NLI) from these data? If not, how can neural-network-based NLI models benefit from external knowledge and how to build NLI models to leverage it? In this paper, we enrich the state-of-the-art neural natural language inference models with external knowledge. We demonstrate that the proposed models improve neural NLI models to achieve the state-of-the-art performance on the SNLI and MultiNLI datasets.},
  urldate = {2019-04-30},
  date = {2017-11-12},
  keywords = {Computer Science - Computation and Language},
  author = {Chen, Qian and Zhu, Xiaodan and Ling, Zhen-Hua and Inkpen, Diana and Wei, Si},
  file = {/home/dimitri/Nextcloud/Zotero/storage/TM3J9VAZ/Chen et al. - 2017 - Neural Natural Language Inference Models Enhanced .pdf;/home/dimitri/Nextcloud/Zotero/storage/5KNZ9MVE/1711.html}
}

@article{kolouriOptimalMassTransport2017,
  title = {Optimal {{Mass Transport}}: {{Signal}} Processing and Machine-Learning Applications},
  volume = {34},
  issn = {1053-5888},
  url = {http://ieeexplore.ieee.org/document/7974883/},
  doi = {10.1109/MSP.2017.2695801},
  shorttitle = {Optimal {{Mass Transport}}},
  number = {4},
  journaltitle = {IEEE Signal Processing Magazine},
  shortjournal = {IEEE Signal Process. Mag.},
  urldate = {2019-04-30},
  date = {2017-07},
  pages = {43-59},
  author = {Kolouri, Soheil and Park, Se Rim and Thorpe, Matthew and Slepcev, Dejan and Rohde, Gustavo K.},
  file = {/home/dimitri/Nextcloud/Zotero/storage/9YQEAT7J/Kolouri et al. - 2017 - Optimal Mass Transport Signal processing and mach.pdf}
}

@article{schmidtSilurianHypothesisWould2019,
  langid = {english},
  title = {The {{Silurian}} Hypothesis: Would It Be Possible to Detect an Industrial Civilization in the Geological Record?},
  volume = {18},
  issn = {1473-5504, 1475-3006},
  url = {https://www.cambridge.org/core/product/identifier/S1473550418000095/type/journal_article},
  doi = {10.1017/S1473550418000095},
  shorttitle = {The {{Silurian}} Hypothesis},
  abstract = {Abstract
            If an industrial civilization had existed on Earth many millions of years prior to our own era, what traces would it have left and would they be detectable today? We summarize the likely geological fingerprint of the Anthropocene, and demonstrate that while clear, it will not differ greatly in many respects from other known events in the geological record. We then propose tests that could plausibly distinguish an industrial cause from an otherwise naturally occurring climate event.},
  number = {2},
  journaltitle = {International Journal of Astrobiology},
  shortjournal = {International Journal of Astrobiology},
  urldate = {2019-05-01},
  date = {2019-04},
  pages = {142-150},
  author = {Schmidt, Gavin A. and Frank, Adam},
  file = {/home/dimitri/Nextcloud/Zotero/storage/XLHC4GEB/Schmidt_Frank_2019_The Silurian hypothesis.pdf}
}

@article{aschOpinionsSocialPressure1955,
  title = {Opinions and {{Social Pressure}}},
  volume = {193},
  issn = {0036-8733},
  url = {https://www.jstor.org/stable/24943779},
  number = {5},
  journaltitle = {Scientific American},
  urldate = {2019-05-01},
  date = {1955},
  pages = {31-35},
  author = {Asch, Solomon E.},
  file = {/home/dimitri/Nextcloud/Zotero/storage/99479367/Asch_1955_Opinions and Social Pressure.pdf}
}

@article{slivkinsIntroductionMultiArmedBandits2019,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1904.07272},
  primaryClass = {cs, stat},
  title = {Introduction to {{Multi}}-{{Armed Bandits}}},
  url = {http://arxiv.org/abs/1904.07272},
  abstract = {Multi-armed bandits a simple but very powerful framework for algorithms that make decisions over time under uncertainty. An enormous body of work has accumulated over the years, covered in several books and surveys. This book provides a more introductory, textbook-like treatment of the subject. Each chapter tackles a particular line of work, providing a self-contained, teachable technical introduction and a review of the more advanced results. The chapters are as follows: Stochastic bandits; Lower bounds; Bayesian Bandits and Thompson Sampling; Lipschitz Bandits; Full Feedback and Adversarial Costs; Adversarial Bandits; Linear Costs and Semi-bandits; Contextual Bandits; Bandits and Zero-Sum Games; Bandits with Knapsacks; Incentivized Exploration and Connections to Mechanism Design.},
  urldate = {2019-05-02},
  date = {2019-04-15},
  keywords = {Statistics - Machine Learning,Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Data Structures and Algorithms},
  author = {Slivkins, Aleksandrs},
  file = {/home/dimitri/Nextcloud/Zotero/storage/2TJYMBL9/Slivkins - 2019 - Introduction to Multi-Armed Bandits.pdf;/home/dimitri/Nextcloud/Zotero/storage/EUHLP8I9/1904.html}
}

@article{botvinickReinforcementLearningFast2019,
  langid = {english},
  title = {Reinforcement {{Learning}}, {{Fast}} and {{Slow}}},
  issn = {13646613},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S1364661319300610},
  doi = {10.1016/j.tics.2019.02.006},
  journaltitle = {Trends in Cognitive Sciences},
  shortjournal = {Trends in Cognitive Sciences},
  urldate = {2019-05-03},
  date = {2019-04},
  pages = {S1364661319300610},
  author = {Botvinick, Matthew and Ritter, Sam and Wang, Jane X. and Kurth-Nelson, Zeb and Blundell, Charles and Hassabis, Demis},
  file = {/home/dimitri/Nextcloud/Zotero/storage/RXKZECCA/Botvinick et al. - 2019 - Reinforcement Learning, Fast and Slow.pdf}
}

@inproceedings{kimEfficientBayesianInference2013,
  location = {{Istanbul, Turkey}},
  title = {Efficient {{Bayesian}} Inference Methods via Convex Optimization and Optimal Transport},
  isbn = {978-1-4799-0446-4},
  url = {http://ieeexplore.ieee.org/document/6620628/},
  doi = {10.1109/ISIT.2013.6620628},
  eventtitle = {2013 {{IEEE International Symposium}} on {{Information Theory}} ({{ISIT}})},
  booktitle = {2013 {{IEEE International Symposium}} on {{Information Theory}}},
  publisher = {{IEEE}},
  urldate = {2019-05-03},
  date = {2013-07},
  pages = {2259-2263},
  author = {Kim, Sanggyun and Ma, Rui and Mesa, Diego and Coleman, Todd P.},
  file = {/home/dimitri/Nextcloud/Zotero/storage/9RGN7V3H/Kim et al. - 2013 - Efficient Bayesian inference methods via convex op.pdf}
}

@article{rahwanMachineBehaviour2019,
  langid = {english},
  title = {Machine Behaviour},
  volume = {568},
  issn = {0028-0836, 1476-4687},
  url = {http://www.nature.com/articles/s41586-019-1138-y},
  doi = {10.1038/s41586-019-1138-y},
  number = {7753},
  journaltitle = {Nature},
  shortjournal = {Nature},
  urldate = {2019-05-03},
  date = {2019-04},
  pages = {477-486},
  author = {Rahwan, Iyad and Cebrian, Manuel and Obradovich, Nick and Bongard, Josh and Bonnefon, Jean-François and Breazeal, Cynthia and Crandall, Jacob W. and Christakis, Nicholas A. and Couzin, Iain D. and Jackson, Matthew O. and Jennings, Nicholas R. and Kamar, Ece and Kloumann, Isabel M. and Larochelle, Hugo and Lazer, David and McElreath, Richard and Mislove, Alan and Parkes, David C. and Pentland, Alex ‘Sandy’ and Roberts, Margaret E. and Shariff, Azim and Tenenbaum, Joshua B. and Wellman, Michael},
  file = {/home/dimitri/Nextcloud/Zotero/storage/VDKW2YEL/Rahwan et al. - 2019 - Machine behaviour.pdf}
}

@article{ducklowUpperOceanCarbon2001,
  title = {Upper {{Ocean Carbon Export}} and the {{Biological Pump}}},
  volume = {14},
  issn = {10428275},
  url = {https://tos.org/oceanography/article/upper-ocean-carbon-export-and-the-biological-pump},
  doi = {10.5670/oceanog.2001.06},
  number = {4},
  journaltitle = {Oceanography},
  shortjournal = {oceanog},
  urldate = {2019-05-04},
  date = {2001},
  pages = {50-58},
  author = {Ducklow, Hugh and Steinberg, Deborah and Buesseler, Ken},
  file = {/home/dimitri/Nextcloud/Zotero/storage/ZZ25525E/Ducklow et al_2001_Upper Ocean Carbon Export and the Biological Pump.pdf}
}

@article{monroeEcoevolutionaryDynamicsCarbon2018,
  langid = {english},
  title = {Ecoevolutionary {{Dynamics}} of {{Carbon Cycling}} in the {{Anthropocene}}},
  volume = {33},
  issn = {01695347},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0169534717303245},
  doi = {10.1016/j.tree.2017.12.006},
  number = {3},
  journaltitle = {Trends in Ecology \& Evolution},
  shortjournal = {Trends in Ecology \& Evolution},
  urldate = {2019-05-04},
  date = {2018-03},
  pages = {213-225},
  author = {Monroe, J. Grey and Markman, David W. and Beck, Whitney S. and Felton, Andrew J. and Vahsen, Megan L. and Pressler, Yamina},
  file = {/home/dimitri/Nextcloud/Zotero/storage/5HAIPSSV/Monroe et al_2018_Ecoevolutionary Dynamics of Carbon Cycling in the Anthropocene.pdf}
}

@article{wommackVirioplanktonVirusesAquatic2000,
  langid = {english},
  title = {Virioplankton: {{Viruses}} in {{Aquatic Ecosystems}}},
  volume = {64},
  issn = {1092-2172},
  url = {http://mmbr.asm.org/cgi/doi/10.1128/MMBR.64.1.69-114.2000},
  doi = {10.1128/MMBR.64.1.69-114.2000},
  shorttitle = {Virioplankton},
  number = {1},
  journaltitle = {Microbiology and Molecular Biology Reviews},
  shortjournal = {Microbiology and Molecular Biology Reviews},
  urldate = {2019-05-04},
  date = {2000-03-01},
  pages = {69-114},
  author = {Wommack, K. E. and Colwell, R. R.},
  file = {/home/dimitri/Nextcloud/Zotero/storage/N5ZGVA2P/Wommack_Colwell_2000_Virioplankton.pdf}
}

@article{munnVirusesPathogensMarine2006,
  langid = {english},
  title = {Viruses as Pathogens of Marine Organisms—from Bacteria to Whales},
  volume = {86},
  issn = {0025-3154, 1469-7769},
  url = {https://www.cambridge.org/core/product/identifier/S002531540601335X/type/journal_article},
  doi = {10.1017/S002531540601335X},
  abstract = {Viruses are the most abundant members of marine ecosystems and play an enormous role in ocean processes through their interactions with all types of marine organisms. This short review provides examples of the dramatic increase in our knowledge of the diversity of marine viruses as pathogens of bacteria, protists, molluscs, crustaceans, cnidaria, reptiles, fish and mammals. Several examples are provided showing evidence of evolution of new strains, changes in virulence, and transfer of viruses between ecosystems. The natural and anthropogenic causes of these shifts are discussed. Despite considerable advances in recent years, knowledge of the importance of viruses in many important groups of marine organisms is lacking or incomplete. Suggestions for future investigations necessary to understand the dynamics of biogeochemical processes and the impacts of disease in our oceans are proposed.},
  number = {3},
  journaltitle = {Journal of the Marine Biological Association of the United Kingdom},
  shortjournal = {J. Mar. Biol. Ass.},
  urldate = {2019-05-04},
  date = {2006-06},
  pages = {453-467},
  author = {Munn, Colin B.},
  file = {/home/dimitri/Nextcloud/Zotero/storage/HRU3Z5XC/Munn_2006_Viruses as pathogens of marine organisms—from bacteria to whales.pdf}
}

@article{ravenAquaticVirusesEmerging2006,
  langid = {english},
  title = {Aquatic Viruses: The Emerging Story},
  volume = {86},
  issn = {0025-3154, 1469-7769},
  url = {https://www.cambridge.org/core/product/identifier/S0025315406013348/type/journal_article},
  doi = {10.1017/S0025315406013348},
  shorttitle = {Aquatic Viruses},
  abstract = {It is likely that all living organisms can be infected by 
one or more viruses. One of the latest higher taxa to be 
converted from ‘no characterized viruses’ to ‘well  characterized 
viruses’ are the diatoms (Bacillariophyceae, 
Heterokontophyta) with the recent publication of three 
papers characterizing an ssRNA and a ssDNA virus from 
two genera (
              Chaetoceros
              and
              Rhizosolenia
              ) of marine  planktonic 
diatom (Nagasaki et al., 2004, 2005; Bettarel et al., 
2005). It would have been strange if viruses had not been 
able to exploit the dominant, in terms of global primary 
production, photosynthetic organisms in the ocean (assimilating 
perhaps as much as 20 Pg inorganic C into organic 
C per year), despite the less than completely convincing 
arguments assembled by Raven \& Waite (2004) as to 
possible anti-viral defences unique to diatoms.},
  number = {3},
  journaltitle = {Journal of the Marine Biological Association of the United Kingdom},
  shortjournal = {J. Mar. Biol. Ass.},
  urldate = {2019-05-04},
  date = {2006-06},
  pages = {449-451},
  author = {Raven, John A.},
  file = {/home/dimitri/Nextcloud/Zotero/storage/PFL36J3D/Raven_2006_Aquatic viruses.pdf}
}

@article{suttleMarineVirusesMajor2007,
  langid = {english},
  title = {Marine Viruses — Major Players in the Global Ecosystem},
  volume = {5},
  issn = {1740-1526, 1740-1534},
  url = {http://www.nature.com/articles/nrmicro1750},
  doi = {10.1038/nrmicro1750},
  number = {10},
  journaltitle = {Nature Reviews Microbiology},
  shortjournal = {Nat Rev Microbiol},
  urldate = {2019-05-04},
  date = {2007-10},
  pages = {801-812},
  author = {Suttle, Curtis A.},
  file = {/home/dimitri/Nextcloud/Zotero/storage/LQ6LQA2N/Suttle_2007_Marine viruses — major players in the global ecosystem.pdf}
}

@article{awodeyStructuralismInvarianceUnivalence2014,
  langid = {english},
  title = {Structuralism, {{Invariance}}, and {{Univalence}}},
  volume = {22},
  issn = {0031-8019, 1744-6406},
  url = {https://academic.oup.com/philmat/article-lookup/doi/10.1093/philmat/nkt030},
  doi = {10.1093/philmat/nkt030},
  number = {1},
  journaltitle = {Philosophia Mathematica},
  shortjournal = {Philosophia Mathematica},
  urldate = {2019-05-06},
  date = {2014-02-01},
  pages = {1-11},
  author = {Awodey, S.},
  file = {/home/dimitri/Nextcloud/Zotero/storage/LYGHZV5Y/awodey2013.pdf}
}

@article{dulac-arnoldChallengesRealWorldReinforcement2019,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1904.12901},
  primaryClass = {cs, stat},
  title = {Challenges of {{Real}}-{{World Reinforcement Learning}}},
  url = {http://arxiv.org/abs/1904.12901},
  abstract = {Reinforcement learning (RL) has proven its worth in a series of artificial domains, and is beginning to show some successes in real-world scenarios. However, much of the research advances in RL are often hard to leverage in real-world systems due to a series of assumptions that are rarely satisfied in practice. We present a set of nine unique challenges that must be addressed to productionize RL to real world problems. For each of these challenges, we specify the exact meaning of the challenge, present some approaches from the literature, and specify some metrics for evaluating that challenge. An approach that addresses all nine challenges would be applicable to a large number of real world problems. We also present an example domain that has been modified to present these challenges as a testbed for practical RL research.},
  urldate = {2019-05-07},
  date = {2019-04-29},
  keywords = {Statistics - Machine Learning,Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics},
  author = {Dulac-Arnold, Gabriel and Mankowitz, Daniel and Hester, Todd},
  file = {/home/dimitri/Nextcloud/Zotero/storage/3RZPJH7U/Dulac-Arnold et al. - 2019 - Challenges of Real-World Reinforcement Learning.pdf;/home/dimitri/Nextcloud/Zotero/storage/BXN8X7BS/1904.html}
}

@article{yiCoherentEngagingSpoken2019,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1904.13015},
  primaryClass = {cs},
  title = {Towards {{Coherent}} and {{Engaging Spoken Dialog Response Generation Using Automatic Conversation Evaluators}}},
  url = {http://arxiv.org/abs/1904.13015},
  abstract = {Encoder-decoder based neural architectures serve as the basis of state-of-the-art approaches in end-to-end open domain dialog systems. Since most of such systems are trained with a maximum likelihood(MLE) objective they suffer from issues such as lack of generalizability and the generic response problem, i.e., a system response that can be an answer to a large number of user utterances, e.g., "Maybe, I don't know." Having explicit feedback on the relevance and interestingness of a system response at each turn can be a useful signal for mitigating such issues and improving system quality by selecting responses from different approaches. Towards this goal, we present a system that evaluates chatbot responses at each dialog turn for coherence and engagement. Our system provides explicit turn-level dialog quality feedback, which we show to be highly correlated with human evaluation. To show that incorporating this feedback in the neural response generation models improves dialog quality, we present two different and complementary mechanisms to incorporate explicit feedback into a neural response generation model: reranking and direct modification of the loss function during training. Our studies show that a response generation model that incorporates these combined feedback mechanisms produce more engaging and coherent responses in an open-domain spoken dialog setting, significantly improving the response quality using both automatic and human evaluation.},
  urldate = {2019-05-07},
  date = {2019-04-29},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  author = {Yi, Sanghyun and Goel, Rahul and Khatri, Chandra and Cervone, Alessandra and Chung, Tagyoung and Hedayatnia, Behnam and Venkatesh, Anu and Gabriel, Raefer and Hakkani-Tur, Dilek},
  file = {/home/dimitri/Nextcloud/Zotero/storage/RTMASP7G/Yi et al. - 2019 - Towards Coherent and Engaging Spoken Dialog Respon.pdf;/home/dimitri/Nextcloud/Zotero/storage/LUTDPQK5/1904.html}
}

@inproceedings{stolckeDialogActModelling1998,
  langid = {english},
  title = {Dialog Act Modelling for Conversational Speech},
  isbn = {978-1-57735-046-0},
  url = {https://www.era.lib.ed.ac.uk/handle/1842/1045},
  abstract = {We describe an integrated approach for statistical modeling of discourse structure for natural conversational speech. Our model is based on 42 'dialog acts’ (e.g., Statement, Question, Backchannel, Agreement, Disagreement, Apology), which were hand-labeled in 1155 conversations from the Switchboard corpus of spontaneous human-to-human telephone speech. We developed several models and algorithms to automatically detect dialog acts from transcribed or automatically recognized words and from prosodic properties of the speech signal, and by using a statistical discourse grammar. All of these components were probabilistic in nature and estimated from data, employing a variety of techniques (hidden Markov models, N-gram language models, maximum entropy estimation, decision tree classifiers, and neural networks). In preliminary studies, we achieved a dialog act labeling accuracy of 65\% based on recognized words and prosody, and an accuracy of 72\% based on word transcripts. Since humans achieve 84\% on this task (with chance performance at 35\%) we find these results encouraging.},
  publisher = {{AAAI Press}},
  urldate = {2019-05-07},
  date = {1998},
  author = {Stolcke, Andreas and Shriberg, Elizabeth and Bates, Rebecca and Coccaro, Noah and Jurafsky, Daniel and Martin, Rachel and Meteer, Marie and Ries, Klaus and Taylor, Paul and Van Ess-Dykema, Carol},
  file = {/home/dimitri/Nextcloud/Zotero/storage/HR25EBGW/Stolcke et al. - 1998 - Dialog act modelling for conversational speech.pdf;/home/dimitri/Nextcloud/Zotero/storage/QCW9JBZL/1045.html}
}

@incollection{courtyDomainAdaptationRegularized2014,
  location = {{Berlin, Heidelberg}},
  title = {Domain {{Adaptation}} with {{Regularized Optimal Transport}}},
  volume = {8724},
  isbn = {978-3-662-44847-2 978-3-662-44848-9},
  url = {http://link.springer.com/10.1007/978-3-662-44848-9_18},
  booktitle = {Machine {{Learning}} and {{Knowledge Discovery}} in {{Databases}}},
  publisher = {{Springer Berlin Heidelberg}},
  urldate = {2019-05-09},
  date = {2014},
  pages = {274-289},
  author = {Courty, Nicolas and Flamary, Rémi and Tuia, Devis},
  editor = {Calders, Toon and Esposito, Floriana and Hüllermeier, Eyke and Meo, Rosa},
  file = {/home/dimitri/Nextcloud/Zotero/storage/5TY3JXAY/Courty et al. - 2014 - Domain Adaptation with Regularized Optimal Transpo.pdf},
  doi = {10.1007/978-3-662-44848-9_18}
}

@article{courtyOptimalTransportDomain2017,
  title = {Optimal {{Transport}} for {{Domain Adaptation}}},
  volume = {39},
  issn = {0162-8828, 2160-9292},
  url = {http://ieeexplore.ieee.org/document/7586038/},
  doi = {10.1109/TPAMI.2016.2615921},
  number = {9},
  journaltitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  urldate = {2019-05-09},
  date = {2017-09-01},
  pages = {1853-1865},
  author = {Courty, Nicolas and Flamary, Remi and Tuia, Devis and Rakotomamonjy, Alain},
  file = {/home/dimitri/Nextcloud/Zotero/storage/6FUGX4KX/Courty et al. - 2017 - Optimal Transport for Domain Adaptation.pdf}
}

@article{elmoselhyBayesianInferenceOptimal2012,
  langid = {english},
  title = {Bayesian Inference with Optimal Maps},
  volume = {231},
  issn = {00219991},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0021999112003956},
  doi = {10.1016/j.jcp.2012.07.022},
  number = {23},
  journaltitle = {Journal of Computational Physics},
  shortjournal = {Journal of Computational Physics},
  urldate = {2019-05-09},
  date = {2012-10},
  pages = {7815-7850},
  author = {El Moselhy, Tarek A. and Marzouk, Youssef M.},
  file = {/home/dimitri/Nextcloud/Zotero/storage/AK5CQTRP/El Moselhy and Marzouk - 2012 - Bayesian inference with optimal maps.pdf}
}

@incollection{frognerLearningWassersteinLoss2015,
  title = {Learning with a {{Wasserstein Loss}}},
  url = {http://papers.nips.cc/paper/5679-learning-with-a-wasserstein-loss.pdf},
  booktitle = {Advances in {{Neural Information Processing Systems}} 28},
  publisher = {{Curran Associates, Inc.}},
  urldate = {2019-05-09},
  date = {2015},
  pages = {2053--2061},
  author = {Frogner, Charlie and Zhang, Chiyuan and Mobahi, Hossein and Araya, Mauricio and Poggio, Tomaso A},
  editor = {Cortes, C. and Lawrence, N. D. and Lee, D. D. and Sugiyama, M. and Garnett, R.},
  file = {/home/dimitri/Nextcloud/Zotero/storage/AJGYHVMN/Frogner et al. - 2015 - Learning with a Wasserstein Loss.pdf;/home/dimitri/Nextcloud/Zotero/storage/LIAAVV45/5679-learning-with-a-wasserstein-loss.html}
}

@article{bonneelSlicedRadonWasserstein2015,
  langid = {english},
  title = {Sliced and {{Radon Wasserstein Barycenters}} of {{Measures}}},
  volume = {51},
  issn = {0924-9907, 1573-7683},
  url = {http://link.springer.com/10.1007/s10851-014-0506-3},
  doi = {10.1007/s10851-014-0506-3},
  number = {1},
  journaltitle = {Journal of Mathematical Imaging and Vision},
  shortjournal = {J Math Imaging Vis},
  urldate = {2019-05-09},
  date = {2015-01},
  pages = {22-45},
  author = {Bonneel, Nicolas and Rabin, Julien and Peyré, Gabriel and Pfister, Hanspeter},
  file = {/home/dimitri/Nextcloud/Zotero/storage/9SKGYDK4/bonneel2014.pdf}
}

@article{frognerLearningEmbeddingsEntropic2019,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1905.03329},
  primaryClass = {cs, stat},
  title = {Learning {{Embeddings}} into {{Entropic Wasserstein Spaces}}},
  url = {http://arxiv.org/abs/1905.03329},
  abstract = {Euclidean embeddings of data are fundamentally limited in their ability to capture latent semantic structures, which need not conform to Euclidean spatial assumptions. Here we consider an alternative, which embeds data as discrete probability distributions in a Wasserstein space, endowed with an optimal transport metric. Wasserstein spaces are much larger and more flexible than Euclidean spaces, in that they can successfully embed a wider variety of metric structures. We exploit this flexibility by learning an embedding that captures semantic information in the Wasserstein distance between embedded distributions. We examine empirically the representational capacity of our learned Wasserstein embeddings, showing that they can embed a wide variety of metric structures with smaller distortion than an equivalent Euclidean embedding. We also investigate an application to word embedding, demonstrating a unique advantage of Wasserstein embeddings: We can visualize the high-dimensional embedding directly, since it is a probability distribution on a low-dimensional space. This obviates the need for dimensionality reduction techniques like t-SNE for visualization.},
  urldate = {2019-05-10},
  date = {2019-05-08},
  keywords = {Statistics - Machine Learning,Computer Science - Machine Learning},
  author = {Frogner, Charlie and Mirzazadeh, Farzaneh and Solomon, Justin},
  file = {/home/dimitri/Nextcloud/Zotero/storage/UAEFXQR2/Frogner et al. - 2019 - Learning Embeddings into Entropic Wasserstein Spac.pdf;/home/dimitri/Nextcloud/Zotero/storage/AY98Y254/1905.html}
}

@incollection{cuturiSinkhornDistancesLightspeed2013,
  title = {Sinkhorn {{Distances}}: {{Lightspeed Computation}} of {{Optimal Transport}}},
  url = {http://papers.nips.cc/paper/4927-sinkhorn-distances-lightspeed-computation-of-optimal-transport.pdf},
  shorttitle = {Sinkhorn {{Distances}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 26},
  publisher = {{Curran Associates, Inc.}},
  urldate = {2019-05-10},
  date = {2013},
  pages = {2292--2300},
  author = {Cuturi, Marco},
  editor = {Burges, C. J. C. and Bottou, L. and Welling, M. and Ghahramani, Z. and Weinberger, K. Q.},
  file = {/home/dimitri/Nextcloud/Zotero/storage/ALCWJV4I/Cuturi - 2013 - Sinkhorn Distances Lightspeed Computation of Opti.pdf;/home/dimitri/Nextcloud/Zotero/storage/D2BFL994/4927-sinkhorn-distances-lightspeed-computation-of-optimal-transport.html}
}

@article{deriuSurveyEvaluationMethods2019,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1905.04071},
  primaryClass = {cs},
  title = {Survey on {{Evaluation Methods}} for {{Dialogue Systems}}},
  url = {http://arxiv.org/abs/1905.04071},
  abstract = {In this paper we survey the methods and concepts developed for the evaluation of dialogue systems. Evaluation is a crucial part during the development process. Often, dialogue systems are evaluated by means of human evaluations and questionnaires. However, this tends to be very cost and time intensive. Thus, much work has been put into finding methods, which allow to reduce the involvement of human labour. In this survey, we present the main concepts and methods. For this, we differentiate between the various classes of dialogue systems (task-oriented dialogue systems, conversational dialogue systems, and question-answering dialogue systems). We cover each class by introducing the main technologies developed for the dialogue systems and then by presenting the evaluation methods regarding this class.},
  urldate = {2019-05-13},
  date = {2019-05-10},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Human-Computer Interaction},
  author = {Deriu, Jan and Rodrigo, Alvaro and Otegi, Arantxa and Echegoyen, Guillermo and Rosset, Sophie and Agirre, Eneko and Cieliebak, Mark},
  file = {/home/dimitri/Nextcloud/Zotero/storage/C3ELPHN2/Deriu et al. - 2019 - Survey on Evaluation Methods for Dialogue Systems.pdf;/home/dimitri/Nextcloud/Zotero/storage/EIMU5STH/1905.html}
}

@online{AINotesInitializing,
  title = {{{AI Notes}}: {{Initializing}} Neural Networks},
  url = {https://www.deeplearning.ai/ai-notes/initialization/},
  shorttitle = {{{AI Notes}}},
  abstract = {AI Notes: Initializing neural networks - deeplearning.ai},
  journaltitle = {deeplearning.ai},
  urldate = {2019-05-13},
  file = {/home/dimitri/Nextcloud/Zotero/storage/YS7UD2IB/initialization.html}
}

@article{serbanDeepReinforcementLearning2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1709.02349},
  primaryClass = {cs, stat},
  title = {A {{Deep Reinforcement Learning Chatbot}}},
  url = {http://arxiv.org/abs/1709.02349},
  abstract = {We present MILABOT: a deep reinforcement learning chatbot developed by the Montreal Institute for Learning Algorithms (MILA) for the Amazon Alexa Prize competition. MILABOT is capable of conversing with humans on popular small talk topics through both speech and text. The system consists of an ensemble of natural language generation and retrieval models, including template-based models, bag-of-words models, sequence-to-sequence neural network and latent variable neural network models. By applying reinforcement learning to crowdsourced data and real-world user interactions, the system has been trained to select an appropriate response from the models in its ensemble. The system has been evaluated through A/B testing with real-world users, where it performed significantly better than many competing systems. Due to its machine learning architecture, the system is likely to improve with additional data.},
  urldate = {2019-05-14},
  date = {2017-09-07},
  keywords = {Statistics - Machine Learning,Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,I.2.7,I.5.1},
  author = {Serban, Iulian V. and Sankar, Chinnadhurai and Germain, Mathieu and Zhang, Saizheng and Lin, Zhouhan and Subramanian, Sandeep and Kim, Taesup and Pieper, Michael and Chandar, Sarath and Ke, Nan Rosemary and Rajeshwar, Sai and family=Brebisson, given=Alexandre, prefix=de, useprefix=true and Sotelo, Jose M. R. and Suhubdy, Dendi and Michalski, Vincent and Nguyen, Alexandre and Pineau, Joelle and Bengio, Yoshua},
  file = {/home/dimitri/Nextcloud/Zotero/storage/TC9VIFMX/Serban et al. - 2017 - A Deep Reinforcement Learning Chatbot.pdf;/home/dimitri/Nextcloud/Zotero/storage/F6BFAJSK/1709.html}
}

@article{shenOrderedNeuronsIntegrating2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1810.09536},
  primaryClass = {cs},
  title = {Ordered {{Neurons}}: {{Integrating Tree Structures}} into {{Recurrent Neural Networks}}},
  url = {http://arxiv.org/abs/1810.09536},
  shorttitle = {Ordered {{Neurons}}},
  abstract = {Natural language is hierarchically structured: smaller units (e.g., phrases) are nested within larger units (e.g., clauses). When a larger constituent ends, all of the smaller constituents that are nested within it must also be closed. While the standard LSTM architecture allows different neurons to track information at different time scales, it does not have an explicit bias towards modeling a hierarchy of constituents. This paper proposes to add such an inductive bias by ordering the neurons; a vector of master input and forget gates ensures that when a given neuron is updated, all the neurons that follow it in the ordering are also updated. Our novel recurrent architecture, ordered neurons LSTM (ON-LSTM), achieves good performance on four different tasks: language modeling, unsupervised parsing, targeted syntactic evaluation, and logical inference.},
  urldate = {2019-05-14},
  date = {2018-10-22},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  author = {Shen, Yikang and Tan, Shawn and Sordoni, Alessandro and Courville, Aaron},
  file = {/home/dimitri/Nextcloud/Zotero/storage/NEEU5N2C/Shen et al. - 2018 - Ordered Neurons Integrating Tree Structures into .pdf;/home/dimitri/Nextcloud/Zotero/storage/SASXRX76/1810.html}
}

@article{kawaguchiGeneralizationDeepLearning2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1710.05468},
  primaryClass = {cs, stat},
  title = {Generalization in {{Deep Learning}}},
  url = {http://arxiv.org/abs/1710.05468},
  abstract = {This paper provides non-vacuous and numerically-tight generalization guarantees for deep learning, as well as theoretical insights into why and how deep learning can generalize well, despite its large capacity, complexity, possible algorithmic instability, nonrobustness, and sharp minima, responding to an open question in the literature. We also propose new open problems and discuss the limitations of our results.},
  urldate = {2019-05-14},
  date = {2017-10-15},
  keywords = {Statistics - Machine Learning,Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  author = {Kawaguchi, Kenji and Kaelbling, Leslie Pack and Bengio, Yoshua},
  file = {/home/dimitri/Nextcloud/Zotero/storage/IGRS7AC2/Kawaguchi et al. - 2017 - Generalization in Deep Learning.pdf;/home/dimitri/Nextcloud/Zotero/storage/TXV7FXKZ/1710.html}
}

@article{vayerOptimalTransportStructured2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1805.09114},
  primaryClass = {cs, stat},
  title = {Optimal {{Transport}} for Structured Data with Application on Graphs},
  url = {http://arxiv.org/abs/1805.09114},
  abstract = {This work considers the problem of computing distances between structured objects such as undirected graphs, seen as probability distributions in a specific metric space. We consider a new transportation distance (i.e. that minimizes a total cost of transporting probability masses) that unveils the geometric nature of the structured objects space. Unlike Wasserstein or Gromov-Wasserstein metrics that focus solely and respectively on features (by considering a metric in the feature space) or structure (by seeing structure as a metric space), our new distance exploits jointly both information, and is consequently called Fused Gromov-Wasserstein (FGW). After discussing its properties and computational aspects, we show results on a graph classification task, where our method outperforms both graph kernels and deep graph convolutional networks. Exploiting further on the metric properties of FGW, interesting geometric objects such as Fr\textbackslash{}'echet means or barycenters of graphs are illustrated and discussed in a clustering context.},
  urldate = {2019-05-14},
  date = {2018-05-23},
  keywords = {Statistics - Machine Learning,Computer Science - Machine Learning},
  author = {Vayer, Titouan and Chapel, Laetitia and Flamary, Rémi and Tavenard, Romain and Courty, Nicolas},
  file = {/home/dimitri/Nextcloud/Zotero/storage/8KCFRIEK/Vayer et al. - 2018 - Optimal Transport for structured data with applica.pdf;/home/dimitri/Nextcloud/Zotero/storage/HS3SLU6S/1805.html}
}

@article{frankleLotteryTicketHypothesis2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1803.03635},
  primaryClass = {cs},
  title = {The {{Lottery Ticket Hypothesis}}: {{Finding Sparse}}, {{Trainable Neural Networks}}},
  url = {http://arxiv.org/abs/1803.03635},
  shorttitle = {The {{Lottery Ticket Hypothesis}}},
  abstract = {Neural network pruning techniques can reduce the parameter counts of trained networks by over 90\%, decreasing storage requirements and improving computational performance of inference without compromising accuracy. However, contemporary experience is that the sparse architectures produced by pruning are difficult to train from the start, which would similarly improve training performance. We find that a standard pruning technique naturally uncovers subnetworks whose initializations made them capable of training effectively. Based on these results, we articulate the "lottery ticket hypothesis:" dense, randomly-initialized, feed-forward networks contain subnetworks ("winning tickets") that - when trained in isolation - reach test accuracy comparable to the original network in a similar number of iterations. The winning tickets we find have won the initialization lottery: their connections have initial weights that make training particularly effective. We present an algorithm to identify winning tickets and a series of experiments that support the lottery ticket hypothesis and the importance of these fortuitous initializations. We consistently find winning tickets that are less than 10-20\% of the size of several fully-connected and convolutional feed-forward architectures for MNIST and CIFAR10. Above this size, the winning tickets that we find learn faster than the original network and reach higher test accuracy.},
  urldate = {2019-05-15},
  date = {2018-03-09},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  author = {Frankle, Jonathan and Carbin, Michael},
  file = {/home/dimitri/Nextcloud/Zotero/storage/AK4564LE/Frankle and Carbin - 2018 - The Lottery Ticket Hypothesis Finding Sparse, Tra.pdf;/home/dimitri/Nextcloud/Zotero/storage/P7E9CAS7/1803.html}
}

@article{zhouDeconstructingLotteryTickets2019,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1905.01067},
  primaryClass = {cs, stat},
  title = {Deconstructing {{Lottery Tickets}}: {{Zeros}}, {{Signs}}, and the {{Supermask}}},
  url = {http://arxiv.org/abs/1905.01067},
  shorttitle = {Deconstructing {{Lottery Tickets}}},
  abstract = {The recent "Lottery Ticket Hypothesis" paper by Frankle \& Carbin showed that a simple approach to creating sparse networks (keep the large weights) results in models that are trainable from scratch, but only when starting from the same initial weights. The performance of these networks often exceeds the performance of the non-sparse base model, but for reasons that were not well understood. In this paper we study the three critical components of the Lottery Ticket (LT) algorithm, showing that each may be varied significantly without impacting the overall results. Ablating these factors leads to new insights for why LT networks perform as well as they do. We show why setting weights to zero is important, how signs are all you need to make the re-initialized network train, and why masking behaves like training. Finally, we discover the existence of Supermasks, or masks that can be applied to an untrained, randomly initialized network to produce a model with performance far better than chance (86\% on MNIST, 41\% on CIFAR-10).},
  urldate = {2019-05-15},
  date = {2019-05-03},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Statistics - Machine Learning,Computer Science - Machine Learning},
  author = {Zhou, Hattie and Lan, Janice and Liu, Rosanne and Yosinski, Jason},
  file = {/home/dimitri/Nextcloud/Zotero/storage/8FFPAI2U/Zhou et al. - 2019 - Deconstructing Lottery Tickets Zeros, Signs, and .pdf;/home/dimitri/Nextcloud/Zotero/storage/RFK86THM/1905.html}
}

@article{hesterDeepQlearningDemonstrations2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1704.03732},
  primaryClass = {cs},
  title = {Deep {{Q}}-Learning from {{Demonstrations}}},
  url = {http://arxiv.org/abs/1704.03732},
  abstract = {Deep reinforcement learning (RL) has achieved several high profile successes in difficult decision-making problems. However, these algorithms typically require a huge amount of data before they reach reasonable performance. In fact, their performance during learning can be extremely poor. This may be acceptable for a simulator, but it severely limits the applicability of deep RL to many real-world tasks, where the agent must learn in the real environment. In this paper we study a setting where the agent may access data from previous control of the system. We present an algorithm, Deep Q-learning from Demonstrations (DQfD), that leverages small sets of demonstration data to massively accelerate the learning process even from relatively small amounts of demonstration data and is able to automatically assess the necessary ratio of demonstration data while learning thanks to a prioritized replay mechanism. DQfD works by combining temporal difference updates with supervised classification of the demonstrator's actions. We show that DQfD has better initial performance than Prioritized Dueling Double Deep Q-Networks (PDD DQN) as it starts with better scores on the first million steps on 41 of 42 games and on average it takes PDD DQN 83 million steps to catch up to DQfD's performance. DQfD learns to out-perform the best demonstration given in 14 of 42 games. In addition, DQfD leverages human demonstrations to achieve state-of-the-art results for 11 games. Finally, we show that DQfD performs better than three related algorithms for incorporating demonstration data into DQN.},
  urldate = {2019-05-16},
  date = {2017-04-12},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  author = {Hester, Todd and Vecerik, Matej and Pietquin, Olivier and Lanctot, Marc and Schaul, Tom and Piot, Bilal and Horgan, Dan and Quan, John and Sendonaris, Andrew and Dulac-Arnold, Gabriel and Osband, Ian and Agapiou, John and Leibo, Joel Z. and Gruslys, Audrunas},
  file = {/home/dimitri/Nextcloud/Zotero/storage/77YC9ZHU/Hester et al. - 2017 - Deep Q-learning from Demonstrations.pdf;/home/dimitri/Nextcloud/Zotero/storage/M7LNGZEI/1704.html}
}

@article{fanHierarchicalNeuralStory2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1805.04833},
  primaryClass = {cs},
  title = {Hierarchical {{Neural Story Generation}}},
  url = {http://arxiv.org/abs/1805.04833},
  abstract = {We explore story generation: creative systems that can build coherent and fluent passages of text about a topic. We collect a large dataset of 300K human-written stories paired with writing prompts from an online forum. Our dataset enables hierarchical story generation, where the model first generates a premise, and then transforms it into a passage of text. We gain further improvements with a novel form of model fusion that improves the relevance of the story to the prompt, and adding a new gated multi-scale self-attention mechanism to model long-range context. Experiments show large improvements over strong baselines on both automated and human evaluations. Human judges prefer stories generated by our approach to those from a strong non-hierarchical model by a factor of two to one.},
  urldate = {2019-05-16},
  date = {2018-05-13},
  keywords = {Computer Science - Computation and Language},
  author = {Fan, Angela and Lewis, Mike and Dauphin, Yann},
  file = {/home/dimitri/Nextcloud/Zotero/storage/3DCR53RA/Fan et al. - 2018 - Hierarchical Neural Story Generation.pdf;/home/dimitri/Nextcloud/Zotero/storage/MKM4L4SD/1805.html}
}

@article{mnihHumanlevelControlDeep2015,
  langid = {english},
  title = {Human-Level Control through Deep Reinforcement Learning},
  volume = {518},
  issn = {0028-0836, 1476-4687},
  url = {http://www.nature.com/articles/nature14236},
  doi = {10.1038/nature14236},
  number = {7540},
  journaltitle = {Nature},
  shortjournal = {Nature},
  urldate = {2019-05-17},
  date = {2015-02},
  pages = {529-533},
  author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A. and Veness, Joel and Bellemare, Marc G. and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K. and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
  file = {/home/dimitri/Nextcloud/Zotero/storage/PN6LQZVH/Mnih et al. - 2015 - Human-level control through deep reinforcement lea.pdf}
}

@article{silverMasteringGameGo2016,
  langid = {english},
  title = {Mastering the Game of {{Go}} with Deep Neural Networks and Tree Search},
  volume = {529},
  issn = {0028-0836, 1476-4687},
  url = {http://www.nature.com/articles/nature16961},
  doi = {10.1038/nature16961},
  number = {7587},
  journaltitle = {Nature},
  shortjournal = {Nature},
  urldate = {2019-05-17},
  date = {2016-01},
  pages = {484-489},
  author = {Silver, David and Huang, Aja and Maddison, Chris J. and Guez, Arthur and Sifre, Laurent and family=Driessche, given=George, prefix=van den, useprefix=true and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and Dieleman, Sander and Grewe, Dominik and Nham, John and Kalchbrenner, Nal and Sutskever, Ilya and Lillicrap, Timothy and Leach, Madeleine and Kavukcuoglu, Koray and Graepel, Thore and Hassabis, Demis},
  file = {/home/dimitri/Nextcloud/Zotero/storage/EE74MJVF/Silver et al. - 2016 - Mastering the game of Go with deep neural networks.pdf}
}

@incollection{cohen-addadHierarchicalClusteringObjective2018,
  title = {Hierarchical {{Clustering}}: {{Objective Functions}} and {{Algorithms}}},
  url = {https://epubs.siam.org/doi/10.1137/1.9781611975031.26},
  shorttitle = {Hierarchical {{Clustering}}},
  abstract = {Hierarchical clustering is a recursive partitioning of a dataset into clusters at an increasingly finer granularity. Motivated by the fact that most work on hierarchical clustering was based on providing algorithms, rather than optimizing a specific objective, [19] framed similarity-based hierarchical clustering as a combinatorial optimization problem, where a ‘good’ hierarchical clustering is one that minimizes some cost function. He showed that this cost function has certain desirable properties, such as in order to achieve optimal cost, disconnected components must be separated first and that in ‘structureless’ graphs, i.e., cliques, all clusterings achieve the same cost. We take an axiomatic approach to defining ‘good’ objective functions for both similarity and dissimilarity-based hierarchical clustering. We characterize a set of admissible objective functions (that includes the one introduced by Dasgupta) that have the property that when the input admits a ‘natural’ ground-truth hierarchical clustering, the ground-truth clustering has an optimal value. Equipped with a suitable objective function, we analyze the performance of practical algorithms, as well as develop better and faster algorithms for hierarchical clustering. For similarity-based hierarchical clustering, [19] showed that a simple recursive sparsest-cut based approach achieves an O(log3/2 n)-approximation on worst-case inputs. We give a more refined analysis of the algorithm and show that it in fact achieves an -approximation1. This improves upon the LP-based O(log n)-approximation of [33]. For dissimilarity-based hierarchical clustering, we show that the classic average-linkage algorithm gives a factor 2 approximation, and provide a simple and better algorithm that gives a factor 3/2 approximation. This aims at explaining the success of these heuristics in practice. Finally, we consider a ‘beyond-worst-case’ scenario through a generalisation of the stochastic block model for hierarchical clustering. We show that Dasgupta's cost function also has desirable properties for these inputs and we provide a simple algorithm that for graphs generated according to this model yields a 1 + o(1) factor approximation.},
  volumes = {0},
  booktitle = {Proceedings of the {{Twenty}}-{{Ninth Annual ACM}}-{{SIAM Symposium}} on {{Discrete Algorithms}}},
  series = {Proceedings},
  publisher = {{Society for Industrial and Applied Mathematics}},
  urldate = {2019-05-17},
  date = {2018-01-01},
  pages = {378-397},
  author = {Cohen-Addad, V. and Kanade, V. and Mallmann-Trenn, F. and Mathieu, C.},
  file = {/home/dimitri/Nextcloud/Zotero/storage/VHDL9VUB/Cohen-Addad et al. - 2018 - Hierarchical Clustering Objective Functions and A.pdf;/home/dimitri/Nextcloud/Zotero/storage/EJTTUVNP/1.9781611975031.html},
  doi = {10.1137/1.9781611975031.26}
}

@inproceedings{thomasHighConfidenceOffpolicy2015,
  title = {High {{Confidence Off}}-Policy {{Evaluation}}},
  isbn = {978-0-262-51129-2},
  url = {https://www.aaai.org/ocs/index.php/AAAI/AAAI15/paper/viewPaper/10042},
  abstract = {Many reinforcement learning algorithms use trajectories collected from the execution of one or more policies to propose a new policy. Because execution of a bad policy can be costly or dangerous, techniques for evaluating the performance of the new policy without requiring its execution have been of recent interest in industry. Such off-policy evaluation methods, which estimate the performance of a policy using trajectories collected from the execution of other policies, heretofore have not provided confidences regarding the accuracy of their estimates. In this paper we propose an off-policy method for computing a lower confidence bound on the expected return of a policy.},
  booktitle = {Proceedings of the {{Twenty}}-{{Ninth AAAI Conference}} on {{Artificial Intelligence}}},
  series = {{{AAAI}}'15},
  publisher = {{AAAI Press}},
  urldate = {2019-05-17},
  date = {2015},
  pages = {3000--3006},
  author = {Thomas, Philip S. and Theocharous, Georgios and Ghavamzadeh, Mohammad},
  file = {/home/dimitri/Nextcloud/Zotero/storage/DSD86HE9/Thomas et al. - 2015 - High Confidence Off-policy Evaluation.pdf},
  venue = {Austin, Texas}
}

@inproceedings{thomasDataEfficientOffPolicyPolicy2016,
  langid = {english},
  title = {Data-{{Efficient Off}}-{{Policy Policy Evaluation}} for {{Reinforcement Learning}}},
  url = {http://proceedings.mlr.press/v48/thomasa16.html},
  abstract = {In this paper we present a new way of predicting the performance of a reinforcement learning policy given historical data that may have been generated by a different policy. The ability to evaluate...},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  booktitle = {International {{Conference}} on {{Machine Learning}}},
  urldate = {2019-05-17},
  date = {2016-06-11},
  pages = {2139-2148},
  author = {Thomas, Philip and Brunskill, Emma},
  file = {/home/dimitri/Nextcloud/Zotero/storage/KRD885VU/Thomas and Brunskill - 2016 - Data-Efficient Off-Policy Policy Evaluation for Re.pdf;/home/dimitri/Nextcloud/Zotero/storage/X2R64R83/Appendix.pdf;/home/dimitri/Nextcloud/Zotero/storage/YPJPLZJ6/thomasa16.html}
}

@inproceedings{farajtabarMoreRobustDoubly2018,
  langid = {english},
  title = {More {{Robust Doubly Robust Off}}-Policy {{Evaluation}}},
  url = {http://proceedings.mlr.press/v80/farajtabar18a.html},
  abstract = {We study the problem of off-policy evaluation (OPE) in reinforcement learning (RL), where the goal is to estimate the performance of a policy from the data generated by another policy(ies). In part...},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  booktitle = {International {{Conference}} on {{Machine Learning}}},
  urldate = {2019-05-17},
  date = {2018-07-03},
  pages = {1447-1456},
  author = {Farajtabar, Mehrdad and Chow, Yinlam and Ghavamzadeh, Mohammad},
  file = {/home/dimitri/Nextcloud/Zotero/storage/NJK9Q7YI/Farajtabar et al. - 2018 - More Robust Doubly Robust Off-policy Evaluation.pdf;/home/dimitri/Nextcloud/Zotero/storage/XYSG7MCZ/farajtabar18a.html}
}

@inproceedings{jiangDoublyRobustOffpolicy2016,
  langid = {english},
  title = {Doubly {{Robust Off}}-Policy {{Value Evaluation}} for {{Reinforcement Learning}}},
  url = {http://proceedings.mlr.press/v48/jiang16.html},
  abstract = {We study the problem of off-policy value evaluation in reinforcement learning (RL), where one aims to estimate the value of a new policy based on data collected by a different policy. This problem ...},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  booktitle = {International {{Conference}} on {{Machine Learning}}},
  urldate = {2019-05-17},
  date = {2016-06-11},
  pages = {652-661},
  author = {Jiang, Nan and Li, Lihong},
  file = {/home/dimitri/Nextcloud/Zotero/storage/6B6SSYEQ/jiang16-supp.pdf;/home/dimitri/Nextcloud/Zotero/storage/IBUZU3LH/Jiang and Li - 2016 - Doubly Robust Off-policy Value Evaluation for Rein.pdf;/home/dimitri/Nextcloud/Zotero/storage/XRBVCV63/jiang16.html}
}

@article{mazarDishonestyEverydayLife2006,
  langid = {english},
  title = {Dishonesty in {{Everyday Life}} and {{Its Policy Implications}}},
  volume = {25},
  issn = {0748-6766, 1547-7207},
  url = {http://journals.sagepub.com/doi/10.1509/jppm.25.1.117},
  doi = {10.1509/jppm.25.1.117},
  number = {1},
  journaltitle = {Journal of Public Policy \& Marketing},
  shortjournal = {Journal of Public Policy \& Marketing},
  urldate = {2019-05-17},
  date = {2006-04},
  pages = {117-126},
  author = {Mazar, Nina and Ariely, Dan},
  file = {/home/dimitri/Nextcloud/Zotero/storage/6NM8HENU/Mazar_Ariely_2006_Dishonesty in Everyday Life and Its Policy Implications.pdf}
}

@article{nagelWhatItBe1974,
  title = {What {{Is It Like}} to {{Be}} a {{Bat}}?},
  volume = {83},
  issn = {00318108},
  url = {https://www.jstor.org/stable/2183914?origin=crossref},
  doi = {10.2307/2183914},
  number = {4},
  journaltitle = {The Philosophical Review},
  shortjournal = {The Philosophical Review},
  urldate = {2019-05-17},
  date = {1974-10},
  pages = {435},
  author = {Nagel, Thomas},
  file = {/home/dimitri/Nextcloud/Zotero/storage/93ZKZZFZ/Nagel_1974_What Is It Like to Be a Bat.pdf}
}

@article{kaufmannImaginationArtificielle1969,
  langid = {french},
  title = {L'imagination artificielle},
  volume = {3},
  issn = {0399-0559},
  url = {https://eudml.org/doc/104480},
  number = {V3},
  journaltitle = {RAIRO - Operations Research - Recherche Opérationnelle},
  urldate = {2019-05-20},
  date = {1969},
  pages = {5-24},
  author = {Kaufmann, A.},
  file = {/home/dimitri/Nextcloud/Zotero/storage/GG5TMAJJ/Kaufmann - 1969 - L'imagination artificielle.pdf;/home/dimitri/Nextcloud/Zotero/storage/INXU95SE/104480.html}
}

@article{swingerWhatAreBiases2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1812.08769},
  primaryClass = {cs},
  title = {What Are the Biases in My Word Embedding?},
  url = {http://arxiv.org/abs/1812.08769},
  abstract = {This paper presents an algorithm for enumerating biases in word embeddings. The algorithm exposes a large number of offensive associations related to sensitive features such as race and gender on publicly available embeddings, including a supposedly "debiased" embedding. These biases are concerning in light of the widespread use of word embeddings. The associations are identified by geometric patterns in word embeddings that run parallel between people's names and common lower-case tokens. The algorithm is highly unsupervised: it does not even require the sensitive features to be pre-specified. This is desirable because: (a) many forms of discrimination--such as racial discrimination--are linked to social constructs that may vary depending on the context, rather than to categories with fixed definitions; and (b) it makes it easier to identify biases against intersectional groups, which depend on combinations of sensitive features. The inputs to our algorithm are a list of target tokens, e.g. names, and a word embedding. It outputs a number of Word Embedding Association Tests (WEATs) that capture various biases present in the data. We illustrate the utility of our approach on publicly available word embeddings and lists of names, and evaluate its output using crowdsourcing. We also show how removing names may not remove potential proxy bias.},
  urldate = {2019-05-21},
  date = {2018-12-20},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  author = {Swinger, Nathaniel and De-Arteaga, Maria and Heffernan IV, Neil Thomas and Leiserson, Mark DM and Kalai, Adam Tauman},
  file = {/home/dimitri/Nextcloud/Zotero/storage/48KWSJ5B/Swinger et al. - 2018 - What are the biases in my word embedding.pdf;/home/dimitri/Nextcloud/Zotero/storage/GLJK7K6P/1812.html}
}

@article{goldtStochasticThermodynamicsLearning2017,
  langid = {english},
  title = {Stochastic {{Thermodynamics}} of {{Learning}}},
  volume = {118},
  issn = {0031-9007, 1079-7114},
  url = {https://link.aps.org/doi/10.1103/PhysRevLett.118.010601},
  doi = {10.1103/PhysRevLett.118.010601},
  number = {1},
  journaltitle = {Physical Review Letters},
  shortjournal = {Phys. Rev. Lett.},
  urldate = {2019-05-22},
  date = {2017-01-06},
  pages = {010601},
  author = {Goldt, Sebastian and Seifert, Udo},
  file = {/home/dimitri/Nextcloud/Zotero/storage/VZIXU9EB/goldt2017.pdf;/home/dimitri/Nextcloud/Zotero/storage/ZS4FRRGR/Goldt and Seifert - 2017 - Stochastic Thermodynamics of Learning.pdf}
}

@article{alemiTherMLThermodynamicsMachine2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1807.04162},
  primaryClass = {cond-mat, stat},
  title = {{{TherML}}: {{Thermodynamics}} of {{Machine Learning}}},
  url = {http://arxiv.org/abs/1807.04162},
  shorttitle = {{{TherML}}},
  abstract = {In this work we offer a framework for reasoning about a wide class of existing objectives in machine learning. We develop a formal correspondence between this work and thermodynamics and discuss its implications.},
  urldate = {2019-05-22},
  date = {2018-07-11},
  keywords = {Statistics - Machine Learning,Condensed Matter - Statistical Mechanics,Computer Science - Machine Learning},
  author = {Alemi, Alexander A. and Fischer, Ian},
  file = {/home/dimitri/Nextcloud/Zotero/storage/XQJKMW9R/Alemi and Fischer - 2018 - TherML Thermodynamics of Machine Learning.pdf;/home/dimitri/Nextcloud/Zotero/storage/4B76X7GC/1807.html}
}

@article{lloydComplexityThermodynamicDepth1988,
  langid = {english},
  title = {Complexity as Thermodynamic Depth},
  volume = {188},
  issn = {00034916},
  url = {https://linkinghub.elsevier.com/retrieve/pii/0003491688900942},
  doi = {10.1016/0003-4916(88)90094-2},
  number = {1},
  journaltitle = {Annals of Physics},
  shortjournal = {Annals of Physics},
  urldate = {2019-05-22},
  date = {1988-11},
  pages = {186-213},
  author = {Lloyd, Seth and Pagels, Heinz},
  file = {/home/dimitri/Nextcloud/Zotero/storage/QRIK24NG/Lloyd and Pagels - 1988 - Complexity as thermodynamic depth.pdf}
}

@article{graysonIntroductionUnivalentFoundations2018,
  langid = {english},
  title = {An Introduction to Univalent Foundations for Mathematicians},
  volume = {55},
  issn = {0273-0979, 1088-9485},
  url = {http://www.ams.org/bull/2018-55-04/S0273-0979-2018-01616-9/},
  doi = {10.1090/bull/1616},
  number = {4},
  journaltitle = {Bulletin of the American Mathematical Society},
  shortjournal = {Bull. Amer. Math. Soc.},
  urldate = {2019-05-23},
  date = {2018-03-05},
  pages = {427-450},
  author = {Grayson, Daniel R.},
  file = {/home/dimitri/Nextcloud/Zotero/storage/T6445HRJ/Grayson - 2018 - An introduction to univalent foundations for mathe.pdf}
}

@article{wietingNoTrainingRequired2019a,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1901.10444},
  primaryClass = {cs},
  title = {No {{Training Required}}: {{Exploring Random Encoders}} for {{Sentence Classification}}},
  url = {http://arxiv.org/abs/1901.10444},
  shorttitle = {No {{Training Required}}},
  abstract = {We explore various methods for computing sentence representations from pre-trained word embeddings without any training, i.e., using nothing but random parameterizations. Our aim is to put sentence embeddings on more solid footing by 1) looking at how much modern sentence embeddings gain over random methods---as it turns out, surprisingly little; and by 2) providing the field with more appropriate baselines going forward---which are, as it turns out, quite strong. We also make important observations about proper experimental protocol for sentence classification evaluation, together with recommendations for future research.},
  urldate = {2019-05-24},
  date = {2019-01-29},
  keywords = {Computer Science - Computation and Language},
  author = {Wieting, John and Kiela, Douwe},
  file = {/home/dimitri/Nextcloud/Zotero/storage/PSEYU45U/Wieting and Kiela - 2019 - No Training Required Exploring Random Encoders fo.pdf;/home/dimitri/Nextcloud/Zotero/storage/2CP6PMIW/1901.html}
}

@article{saxeMathematicalTheorySemantic2019,
  langid = {english},
  title = {A Mathematical Theory of Semantic Development in Deep Neural Networks},
  issn = {0027-8424, 1091-6490},
  url = {http://www.pnas.org/lookup/doi/10.1073/pnas.1820226116},
  doi = {10.1073/pnas.1820226116},
  abstract = {An extensive body of empirical research has revealed remarkable regularities in the acquisition, organization, deployment, and neural representation of human semantic knowledge, thereby raising a fundamental conceptual question: What are the theoretical principles governing the ability of neural networks to acquire, organize, and deploy abstract knowledge by integrating across many individual experiences? We address this question by mathematically analyzing the nonlinear dynamics of learning in deep linear networks. We find exact solutions to this learning dynamics that yield a conceptual explanation for the prevalence of many disparate phenomena in semantic cognition, including the hierarchical differentiation of concepts through rapid developmental transitions, the ubiquity of semantic illusions between such transitions, the emergence of item typicality and category coherence as factors controlling the speed of semantic processing, changing patterns of inductive projection over development, and the conservation of semantic similarity in neural representations across species. Thus, surprisingly, our simple neural model qualitatively recapitulates many diverse regularities underlying semantic development, while providing analytic insight into how the statistical structure of an environment can interact with nonlinear deep-learning dynamics to give rise to these regularities.},
  journaltitle = {Proceedings of the National Academy of Sciences},
  shortjournal = {Proc Natl Acad Sci USA},
  urldate = {2019-05-24},
  date = {2019-05-17},
  pages = {201820226},
  author = {Saxe, Andrew M. and McClelland, James L. and Ganguli, Surya},
  file = {/home/dimitri/Nextcloud/Zotero/storage/QU842KYR/Saxe et al. - 2019 - A mathematical theory of semantic development in d.pdf}
}

@article{ben-davidLearnabilityCanBe2019,
  langid = {english},
  title = {Learnability Can Be Undecidable},
  volume = {1},
  issn = {2522-5839},
  url = {https://www.nature.com/articles/s42256-018-0002-3},
  doi = {10.1038/s42256-018-0002-3},
  abstract = {Not all mathematical questions can be resolved, according to Gödel’s famous incompleteness theorems. It turns out that machine learning can be vulnerable to undecidability too, as is illustrated with an example problem where learnability cannot be proved nor refuted.},
  number = {1},
  journaltitle = {Nature Machine Intelligence},
  urldate = {2019-05-24},
  date = {2019-01},
  pages = {44},
  author = {Ben-David, Shai and Hrubeš, Pavel and Moran, Shay and Shpilka, Amir and Yehudayoff, Amir},
  file = {/home/dimitri/Nextcloud/Zotero/storage/VD6BTBEG/Ben-David et al. - 2019 - Learnability can be undecidable.pdf;/home/dimitri/Nextcloud/Zotero/storage/K9RU5GU4/s42256-018-0002-3.html}
}

@book{theunivalentfoundationsprogramHomotopyTypeTheory2013,
  location = {{Institute for Advanced Study}},
  title = {Homotopy {{Type Theory}}: {{Univalent Foundations}} of {{Mathematics}}},
  url = {https://homotopytypetheory.org/book/},
  shorttitle = {Homotopy {{Type Theory}}},
  date = {2013},
  author = {The Univalent Foundations Program},
  note = {Open Library ID: OL25428110M}
}

@article{vayerSlicedGromovWasserstein2019,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1905.10124},
  primaryClass = {cs, stat},
  title = {Sliced {{Gromov}}-{{Wasserstein}}},
  url = {http://arxiv.org/abs/1905.10124},
  abstract = {Recently used in various machine learning contexts, the Gromov-Wasserstein distance (GW) allows for comparing distributions that do not necessarily lie in the same metric space. However, this Optimal Transport (OT) distance requires solving a complex non convex quadratic program which is most of the time very costly both in time and memory. Contrary to GW, the Wasserstein distance (W) enjoys several properties (e.g. duality) that permit large scale optimization. Among those, the Sliced Wasserstein (SW) distance exploits the direct solution of W on the line, that only requires sorting discrete samples in 1D. This paper propose a new divergence based on GW akin to SW. We first derive a closed form for GW when dealing with 1D distributions, based on a new result for the related quadratic assignment problem. We then define a novel OT discrepancy that can deal with large scale distributions via a slicing approach and we show how it relates to the GW distance while being \$O(n\^2)\$ to compute. We illustrate the behavior of this so called Sliced Gromov-Wasserstein (SGW) discrepancy in experiments where we demonstrate its ability to tackle similar problems as GW while being several order of magnitudes faster to compute},
  urldate = {2019-05-29},
  date = {2019-05-24},
  keywords = {Statistics - Machine Learning,Computer Science - Machine Learning},
  author = {Vayer, Titouan and Flamary, Rémi and Tavenard, Romain and Chapel, Laetitia and Courty, Nicolas},
  file = {/home/dimitri/Nextcloud/Zotero/storage/8ZGBPMIP/Vayer et al. - 2019 - Sliced Gromov-Wasserstein.pdf;/home/dimitri/Nextcloud/Zotero/storage/EIJKFLT5/1905.html}
}

@article{zhangERNIEEnhancedLanguage2019,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1905.07129},
  primaryClass = {cs},
  title = {{{ERNIE}}: {{Enhanced Language Representation}} with {{Informative Entities}}},
  url = {http://arxiv.org/abs/1905.07129},
  shorttitle = {{{ERNIE}}},
  abstract = {Neural language representation models such as BERT pre-trained on large-scale corpora can well capture rich semantic patterns from plain text, and be fine-tuned to consistently improve the performance of various NLP tasks. However, the existing pre-trained language models rarely consider incorporating knowledge graphs (KGs), which can provide rich structured knowledge facts for better language understanding. We argue that informative entities in KGs can enhance language representation with external knowledge. In this paper, we utilize both large-scale textual corpora and KGs to train an enhanced language representation model (ERNIE), which can take full advantage of lexical, syntactic, and knowledge information simultaneously. The experimental results have demonstrated that ERNIE achieves significant improvements on various knowledge-driven tasks, and meanwhile is comparable with the state-of-the-art model BERT on other common NLP tasks. The source code of this paper can be obtained from https://github.com/thunlp/ERNIE.},
  urldate = {2019-05-29},
  date = {2019-05-17},
  keywords = {Computer Science - Computation and Language},
  author = {Zhang, Zhengyan and Han, Xu and Liu, Zhiyuan and Jiang, Xin and Sun, Maosong and Liu, Qun},
  file = {/home/dimitri/Nextcloud/Zotero/storage/9IB4FYL3/Zhang et al. - 2019 - ERNIE Enhanced Language Representation with Infor.pdf;/home/dimitri/Nextcloud/Zotero/storage/5UXV3APF/1905.html}
}

@book{ambrosioGradientFlowsMetric2008,
  langid = {english},
  location = {{Basel}},
  title = {Gradient Flows in Metric Spaces and in the Space of Probability Measures},
  edition = {2. ed},
  isbn = {978-3-7643-8722-8 978-3-7643-8721-1},
  pagetotal = {334},
  series = {Lectures in Mathematics {{ETH Zürich}}},
  publisher = {{Birkhäuser}},
  date = {2008},
  author = {Ambrosio, Luigi and Gigli, Nicola and Savaré, Giuseppe},
  file = {/home/dimitri/Nextcloud/Zotero/storage/AJXKRTAG/gradient-flows-2008.pdf},
  note = {OCLC: 254181287}
}

@article{dyerRecurrentNeuralNetwork2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1602.07776},
  primaryClass = {cs},
  title = {Recurrent {{Neural Network Grammars}}},
  url = {http://arxiv.org/abs/1602.07776},
  abstract = {We introduce recurrent neural network grammars, probabilistic models of sentences with explicit phrase structure. We explain efficient inference procedures that allow application to both parsing and language modeling. Experiments show that they provide better parsing in English than any single previously published supervised generative model and better language modeling than state-of-the-art sequential RNNs in English and Chinese.},
  urldate = {2019-06-03},
  date = {2016-02-24},
  keywords = {Computer Science - Computation and Language,Computer Science - Neural and Evolutionary Computing},
  author = {Dyer, Chris and Kuncoro, Adhiguna and Ballesteros, Miguel and Smith, Noah A.},
  file = {/home/dimitri/Nextcloud/Zotero/storage/WQVMS2ZL/Dyer et al. - 2016 - Recurrent Neural Network Grammars.pdf;/home/dimitri/Nextcloud/Zotero/storage/G457GBIL/1602.html}
}

@article{bruel-gabrielssonTopologyLayerMachine2019,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1905.12200},
  primaryClass = {cs, stat},
  title = {A {{Topology Layer}} for {{Machine Learning}}},
  url = {http://arxiv.org/abs/1905.12200},
  abstract = {Topology applied to real world data using persistent homology has started to find applications within machine learning, including deep learning. We present a differentiable topology layer that computes persistent homology based on level set filtrations and distance-bases filtrations. We present three novel applications: the topological layer can (i) serve as a regularizer directly on data or the weights of machine learning models, (ii) construct a loss on the output of a deep generative network to incorporate topological priors, and (iii) perform topological adversarial attacks on deep networks trained with persistence features. The code is publicly available and we hope its availability will facilitate the use of persistent homology in deep learning and other gradient based applications.},
  urldate = {2019-06-03},
  date = {2019-05-28},
  keywords = {Statistics - Machine Learning,Computer Science - Machine Learning},
  author = {Brüel-Gabrielsson, Rickard and Nelson, Bradley J. and Dwaraknath, Anjan and Skraba, Primoz and Guibas, Leonidas J. and Carlsson, Gunnar},
  file = {/home/dimitri/Nextcloud/Zotero/storage/M2QTLMQG/Brüel-Gabrielsson et al. - 2019 - A Topology Layer for Machine Learning.pdf;/home/dimitri/Nextcloud/Zotero/storage/S6DIBJTQ/1905.html}
}

@article{tangTargetGuidedOpenDomainConversation2019,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1905.11553},
  primaryClass = {cs},
  title = {Target-{{Guided Open}}-{{Domain Conversation}}},
  url = {http://arxiv.org/abs/1905.11553},
  abstract = {Many real-world open-domain conversation applications have specific goals to achieve during open-ended chats, such as recommendation, psychotherapy, education, etc. We study the problem of imposing conversational goals on open-domain chat agents. In particular, we want a conversational system to chat naturally with human and proactively guide the conversation to a designated target subject. The problem is challenging as no public data is available for learning such a target-guided strategy. We propose a structured approach that introduces coarse-grained keywords to control the intended content of system responses. We then attain smooth conversation transition through turn-level supervised learning, and drive the conversation towards the target with discourse-level constraints. We further derive a keyword-augmented conversation dataset for the study. Quantitative and human evaluations show our system can produce meaningful and effective conversations, significantly improving over other approaches.},
  urldate = {2019-06-03},
  date = {2019-05-27},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  author = {Tang, Jianheng and Zhao, Tiancheng and Xiong, Chenyan and Liang, Xiaodan and Xing, Eric P. and Hu, Zhiting},
  file = {/home/dimitri/Nextcloud/Zotero/storage/IL2RB6MX/Tang et al. - 2019 - Target-Guided Open-Domain Conversation.pdf;/home/dimitri/Nextcloud/Zotero/storage/LUZRZYFE/1905.html}
}

@article{jaderbergHumanlevelPerformance3D2019,
  langid = {english},
  title = {Human-Level Performance in {{3D}} Multiplayer Games with Population-Based Reinforcement Learning},
  volume = {364},
  issn = {0036-8075, 1095-9203},
  url = {https://science.sciencemag.org/content/364/6443/859},
  doi = {10.1126/science.aau6249},
  abstract = {Artificial teamwork
Artificially intelligent agents are getting better and better at two-player games, but most real-world endeavors require teamwork. Jaderberg et al. designed a computer program that excels at playing the video game Quake III Arena in Capture the Flag mode, where two multiplayer teams compete in capturing the flags of the opposing team. The agents were trained by playing thousands of games, gradually learning successful strategies not unlike those favored by their human counterparts. Computer agents competed successfully against humans even when their reaction times were slowed to match those of humans.
Science, this issue p. 859
Reinforcement learning (RL) has shown great success in increasingly complex single-agent environments and two-player turn-based games. However, the real world contains multiple agents, each learning and acting independently to cooperate and compete with other agents. We used a tournament-style evaluation to demonstrate that an agent can achieve human-level performance in a three-dimensional multiplayer first-person video game, Quake III Arena in Capture the Flag mode, using only pixels and game points scored as input. We used a two-tier optimization process in which a population of independent RL agents are trained concurrently from thousands of parallel matches on randomly generated environments. Each agent learns its own internal reward signal and rich representation of the world. These results indicate the great potential of multiagent reinforcement learning for artificial intelligence research.
Teams of artificial agents compete successfully against humans in the video game Quake III Arena in Capture the Flag mode.
Teams of artificial agents compete successfully against humans in the video game Quake III Arena in Capture the Flag mode.},
  number = {6443},
  journaltitle = {Science},
  urldate = {2019-06-03},
  date = {2019-05-31},
  pages = {859-865},
  author = {Jaderberg, Max and Czarnecki, Wojciech M. and Dunning, Iain and Marris, Luke and Lever, Guy and Castañeda, Antonio Garcia and Beattie, Charles and Rabinowitz, Neil C. and Morcos, Ari S. and Ruderman, Avraham and Sonnerat, Nicolas and Green, Tim and Deason, Louise and Leibo, Joel Z. and Silver, David and Hassabis, Demis and Kavukcuoglu, Koray and Graepel, Thore},
  file = {/home/dimitri/Nextcloud/Zotero/storage/BKW8SC9N/Jaderberg et al. - 2019 - Human-level performance in 3D multiplayer games wi.pdf;/home/dimitri/Nextcloud/Zotero/storage/PHLALIVP/859.html},
  eprinttype = {pmid},
  eprint = {31147514}
}

@article{leinsterRethinkingSetTheory2012,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1212.6543},
  primaryClass = {math},
  title = {Rethinking Set Theory},
  url = {http://arxiv.org/abs/1212.6543},
  abstract = {Mathematicians manipulate sets with confidence almost every day, rarely making mistakes. Few of us, however, could accurately quote what are often referred to as "the" axioms of set theory. This suggests that we all carry around with us, perhaps subconsciously, a reliable body of operating principles for manipulating sets. What if we were to take some of those principles and adopt them as our axioms instead? The message of this article is that this can be done, in a simple, practical way (due to Lawvere). The resulting axioms are ten thoroughly mundane statements about sets. This is an expository article for a general mathematical readership.},
  urldate = {2019-06-04},
  date = {2012-12-28},
  keywords = {Mathematics - Category Theory,Mathematics - Logic},
  author = {Leinster, Tom},
  file = {/home/dimitri/Nextcloud/Zotero/storage/9AIKI668/Leinster - 2012 - Rethinking set theory.pdf;/home/dimitri/Nextcloud/Zotero/storage/SBVQGL23/1212.html}
}

@article{carraraBudgetedReinforcementLearning2019,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1903.01004},
  primaryClass = {cs, stat},
  title = {Budgeted {{Reinforcement Learning}} in {{Continuous State Space}}},
  url = {http://arxiv.org/abs/1903.01004},
  abstract = {A Budgeted Markov Decision Process (BMDP) is an extension of a Markov Decision Process to critical applications requiring safety constraints. It relies on a notion of risk implemented in the shape of a cost signal constrained to lie below an - adjustable - threshold. So far, BMDPs could only be solved in the case of finite state spaces with known dynamics. This work extends the state-of-the-art to continuous spaces environments and unknown dynamics. We show that the solution to a BMDP is a fixed point of a novel Budgeted Bellman Optimality operator. This observation allows us to introduce natural extensions of Deep Reinforcement Learning algorithms to address large-scale BMDPs. We validate our approach on two simulated applications: spoken dialogue and autonomous driving.},
  urldate = {2019-06-05},
  date = {2019-03-03},
  keywords = {Statistics - Machine Learning,Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  author = {Carrara, Nicolas and Leurent, Edouard and Laroche, Romain and Urvoy, Tanguy and Maillard, Odalric-Ambrym and Pietquin, Olivier},
  file = {/home/dimitri/Nextcloud/Zotero/storage/Q97ZL24I/Carrara et al. - 2019 - Budgeted Reinforcement Learning in Continuous Stat.pdf;/home/dimitri/Nextcloud/Zotero/storage/MKNRYBMH/1903.html}
}

@article{mehriPretrainingMethodsDialog2019,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1906.00414},
  primaryClass = {cs},
  title = {Pretraining {{Methods}} for {{Dialog Context Representation Learning}}},
  url = {http://arxiv.org/abs/1906.00414},
  abstract = {This paper examines various unsupervised pretraining objectives for learning dialog context representations. Two novel methods of pretraining dialog context encoders are proposed, and a total of four methods are examined. Each pretraining objective is fine-tuned and evaluated on a set of downstream dialog tasks using the MultiWoz dataset and strong performance improvement is observed. Further evaluation shows that our pretraining objectives result in not only better performance, but also better convergence, models that are less data hungry and have better domain generalizability.},
  urldate = {2019-06-05},
  date = {2019-06-02},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  author = {Mehri, Shikib and Razumovskaia, Evgeniia and Zhao, Tiancheng and Eskenazi, Maxine},
  file = {/home/dimitri/Nextcloud/Zotero/storage/SX4T6RF8/Mehri et al. - 2019 - Pretraining Methods for Dialog Context Representat.pdf;/home/dimitri/Nextcloud/Zotero/storage/GFW2T6ZV/1906.html}
}

@article{marblestoneIntegrationDeepLearning2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1606.03813},
  primaryClass = {q-bio},
  title = {Towards an Integration of Deep Learning and Neuroscience},
  url = {http://arxiv.org/abs/1606.03813},
  abstract = {Neuroscience has focused on the detailed implementation of computation, studying neural codes, dynamics and circuits. In machine learning, however, artificial neural networks tend to eschew precisely designed codes, dynamics or circuits in favor of brute force optimization of a cost function, often using simple and relatively uniform initial architectures. Two recent developments have emerged within machine learning that create an opportunity to connect these seemingly divergent perspectives. First, structured architectures are used, including dedicated systems for attention, recursion and various forms of short- and long-term memory storage. Second, cost functions and training procedures have become more complex and are varied across layers and over time. Here we think about the brain in terms of these ideas. We hypothesize that (1) the brain optimizes cost functions, (2) these cost functions are diverse and differ across brain locations and over development, and (3) optimization operates within a pre-structured architecture matched to the computational problems posed by behavior. Such a heterogeneously optimized system, enabled by a series of interacting cost functions, serves to make learning data-efficient and precisely targeted to the needs of the organism. We suggest directions by which neuroscience could seek to refine and test these hypotheses.},
  urldate = {2019-06-05},
  date = {2016-06-13},
  keywords = {Quantitative Biology - Neurons and Cognition},
  author = {Marblestone, Adam and Wayne, Greg and Kording, Konrad},
  file = {/home/dimitri/Nextcloud/Zotero/storage/F6UGWEZX/Marblestone et al. - 2016 - Towards an integration of deep learning and neuros.pdf;/home/dimitri/Nextcloud/Zotero/storage/SJDK35YE/1606.html}
}

@inproceedings{barhamMachineLearningSystems2019,
  location = {{New York, NY, USA}},
  title = {Machine {{Learning Systems Are Stuck}} in a {{Rut}}},
  isbn = {978-1-4503-6727-1},
  url = {http://doi.acm.org/10.1145/3317550.3321441},
  doi = {10.1145/3317550.3321441},
  abstract = {In this paper we argue that systems for numerical computing are stuck in a local basin of performance and programmability. Systems researchers are doing an excellent job improving the performance of 5-year-old benchmarks, but gradually making it harder to explore innovative machine learning research ideas. We explain how the evolution of hardware accelerators favors compiler back ends that hyper-optimize large monolithic kernels, show how this reliance on high-performance but inflexible kernels reinforces the dominant style of programming model, and argue these programming abstractions lack expressiveness, maintainability, and modularity; all of which hinders research progress. We conclude by noting promising directions in the field, and advocate steps to advance progress towards high-performance general purpose numerical computing systems on modern accelerators.},
  booktitle = {Proceedings of the {{Workshop}} on {{Hot Topics}} in {{Operating Systems}}},
  series = {{{HotOS}} '19},
  publisher = {{ACM}},
  urldate = {2019-06-07},
  date = {2019},
  pages = {177--183},
  author = {Barham, Paul and Isard, Michael},
  file = {/home/dimitri/Nextcloud/Zotero/storage/KK5T5X4X/Barham and Isard - 2019 - Machine Learning Systems Are Stuck in a Rut.pdf},
  venue = {Bertinoro, Italy}
}

@article{robertShortHistoryMarkov2011,
  langid = {english},
  title = {A {{Short History}} of {{Markov Chain Monte Carlo}}: {{Subjective Recollections}} from {{Incomplete Data}}},
  volume = {26},
  issn = {0883-4237},
  url = {http://projecteuclid.org/euclid.ss/1307626568},
  doi = {10.1214/10-STS351},
  shorttitle = {A {{Short History}} of {{Markov Chain Monte Carlo}}},
  number = {1},
  journaltitle = {Statistical Science},
  shortjournal = {Statist. Sci.},
  urldate = {2019-06-13},
  date = {2011-02},
  pages = {102-115},
  author = {Robert, Christian and Casella, George},
  file = {/home/dimitri/Nextcloud/Zotero/storage/6QNRSI58/Robert_Casella_2011_A Short History of Markov Chain Monte Carlo.pdf}
}

@book{highamAccuracyStabilityNumerical2002,
  location = {{Philadelphia}},
  title = {Accuracy and Stability of Numerical Algorithms},
  edition = {2nd ed},
  isbn = {978-0-89871-521-7},
  pagetotal = {680},
  publisher = {{Society for Industrial and Applied Mathematics}},
  date = {2002},
  keywords = {Data processing,Computer algorithms,Numerical analysis},
  author = {Higham, Nicholas J.},
  file = {/home/dimitri/Nextcloud/Zotero/storage/QSQRZ8JC/Higham_2002_Accuracy and stability of numerical algorithms.pdf}
}

@inproceedings{valiantTheoryLearnable1984,
  langid = {english},
  location = {{Not Known}},
  title = {A Theory of the Learnable},
  isbn = {978-0-89791-133-7},
  url = {http://portal.acm.org/citation.cfm?doid=800057.808710},
  doi = {10.1145/800057.808710},
  eventtitle = {The Sixteenth Annual {{ACM}} Symposium},
  booktitle = {Proceedings of the Sixteenth Annual {{ACM}} Symposium on {{Theory}} of Computing  - {{STOC}} '84},
  publisher = {{ACM Press}},
  urldate = {2019-06-17},
  date = {1984},
  pages = {436-445},
  author = {Valiant, L. G.},
  file = {/home/dimitri/Nextcloud/Zotero/storage/QVP7SBJ3/Valiant - 1984 - A theory of the learnable.pdf}
}

@article{fujimotoOffPolicyDeepReinforcement2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1812.02900},
  primaryClass = {cs, stat},
  title = {Off-{{Policy Deep Reinforcement Learning}} without {{Exploration}}},
  url = {http://arxiv.org/abs/1812.02900},
  abstract = {Many practical applications of reinforcement learning constrain agents to learn from a fixed batch of data which has already been gathered, without offering further possibility for data collection. In this paper, we demonstrate that due to errors introduced by extrapolation, standard off-policy deep reinforcement learning algorithms, such as DQN and DDPG, are incapable of learning with data uncorrelated to the distribution under the current policy, making them ineffective for this fixed batch setting. We introduce a novel class of off-policy algorithms, batch-constrained reinforcement learning, which restricts the action space in order to force the agent towards behaving close to on-policy with respect to a subset of the given data. We present the first continuous control deep reinforcement learning algorithm which can learn effectively from arbitrary, fixed batch data, and empirically demonstrate the quality of its behavior in several tasks.},
  urldate = {2019-06-18},
  date = {2018-12-06},
  keywords = {Statistics - Machine Learning,Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  author = {Fujimoto, Scott and Meger, David and Precup, Doina},
  file = {/home/dimitri/Nextcloud/Zotero/storage/EBAYPT9G/Fujimoto et al. - 2018 - Off-Policy Deep Reinforcement Learning without Exp.pdf;/home/dimitri/Nextcloud/Zotero/storage/AGZ8L538/1812.html}
}

@article{hannaImportanceSamplingPolicy2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1806.01347},
  primaryClass = {cs, stat},
  title = {Importance {{Sampling Policy Evaluation}} with an {{Estimated Behavior Policy}}},
  url = {http://arxiv.org/abs/1806.01347},
  abstract = {We consider the problem of off-policy evaluation in Markov decision processes. Off-policy evaluation is the task of evaluating the expected return of one policy with data generated by a different, behavior policy. Importance sampling is a technique for off-policy evaluation that re-weights off-policy returns to account for differences in the likelihood of the returns between the two policies. In this paper, we study importance sampling with an estimated behavior policy where the behavior policy estimate comes from the same set of data used to compute the importance sampling estimate. We find that this estimator often lowers the mean squared error of off-policy evaluation compared to importance sampling with the true behavior policy or using a behavior policy that is estimated from a separate data set. Intuitively, estimating the behavior policy in this way corrects for error due to sampling in the action-space. Our empirical results also extend to other popular variants of importance sampling and show that estimating a non-Markovian behavior policy can further lower large-sample mean squared error even when the true behavior policy is Markovian.},
  urldate = {2019-06-18},
  date = {2018-06-04},
  keywords = {Statistics - Machine Learning,Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  author = {Hanna, Josiah P. and Niekum, Scott and Stone, Peter},
  file = {/home/dimitri/Nextcloud/Zotero/storage/FFTG8CRE/Hanna et al. - 2018 - Importance Sampling Policy Evaluation with an Esti.pdf;/home/dimitri/Nextcloud/Zotero/storage/D3GYPQ3B/1806.html}
}

@article{chandakLearningActionRepresentations2019a,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1902.00183},
  primaryClass = {cs, stat},
  title = {Learning {{Action Representations}} for {{Reinforcement Learning}}},
  url = {http://arxiv.org/abs/1902.00183},
  abstract = {Most model-free reinforcement learning methods leverage state representations (embeddings) for generalization, but either ignore structure in the space of actions or assume the structure is provided a priori. We show how a policy can be decomposed into a component that acts in a low-dimensional space of action representations and a component that transforms these representations into actual actions. These representations improve generalization over large, finite action sets by allowing the agent to infer the outcomes of actions similar to actions already taken. We provide an algorithm to both learn and use action representations and provide conditions for its convergence. The efficacy of the proposed method is demonstrated on large-scale real-world problems.},
  urldate = {2019-06-18},
  date = {2019-01-31},
  keywords = {Statistics - Machine Learning,Computer Science - Machine Learning},
  author = {Chandak, Yash and Theocharous, Georgios and Kostas, James and Jordan, Scott and Thomas, Philip S.},
  file = {/home/dimitri/Nextcloud/Zotero/storage/IALS2P6C/Chandak et al. - 2019 - Learning Action Representations for Reinforcement .pdf;/home/dimitri/Nextcloud/Zotero/storage/SC7ZUA3I/1902.html}
}

@article{gottesmanCombiningParametricNonparametric2019,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1905.05787},
  primaryClass = {cs, stat},
  title = {Combining {{Parametric}} and {{Nonparametric Models}} for {{Off}}-{{Policy Evaluation}}},
  url = {http://arxiv.org/abs/1905.05787},
  abstract = {We consider a model-based approach to perform batch off-policy evaluation in reinforcement learning. Our method takes a mixture-of-experts approach to combine parametric and non-parametric models of the environment such that the final value estimate has the least expected error. We do so by first estimating the local accuracy of each model and then using a planner to select which model to use at every time step as to minimize the return error estimate along entire trajectories. Across a variety of domains, our mixture-based approach outperforms the individual models alone as well as state-of-the-art importance sampling-based estimators.},
  urldate = {2019-06-18},
  date = {2019-05-14},
  keywords = {Statistics - Machine Learning,Computer Science - Machine Learning},
  author = {Gottesman, Omer and Liu, Yao and Sussex, Scott and Brunskill, Emma and Doshi-Velez, Finale},
  file = {/home/dimitri/Nextcloud/Zotero/storage/XXS4XBWY/Gottesman et al. - 2019 - Combining Parametric and Nonparametric Models for .pdf;/home/dimitri/Nextcloud/Zotero/storage/YTHD3LMY/1905.html}
}

@article{leBatchPolicyLearning2019,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1903.08738},
  primaryClass = {cs, math, stat},
  title = {Batch {{Policy Learning}} under {{Constraints}}},
  url = {http://arxiv.org/abs/1903.08738},
  abstract = {When learning policies for real-world domains, two important questions arise: (i) how to efficiently use pre-collected off-policy, non-optimal behavior data; and (ii) how to mediate among different competing objectives and constraints. We thus study the problem of batch policy learning under multiple constraints, and offer a systematic solution. We first propose a flexible meta-algorithm that admits any batch reinforcement learning and online learning procedure as subroutines. We then present a specific algorithmic instantiation and provide performance guarantees for the main objective and all constraints. To certify constraint satisfaction, we propose a new and simple method for off-policy policy evaluation (OPE) and derive PAC-style bounds. Our algorithm achieves strong empirical results in different domains, including in a challenging problem of simulated car driving subject to multiple constraints such as lane keeping and smooth driving. We also show experimentally that our OPE method outperforms other popular OPE techniques on a standalone basis, especially in a high-dimensional setting.},
  urldate = {2019-06-18},
  date = {2019-03-20},
  keywords = {Statistics - Machine Learning,Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Mathematics - Optimization and Control},
  author = {Le, Hoang M. and Voloshin, Cameron and Yue, Yisong},
  file = {/home/dimitri/Nextcloud/Zotero/storage/ELWQXAKB/Le et al. - 2019 - Batch Policy Learning under Constraints.pdf;/home/dimitri/Nextcloud/Zotero/storage/34RTF36T/1903.html}
}

@article{yangXLNetGeneralizedAutoregressive2019,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1906.08237},
  primaryClass = {cs},
  title = {{{XLNet}}: {{Generalized Autoregressive Pretraining}} for {{Language Understanding}}},
  url = {http://arxiv.org/abs/1906.08237},
  shorttitle = {{{XLNet}}},
  abstract = {With the capability of modeling bidirectional contexts, denoising autoencoding based pretraining like BERT achieves better performance than pretraining approaches based on autoregressive language modeling. However, relying on corrupting the input with masks, BERT neglects dependency between the masked positions and suffers from a pretrain-finetune discrepancy. In light of these pros and cons, we propose XLNet, a generalized autoregressive pretraining method that (1) enables learning bidirectional contexts by maximizing the expected likelihood over all permutations of the factorization order and (2) overcomes the limitations of BERT thanks to its autoregressive formulation. Furthermore, XLNet integrates ideas from Transformer-XL, the state-of-the-art autoregressive model, into pretraining. Empirically, XLNet outperforms BERT on 20 tasks, often by a large margin, and achieves state-of-the-art results on 18 tasks including question answering, natural language inference, sentiment analysis, and document ranking.},
  urldate = {2019-06-21},
  date = {2019-06-19},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  author = {Yang, Zhilin and Dai, Zihang and Yang, Yiming and Carbonell, Jaime and Salakhutdinov, Ruslan and Le, Quoc V.},
  file = {/home/dimitri/Nextcloud/Zotero/storage/FCUWCRCK/Yang et al. - 2019 - XLNet Generalized Autoregressive Pretraining for .pdf;/home/dimitri/Nextcloud/Zotero/storage/FB7CLIH3/1906.html}
}

@article{coenenVisualizingMeasuringGeometry2019,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1906.02715},
  primaryClass = {cs, stat},
  title = {Visualizing and {{Measuring}} the {{Geometry}} of {{BERT}}},
  url = {http://arxiv.org/abs/1906.02715},
  abstract = {Transformer architectures show significant promise for natural language processing. Given that a single pretrained model can be fine-tuned to perform well on many different tasks, these networks appear to extract generally useful linguistic features. A natural question is how such networks represent this information internally. This paper describes qualitative and quantitative investigations of one particularly effective model, BERT. At a high level, linguistic features seem to be represented in separate semantic and syntactic subspaces. We find evidence of a fine-grained geometric representation of word senses. We also present empirical descriptions of syntactic representations in both attention matrices and individual word embeddings, as well as a mathematical argument to explain the geometry of these representations.},
  urldate = {2019-06-21},
  date = {2019-06-06},
  keywords = {Statistics - Machine Learning,Computer Science - Computation and Language,Computer Science - Machine Learning},
  author = {Coenen, Andy and Reif, Emily and Yuan, Ann and Kim, Been and Pearce, Adam and Viégas, Fernanda and Wattenberg, Martin},
  file = {/home/dimitri/Nextcloud/Zotero/storage/7D3K8L65/Coenen et al. - 2019 - Visualizing and Measuring the Geometry of BERT.pdf;/home/dimitri/Nextcloud/Zotero/storage/7WX24LBK/1906.html}
}

@article{pacchianoWassersteinReinforcementLearning2019,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1906.04349},
  primaryClass = {cs, stat},
  title = {Wasserstein {{Reinforcement Learning}}},
  url = {http://arxiv.org/abs/1906.04349},
  abstract = {We propose behavior-driven optimization via Wasserstein distances (WDs) to improve several classes of state-of-the-art reinforcement learning (RL) algorithms. We show that WD regularizers acting on appropriate policy embeddings efficiently incorporate behavioral characteristics into policy optimization. We demonstrate that they improve Evolution Strategy methods by encouraging more efficient exploration, can be applied in imitation learning and to speed up training of Trust Region Policy Optimization methods. Since the exact computation of WDs is expensive, we develop approximate algorithms based on the combination of different methods: dual formulation of the optimal transport problem, alternating optimization and random feature maps, to effectively replace exact WD computations in the RL tasks considered. We provide theoretical analysis of our algorithms and exhaustive empirical evaluation in a variety of RL settings.},
  urldate = {2019-06-21},
  date = {2019-06-10},
  keywords = {Statistics - Machine Learning,Computer Science - Machine Learning},
  author = {Pacchiano, Aldo and Parker-Holder, Jack and Tang, Yunhao and Choromanska, Anna and Choromanski, Krzysztof and Jordan, Michael I.},
  file = {/home/dimitri/Nextcloud/Zotero/storage/PW86XBCQ/Pacchiano et al. - 2019 - Wasserstein Reinforcement Learning.pdf;/home/dimitri/Nextcloud/Zotero/storage/4BPXH577/1906.html}
}

@article{norenzayanCulturalEvolutionProsocial2016,
  langid = {english},
  title = {The Cultural Evolution of Prosocial Religions},
  volume = {39},
  issn = {0140-525X, 1469-1825},
  url = {https://www.cambridge.org/core/journals/behavioral-and-brain-sciences/article/cultural-evolution-of-prosocial-religions/01B053B0294890F8CFACFB808FE2A0EF},
  doi = {10.1017/S0140525X14001356},
  abstract = {We develop a cultural evolutionary theory of the origins of prosocial religions and apply it to resolve two puzzles in human psychology and cultural history: (1) the rise of large-scale cooperation among strangers and, simultaneously, (2) the spread of prosocial religions in the last 10–12 millennia. We argue that these two developments were importantly linked and mutually energizing. We explain how a package of culturally evolved religious beliefs and practices characterized by increasingly potent, moralizing, supernatural agents, credible displays of faith, and other psychologically active elements conducive to social solidarity promoted high fertility rates and large-scale cooperation with co-religionists, often contributing to success in intergroup competition and conflict. In turn, prosocial religious beliefs and practices spread and aggregated as these successful groups expanded, or were copied by less successful groups. This synthesis is grounded in the idea that although religious beliefs and practices originally arose as nonadaptive by-products of innate cognitive functions, particular cultural variants were then selected for their prosocial effects in a long-term, cultural evolutionary process. This framework (1) reconciles key aspects of the adaptationist and by-product approaches to the origins of religion, (2) explains a variety of empirical observations that have not received adequate attention, and (3) generates novel predictions. Converging lines of evidence drawn from diverse disciplines provide empirical support while at the same time encouraging new research directions and opening up new questions for exploration and debate.},
  journaltitle = {Behavioral and Brain Sciences},
  urldate = {2019-06-21},
  year = {2016/ed},
  keywords = {belief,cooperation,culture,evolution,prosociality,religion,ritual},
  author = {Norenzayan, Ara and Shariff, Azim F. and Gervais, Will M. and Willard, Aiyana K. and McNamara, Rita A. and Slingerland, Edward and Henrich, Joseph},
  file = {/home/dimitri/Nextcloud/Zotero/storage/CQ8HVTSM/Norenzayan et al_2016_The cultural evolution of prosocial religions.pdf;/home/dimitri/Nextcloud/Zotero/storage/C2EHTZPD/01B053B0294890F8CFACFB808FE2A0EF.html}
}

@article{atranEvolutionReligionHow2010,
  langid = {english},
  title = {The {{Evolution}} of {{Religion}}: {{How Cognitive By}}-{{Products}}, {{Adaptive Learning Heuristics}}, {{Ritual Displays}}, and {{Group Competition Generate Deep Commitments}} to {{Prosocial Religions}}},
  volume = {5},
  issn = {1555-5542, 1555-5550},
  url = {http://link.springer.com/10.1162/BIOT_a_00018},
  doi = {10.1162/BIOT_a_00018},
  shorttitle = {The {{Evolution}} of {{Religion}}},
  number = {1},
  journaltitle = {Biological Theory},
  shortjournal = {Biol Theory},
  urldate = {2019-06-21},
  date = {2010-03},
  pages = {18-30},
  author = {Atran, Scott and Henrich, Joseph},
  file = {/home/dimitri/Nextcloud/Zotero/storage/M4SNZT3C/Atran_Henrich_2010_The Evolution of Religion.pdf}
}

@article{henrichEvolutionCostlyDisplays2009,
  langid = {english},
  title = {The Evolution of Costly Displays, Cooperation and Religion},
  volume = {30},
  issn = {10905138},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S1090513809000245},
  doi = {10.1016/j.evolhumbehav.2009.03.005},
  number = {4},
  journaltitle = {Evolution and Human Behavior},
  shortjournal = {Evolution and Human Behavior},
  urldate = {2019-06-21},
  date = {2009-07},
  pages = {244-260},
  author = {Henrich, Joseph},
  file = {/home/dimitri/Nextcloud/Zotero/storage/EKRNQ87Q/Henrich_2009_The evolution of costly displays, cooperation and religion.pdf}
}

@article{irpanOffPolicyEvaluationOffPolicy2019,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1906.01624},
  primaryClass = {cs, stat},
  title = {Off-{{Policy Evaluation}} via {{Off}}-{{Policy Classification}}},
  url = {http://arxiv.org/abs/1906.01624},
  abstract = {In this work, we consider the problem of model selection for deep reinforcement learning (RL) in real-world environments. Typically, the performance of deep RL algorithms is evaluated via on-policy interactions with the target environment. However, comparing models in a real-world environment for the purposes of early stopping or hyperparameter tuning is costly and often practically infeasible. This leads us to examine off-policy policy evaluation (OPE) in such settings. We focus on OPE for value-based methods, which are of particular interest in deep RL, with applications like robotics, where off-policy algorithms based on Q-function estimation can often attain better sample complexity than direct policy optimization. Existing OPE metrics either rely on a model of the environment, or the use of importance sampling (IS) to correct for the data being off-policy. However, for high-dimensional observations, such as images, models of the environment can be difficult to fit and value-based methods can make IS hard to use or even ill-conditioned, especially when dealing with continuous action spaces. In this paper, we focus on the specific case of MDPs with continuous action spaces and sparse binary rewards, which is representative of many important real-world applications. We propose an alternative metric that relies on neither models nor IS, by framing OPE as a positive-unlabeled (PU) classification problem with the Q-function as the decision function. We experimentally show that this metric outperforms baselines on a number of tasks. Most importantly, it can reliably predict the relative performance of different policies in a number of generalization scenarios, including the transfer to the real-world of policies trained in simulation for an image-based robotic manipulation task.},
  urldate = {2019-06-24},
  date = {2019-06-04},
  keywords = {Statistics - Machine Learning,Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics},
  author = {Irpan, Alex and Rao, Kanishka and Bousmalis, Konstantinos and Harris, Chris and Ibarz, Julian and Levine, Sergey},
  file = {/home/dimitri/Nextcloud/Zotero/storage/2DE9I4IE/Irpan et al. - 2019 - Off-Policy Evaluation via Off-Policy Classificatio.pdf;/home/dimitri/Nextcloud/Zotero/storage/I3D7GBEL/1906.html}
}

@article{bradleyWhatAppliedCategory2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1809.05923},
  primaryClass = {math},
  title = {What Is {{Applied Category Theory}}?},
  url = {http://arxiv.org/abs/1809.05923},
  abstract = {This is a collection of introductory, expository notes on applied category theory, inspired by the 2018 Applied Category Theory Workshop, and in these notes we take a leisurely stroll through two themes (functorial semantics and compositionality), two constructions (monoidal categories and decorated cospans) and two examples (chemical reaction networks and natural language processing) within the field.},
  urldate = {2019-06-25},
  date = {2018-09-16},
  keywords = {Mathematics - Category Theory},
  author = {Bradley, Tai-Danae},
  file = {/home/dimitri/Nextcloud/Zotero/storage/WCBRMZVX/Bradley - 2018 - What is Applied Category Theory.pdf;/home/dimitri/Nextcloud/Zotero/storage/RS4RK4HS/1809.html}
}

@book{spivakCategoryTheorySciences2014,
  location = {{Cambridge, Massachusetts}},
  title = {Category Theory for the Sciences},
  isbn = {978-0-262-02813-4},
  pagetotal = {486},
  publisher = {{The MIT Press}},
  date = {2014},
  keywords = {Categories (Mathematics),Mathematical models,Science},
  author = {Spivak, David I.},
  file = {/home/dimitri/Nextcloud/Zotero/storage/HI5KSG9I/Spivak - 2014 - Category theory for the sciences.pdf}
}

@article{leinsterBasicCategoryTheory2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1612.09375},
  primaryClass = {math},
  title = {Basic {{Category Theory}}},
  url = {http://arxiv.org/abs/1612.09375},
  abstract = {This short introduction to category theory is for readers with relatively little mathematical background. At its heart is the concept of a universal property, important throughout mathematics. After a chapter introducing the basic definitions, separate chapters present three ways of expressing universal properties: via adjoint functors, representable functors, and limits. A final chapter ties the three together. For each new categorical concept, a generous supply of examples is provided, taken from different parts of mathematics. At points where the leap in abstraction is particularly great (such as the Yoneda lemma), the reader will find careful and extensive explanations.},
  urldate = {2019-06-27},
  date = {2016-12-29},
  keywords = {Mathematics - Algebraic Topology,Mathematics - Category Theory,Mathematics - Logic},
  author = {Leinster, Tom},
  file = {/home/dimitri/Nextcloud/Zotero/storage/F3NW5R44/Leinster - 2016 - Basic Category Theory.pdf;/home/dimitri/Nextcloud/Zotero/storage/3BNPC6VP/1612.html}
}

@book{riehlCategoryTheoryContext2017,
  langid = {english},
  location = {{United States}},
  title = {Category Theory in Context},
  isbn = {978-0-486-82080-4},
  publisher = {{Dover Publications : Made available through hoopla}},
  date = {2017},
  author = {Riehl, Emily},
  file = {/home/dimitri/Nextcloud/Zotero/storage/H2XLYX3I/Riehl - 2017 - Category theory in context.pdf},
  note = {OCLC: 1098977147}
}

@article{maheswaranathanReverseEngineeringRecurrent2019,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1906.10720},
  primaryClass = {cs, stat},
  title = {Reverse Engineering Recurrent Networks for Sentiment Classification Reveals Line Attractor Dynamics},
  url = {http://arxiv.org/abs/1906.10720},
  abstract = {Recurrent neural networks (RNNs) are a widely used tool for modeling sequential data, yet they are often treated as inscrutable black boxes. Given a trained recurrent network, we would like to reverse engineer it--to obtain a quantitative, interpretable description of how it solves a particular task. Even for simple tasks, a detailed understanding of how recurrent networks work, or a prescription for how to develop such an understanding, remains elusive. In this work, we use tools from dynamical systems analysis to reverse engineer recurrent networks trained to perform sentiment classification, a foundational natural language processing task. Given a trained network, we find fixed points of the recurrent dynamics and linearize the nonlinear system around these fixed points. Despite their theoretical capacity to implement complex, high-dimensional computations, we find that trained networks converge to highly interpretable, low-dimensional representations. In particular, the topological structure of the fixed points and corresponding linearized dynamics reveal an approximate line attractor within the RNN, which we can use to quantitatively understand how the RNN solves the sentiment analysis task. Finally, we find this mechanism present across RNN architectures (including LSTMs, GRUs, and vanilla RNNs) trained on multiple datasets, suggesting that our findings are not unique to a particular architecture or dataset. Overall, these results demonstrate that surprisingly universal and human interpretable computations can arise across a range of recurrent networks.},
  urldate = {2019-06-28},
  date = {2019-06-25},
  keywords = {Statistics - Machine Learning,Computer Science - Machine Learning},
  author = {Maheswaranathan, Niru and Williams, Alex and Golub, Matthew D. and Ganguli, Surya and Sussillo, David},
  file = {/home/dimitri/Nextcloud/Zotero/storage/KSL5IDVM/Maheswaranathan et al. - 2019 - Reverse engineering recurrent networks for sentime.pdf;/home/dimitri/Nextcloud/Zotero/storage/LLWPJ6D8/1906.html}
}

@article{yurochkinHierarchicalOptimalTransport2019,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1906.10827},
  primaryClass = {cs, stat},
  title = {Hierarchical {{Optimal Transport}} for {{Document Representation}}},
  url = {http://arxiv.org/abs/1906.10827},
  abstract = {The ability to measure similarity between documents enables intelligent summarization and analysis of large corpora. Past distances between documents suffer from either an inability to incorporate semantic similarities between words or from scalability issues. As an alternative, we introduce hierarchical optimal transport as a meta-distance between documents, where documents are modeled as distributions over topics, which themselves are modeled as distributions over words. We then solve an optimal transport problem on the smaller topic space to compute a similarity score. We give conditions on the topics under which this construction defines a distance, and we relate it to the word mover's distance. We evaluate our technique for \$k\$-NN classification and show better interpretability and scalability with comparable performance to current methods at a fraction of the cost.},
  urldate = {2019-06-28},
  date = {2019-06-25},
  keywords = {Statistics - Machine Learning,Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Information Retrieval},
  author = {Yurochkin, Mikhail and Claici, Sebastian and Chien, Edward and Mirzazadeh, Farzaneh and Solomon, Justin},
  file = {/home/dimitri/Nextcloud/Zotero/storage/EJGKCIUG/Yurochkin et al. - 2019 - Hierarchical Optimal Transport for Document Repres.pdf;/home/dimitri/Nextcloud/Zotero/storage/EC9XIVU7/1906.html}
}

@article{betancourtConceptualIntroductionHamiltonian2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1701.02434},
  primaryClass = {stat},
  title = {A {{Conceptual Introduction}} to {{Hamiltonian Monte Carlo}}},
  url = {http://arxiv.org/abs/1701.02434},
  abstract = {Hamiltonian Monte Carlo has proven a remarkable empirical success, but only recently have we begun to develop a rigorous understanding of why it performs so well on difficult problems and how it is best applied in practice. Unfortunately, that understanding is confined within the mathematics of differential geometry which has limited its dissemination, especially to the applied communities for which it is particularly important. In this review I provide a comprehensive conceptual account of these theoretical foundations, focusing on developing a principled intuition behind the method and its optimal implementations rather of any exhaustive rigor. Whether a practitioner or a statistician, the dedicated reader will acquire a solid grasp of how Hamiltonian Monte Carlo works, when it succeeds, and, perhaps most importantly, when it fails.},
  urldate = {2019-06-28},
  date = {2017-01-09},
  keywords = {Statistics - Methodology},
  author = {Betancourt, Michael},
  file = {/home/dimitri/Nextcloud/Zotero/storage/8ZS8KLKS/Betancourt_2017_A Conceptual Introduction to Hamiltonian Monte Carlo.pdf;/home/dimitri/Nextcloud/Zotero/storage/UG4K45FI/1701.html}
}

@article{diaconisMarkovChainMonte2008,
  langid = {english},
  title = {The {{Markov}} Chain {{Monte Carlo}} Revolution},
  volume = {46},
  issn = {0273-0979},
  url = {http://www.ams.org/journal-getitem?pii=S0273-0979-08-01238-X},
  doi = {10.1090/S0273-0979-08-01238-X},
  number = {2},
  journaltitle = {Bulletin of the American Mathematical Society},
  shortjournal = {Bull. Amer. Math. Soc.},
  urldate = {2019-06-28},
  date = {2008-11-20},
  pages = {179-205},
  author = {Diaconis, Persi},
  file = {/home/dimitri/Nextcloud/Zotero/storage/B34NLYEQ/Diaconis_2008_The Markov chain Monte Carlo revolution.pdf}
}

@thesis{rainforthAutomatingInferenceLearning2017,
  langid = {english},
  title = {Automating Inference, Learning, and Design Using Probabilistic Programming},
  url = {https://ora.ox.ac.uk/objects/uuid:e276f3b4-ff1d-44bf-9d67-013f68ce81f0#citeForm},
  abstract = {Imagine a world where computational simulations can be inverted as easily as running them forwards, where data can be used to refine models automatically, and where the only expertise one needs to carry out powerful statistical analysis is a basic proficiency in scientific coding. Creating such a world is the ambitious long-term aim of probabilistic programming. The bottleneck for improving the probabilistic models, or simulators, used throughout the quantitative sciences, is often not an ability to devise better models conceptually, but a lack of expertise, time, or resources to realize such innovations. Probabilistic programming systems (PPSs) help alleviate this bottleneck by providing an expressive and accessible modeling framework, then automating the required computation to draw inferences from the model, for example finding the model parameters likely to give rise to a certain output. By decoupling model specification and inference, PPSs streamline the process of developing and drawing inferences from new models, while opening up powerful statistical methods to non-experts. Many systems further provide the flexibility to write new and exciting models which would be hard, or even impossible, to convey using conventional statistical frameworks. The central goal of this thesis is to improve and extend PPSs. In particular, we will make advancements to the underlying inference engines and increase the range of problems which can be tackled. For example, we will extend PPSs to a mixed inference-optimization framework, thereby providing automation of tasks such as model learning and engineering design. Meanwhile, we make inroads into constructing systems for automating adaptive sequential design problems, providing potential applications across the sciences. Furthermore, the contributions of the work reach far beyond probabilistic programming, as achieving our goal will require us to make advancements in a number of related fields such as particle Markov chain Monte Carlo methods, Bayesian optimization, and Monte Carlo fundamentals.},
  institution = {{University of Oxford}},
  type = {http://purl.org/dc/dcmitype/Text},
  urldate = {2019-06-28},
  date = {2017},
  author = {Rainforth, Thomas William Gamlen},
  file = {/home/dimitri/Nextcloud/Zotero/storage/4QUDLF5N/Rainforth_2017_Automating inference, learning, and design using probabilistic programming.pdf;/home/dimitri/Nextcloud/Zotero/storage/E2GS2T7K/uuide276f3b4-ff1d-44bf-9d67-013f68ce81f0.html}
}


